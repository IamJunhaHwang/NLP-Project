{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c2608d1",
   "metadata": {},
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c912dc3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:02:43.157174Z",
     "start_time": "2022-12-10T09:02:40.900984Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5f5b299",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:02:43.273075Z",
     "start_time": "2022-12-10T09:02:43.170538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: <class 'torch.cuda.device'>\n",
      "Count of using GPUs: 2\n",
      "Current cuda device: 0\n"
     ]
    }
   ],
   "source": [
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  # 디버깅 위한 세팅\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print('Device:', torch.cuda.device)  # 출력결과: cuda \n",
    "print('Count of using GPUs:', torch.cuda.device_count())   \n",
    "print('Current cuda device:', torch.cuda.current_device()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e99d71c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T10:11:57.190753Z",
     "start_time": "2022-12-02T10:11:57.186018Z"
    }
   },
   "source": [
    "### Load Data\n",
    "Data가 위치한 PATH에서 Data를 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c79d02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:02:44.994476Z",
     "start_time": "2022-12-10T09:02:44.986629Z"
    }
   },
   "outputs": [],
   "source": [
    "def jsonload(fname, encoding=\"utf-8\"):\n",
    "    with open(fname, encoding=encoding) as f:\n",
    "        j = json.load(f)\n",
    "\n",
    "    return j\n",
    "\n",
    "# json 개체를 파일이름으로 깔끔하게 저장\n",
    "def jsondump(j, fname):\n",
    "    with open(fname, \"w\", encoding=\"UTF8\") as f:\n",
    "        json.dump(j, f, ensure_ascii=False)\n",
    "\n",
    "# jsonl 파일 읽어서 list에 저장\n",
    "def jsonlload(fname, encoding=\"utf-8\"):\n",
    "    json_list = []\n",
    "    with open(fname, encoding=encoding) as f:\n",
    "        for line in f.readlines():\n",
    "            json_list.append(json.loads(line))\n",
    "    return json_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "574f7c53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:02:45.534294Z",
     "start_time": "2022-12-10T09:02:45.490825Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = jsonlload('nikluge-sa-2022-train.jsonl')\n",
    "dev_data = jsonlload('nikluge-sa-2022-dev.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000ae698",
   "metadata": {},
   "source": [
    "## Preprocessing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9260a723",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:02:47.016543Z",
     "start_time": "2022-12-10T09:02:47.010081Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(text: str, only_kor: bool = True):\n",
    "    \"\"\"한국어 문장을 옵션에 맞게 전처리\"\"\"\n",
    "    # 한국어 모음과 특수 문자, 숫자 및 영어 제거\n",
    "    if only_kor:\n",
    "        text = re.sub(f\"[^가-힣| |]+\", \"\", text)\n",
    "    else:\n",
    "        text = re.sub(f\"[^가-힣|ㄱ-ㅎ|0-9|]+\", \"\", text)\n",
    "\n",
    "    # 연속 공백 제거\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "\n",
    "    # 좌우 불필요한 공백 제거\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d5db16",
   "metadata": {},
   "source": [
    "## 재현성 위한 seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20c859ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:02:47.729876Z",
     "start_time": "2022-12-10T09:02:47.722002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed:int = 1004):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "    \n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495015f4",
   "metadata": {},
   "source": [
    "## 태그셋 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "658b9bcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:02:48.563789Z",
     "start_time": "2022-12-10T09:02:48.546321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#개체#속성 태그셋 정의\n",
    "entity_property_pair = [\n",
    "    '제품 전체#품질', '제품 전체#편의성', '제품 전체#디자인', '제품 전체#일반', '제품 전체#가격', \n",
    "    '제품 전체#인지도',  '제품 전체#다양성',\n",
    "    '패키지/구성품#디자인', '패키지/구성품#가격', '패키지/구성품#다양성', '패키지/구성품#일반',\n",
    "    '패키지/구성품#편의성', '패키지/구성품#품질',\n",
    "    '본품#일반', '본품#다양성', '본품#품질', '본품#편의성', '본품#디자인', '본품#가격',\n",
    "    '브랜드#인지도', '브랜드#일반', '브랜드#디자인', '브랜드#품질', '브랜드#가격']\n",
    "\n",
    "rep_entity_property_pair = [\n",
    "    '제품전체#품질', '제품전체#편의성', '제품전체#디자인', '제품전체#일반', '제품전체#가격', \n",
    "    '제품전체#인지도',\n",
    "    '패키지/구성품#디자인', '패키지/구성품#일반',\n",
    "    '패키지/구성품#편의성', '패키지/구성품#품질',\n",
    "    '본품#일반', '본품#다양성', '본품#품질', '본품#편의성', '본품#디자인',\n",
    "    '브랜드#인지도', '브랜드#일반', '브랜드#품질']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Train데이터에 등장 안하거나 희소하게 등장한 태그들 ==> rep에서 모두 지워주었음.\n",
    "\n",
    "제품 전체#다양성  0\n",
    "\n",
    "패키지/구성품#가격 0 \n",
    "패키지/구성품#다양성 1\n",
    "\n",
    "본품#인지도 1\n",
    "본품#가격  2\n",
    "본품#인지도 1\n",
    "\n",
    "브랜드#디자인  0\n",
    "브랜드#가격  3\n",
    "\"\"\"\n",
    "\n",
    "len(rep_entity_property_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aa8c8e",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b53f3d7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:02:54.994214Z",
     "start_time": "2022-12-10T09:02:49.266129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='klue/roberta-large', vocab_size=32000, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = 'klue/roberta-large'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d29a80d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:02:55.035593Z",
     "start_time": "2022-12-10T09:02:55.030075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "special_tokens_dict = {\n",
    "    'additional_special_tokens': rep_entity_property_pair\n",
    "}\n",
    "\n",
    "print(tokenizer.vocab_size)\n",
    "\n",
    "# 토크나이저는 각 모델 별로 만들지 않아도 된다. [임베딩 layer에만 영향 주므로]\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "print(num_added_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8a12794",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:02:55.095351Z",
     "start_time": "2022-12-10T09:02:55.089108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['야', '아이패드', '가격', '왜', '이래', '본품#일반']\n",
      "{'input_ids': [0, 1396, 15641, 3852, 1460, 5625, 32010, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] 야 아이패드 가격 왜 이래 본품#일반 [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"야 아이패드 가격 왜 이래 본품#일반\"))\n",
    "print(tokenizer(\"야 아이패드 가격 왜 이래 본품#일반\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"야 아이패드 가격 왜 이래 본품#일반\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71935b36",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb992cd5",
   "metadata": {},
   "source": [
    "## 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63df4574",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:02:55.171251Z",
     "start_time": "2022-12-10T09:02:55.169032Z"
    }
   },
   "outputs": [],
   "source": [
    "polarity_id_to_name = ['positive', 'negative', 'neutral']  # 차례대로 0, 1, 2\n",
    "polarity_name_to_id = {polarity_id_to_name[i]: i for i in range(len(polarity_id_to_name))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eb8ee7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:02:59.058703Z",
     "start_time": "2022-12-10T09:02:58.802592Z"
    }
   },
   "outputs": [],
   "source": [
    "ACD_Datas = [[] for i in range(len(entity_property_pair))]  # 속성 각각마다의 데이터 셋\n",
    "ACD_labels = [[] for i in range(len(entity_property_pair))]\n",
    "\n",
    "POL_Datas = []\n",
    "POL_labels = []\n",
    "\n",
    "### sentence들 먼저 모으고 한꺼번에 tokenize 하자\n",
    "\n",
    "for idx, pair in enumerate(rep_entity_property_pair):\n",
    "    for idx2, datas in enumerate(train_data):\n",
    "        sen = preprocess(datas['sentence_form'])\n",
    "        annos = datas['annotation']  # 여러 개 일 수도 있음. (이중 리스트)\n",
    "        check_point = False\n",
    "\n",
    "        \"\"\"\n",
    "        개체#속성 태그 셋의 경우 sentence를 보고 모델이 태그를 True/False로 알려줄 것이므로 pair를 넣으면 안됨.\n",
    "        \n",
    "        감성 분석을 위한 데이터셋의 경우 sentence + 태그를 본 후 해당 태그에 대한 감성 분석을 기대하므로 pair를 함께 넣어줌.\n",
    "        \"\"\"\n",
    "        \n",
    "        ACD_Datas[idx].append(sen)\n",
    "        \n",
    "        for idx3, annotation in enumerate(annos):\n",
    "            entity_property = annotation[0]  # raw_data의 annotation 추출\n",
    "            entity_property = entity_property.replace(\" \", \"\")  # 띄어쓰기 제거\n",
    "            polarity = annotation[2]\n",
    "\n",
    "            if entity_property == pair:\n",
    "                check_point = True\n",
    "                \n",
    "            else:\n",
    "                ;\n",
    "                \n",
    "        if check_point:\n",
    "            ACD_labels[idx].append(1)  # 해당 태그 맞음.\n",
    "            POL_Datas.append(sen + \" \"+ pair)\n",
    "            POL_labels.append(polarity_name_to_id[polarity])\n",
    "        \n",
    "        else:\n",
    "            ACD_labels[idx].append(0)\n",
    "            \n",
    "    ACD_Datas[idx], ACD_labels[idx] = shuffle(ACD_Datas[idx], ACD_labels[idx], random_state = 42)\n",
    "    \n",
    "POL_Datas, POL_labels = shuffle(POL_Datas, POL_labels, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69b51e42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:02:59.304241Z",
     "start_time": "2022-12-10T09:02:59.300017Z"
    }
   },
   "outputs": [],
   "source": [
    "devs = dev_data[:838]\n",
    "tests = dev_data[838:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89e67cba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:03:03.687054Z",
     "start_time": "2022-12-10T09:03:03.586478Z"
    }
   },
   "outputs": [],
   "source": [
    "################### Dev set #######################\n",
    "dev_ACD_Datas = [[] for i in range(len(entity_property_pair))]  # 속성 각각마다의 데이터 셋\n",
    "dev_ACD_labels = [[] for i in range(len(entity_property_pair))]\n",
    "\n",
    "dev_POL_Datas = []\n",
    "dev_POL_labels = []\n",
    "\n",
    "### sentence들 먼저 모으고 한꺼번에 tokenize 하자\n",
    "\n",
    "for idx, pair in enumerate(rep_entity_property_pair):\n",
    "    for idx2, datas in enumerate(devs):\n",
    "        sen = preprocess(datas['sentence_form'])\n",
    "        annos = datas['annotation']  # 여러 개 일 수도 있음. (이중 리스트)\n",
    "        check_point = False\n",
    "\n",
    "        \"\"\"\n",
    "        개체#속성 태그 셋의 경우 sentence를 보고 모델이 태그를 True/False로 알려줄 것이므로 pair를 넣으면 안됨.\n",
    "        \n",
    "        감성 분석을 위한 데이터셋의 경우 sentence + 태그를 본 후 해당 태그에 대한 감성 분석을 기대하므로 pair를 함께 넣어줌.\n",
    "        \"\"\"\n",
    "        \n",
    "        dev_ACD_Datas[idx].append(sen)\n",
    "        \n",
    "        for idx3, annotation in enumerate(annos):\n",
    "            entity_property = annotation[0]  # raw_data의 annotation 추출\n",
    "            entity_property = entity_property.replace(\" \", \"\")  # 띄어쓰기 제거\n",
    "            polarity = annotation[2]\n",
    "\n",
    "            if entity_property == pair:\n",
    "                check_point = True\n",
    "                \n",
    "            else:\n",
    "                ;\n",
    "                \n",
    "        if check_point:\n",
    "            dev_ACD_labels[idx].append(1)  # 해당 태그 맞음.\n",
    "            dev_POL_Datas.append(sen + \" \"+ pair)\n",
    "            dev_POL_labels.append(polarity_name_to_id[polarity])\n",
    "        \n",
    "        else:\n",
    "            dev_ACD_labels[idx].append(0)\n",
    "        \n",
    "    dev_ACD_Datas[idx], dev_ACD_labels[idx] = shuffle(dev_ACD_Datas[idx], dev_ACD_labels[idx], random_state = 42)\n",
    "    \n",
    "dev_POL_Datas, dev_POL_labels = shuffle(dev_POL_Datas, dev_POL_labels, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e7202bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:03:04.252052Z",
     "start_time": "2022-12-10T09:03:04.074980Z"
    }
   },
   "outputs": [],
   "source": [
    "################### Test set #######################\n",
    "test_ACD_Datas = [[] for i in range(len(entity_property_pair))]  # 속성 각각마다의 데이터 셋\n",
    "test_ACD_labels = [[] for i in range(len(entity_property_pair))]\n",
    "\n",
    "test_POL_Datas = []\n",
    "test_POL_labels = []\n",
    "\n",
    "### sentence들 먼저 모으고 한꺼번에 tokenize 하자\n",
    "\n",
    "for idx, pair in enumerate(rep_entity_property_pair):\n",
    "    for idx2, datas in enumerate(tests):\n",
    "        sen = preprocess(datas['sentence_form'])\n",
    "        annos = datas['annotation']  # 여러 개 일 수도 있음. (이중 리스트)\n",
    "        check_point = False\n",
    "\n",
    "        \"\"\"\n",
    "        개체#속성 태그 셋의 경우 sentence를 보고 모델이 태그를 True/False로 알려줄 것이므로 pair를 넣으면 안됨.\n",
    "        \n",
    "        감성 분석을 위한 데이터셋의 경우 sentence + 태그를 본 후 해당 태그에 대한 감성 분석을 기대하므로 pair를 함께 넣어줌.\n",
    "        \"\"\"\n",
    "        \n",
    "        test_ACD_Datas[idx].append(sen)\n",
    "        \n",
    "        for idx3, annotation in enumerate(annos):\n",
    "            entity_property = annotation[0]  # raw_data의 annotation 추출\n",
    "            entity_property = entity_property.replace(\" \", \"\")  # 띄어쓰기 제거\n",
    "            polarity = annotation[2]\n",
    "\n",
    "            if entity_property == pair:\n",
    "                check_point = True\n",
    "                \n",
    "            else:\n",
    "                ;\n",
    "                \n",
    "        if check_point:\n",
    "            test_ACD_labels[idx].append(1)  # 해당 태그 맞음.\n",
    "            test_POL_Datas.append(sen + \" \"+ pair)\n",
    "            test_POL_labels.append(polarity_name_to_id[polarity])\n",
    "        \n",
    "        else:\n",
    "            test_ACD_labels[idx].append(0)\n",
    "            \n",
    "    test_ACD_Datas[idx], test_ACD_labels[idx] = shuffle(test_ACD_Datas[idx], test_ACD_labels[idx], random_state = 42)\n",
    "    \n",
    "test_POL_Datas, test_POL_labels = shuffle(test_POL_Datas, test_POL_labels, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba20d121",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:03:04.785559Z",
     "start_time": "2022-12-10T09:03:04.778238Z"
    }
   },
   "outputs": [],
   "source": [
    "class klue_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, label):  # 전처리된 데이터 셋이 들어옴\n",
    "        self.dataset = dataset\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # getitem이므로 gradient 계산에 영향을 주지 않게 clone().detach() 실행\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.dataset.items()}\n",
    "        item['label'] = torch.tensor(self.label[idx])\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):  # 샘플 수\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45cbb361",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:03:10.037945Z",
     "start_time": "2022-12-10T09:03:05.169908Z"
    }
   },
   "outputs": [],
   "source": [
    "klue_sets = []\n",
    "\n",
    "for i in range(len(rep_entity_property_pair)):\n",
    "    tok_sen = tokenizer(ACD_Datas[i], padding='max_length', return_tensors=\"pt\",\n",
    "                    max_length=256, truncation=True, add_special_tokens=True)  \n",
    "    \n",
    "    klue_sets.append(klue_Dataset(tok_sen, ACD_labels[i]))  # klue_Dataset에서 1차원 텐서로 바뀜.\n",
    "    \n",
    "klue_sets[5].__getitem__(758)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72064403",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:03:11.514225Z",
     "start_time": "2022-12-10T09:03:10.339393Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0, 11962,  2122,  5077, 27135,   677,  1233,  2275,  2205,  2259,\n",
       "          9648,  2643, 15116,  2643,  5576,  2119,  2205,  2321,     2,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'label': tensor(1)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_klue_sets = []\n",
    "\n",
    "for i in range(len(rep_entity_property_pair)):\n",
    "    tok_sen = tokenizer(dev_ACD_Datas[i], padding='max_length', return_tensors=\"pt\",\n",
    "                    max_length=256, truncation=True, add_special_tokens=True)  \n",
    "    \n",
    "    dev_klue_sets.append(klue_Dataset(tok_sen, dev_ACD_labels[i]))  # klue_Dataset에서 1차원 텐서로 바뀜.\n",
    "    \n",
    "dev_klue_sets[5].__getitem__(758)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24b60ec5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:03:14.749118Z",
     "start_time": "2022-12-10T09:03:11.633107Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0, 21253,  2170,  2318,  2457,  2608,  2073,  2311,  2323,  2047,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'label': tensor(0)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_klue_sets = []\n",
    "\n",
    "for i in range(len(rep_entity_property_pair)):\n",
    "    tok_sen = tokenizer(test_ACD_Datas[i], padding='max_length', return_tensors=\"pt\",\n",
    "                    max_length=256, truncation=True, add_special_tokens=True)  \n",
    "    \n",
    "    test_klue_sets.append(klue_Dataset(tok_sen, test_ACD_labels[i]))  # klue_Dataset에서 1차원 텐서로 바뀜.\n",
    "    \n",
    "test_klue_sets[5].__getitem__(758)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51c616e",
   "metadata": {},
   "source": [
    "### Polarity datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "baefe0f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:03:15.151225Z",
     "start_time": "2022-12-10T09:03:14.869653Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0,  4482,  2119,  7693,  2088, 11410,  2073,   543,   554,  2088,\n",
       "           769,  2015,  2119,  1889,  2259,  2284, 32006,     2,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'label': tensor(0)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_tok_sen = tokenizer(POL_Datas, padding='max_length', return_tensors=\"pt\",\n",
    "                    max_length=256, truncation=True, add_special_tokens=True)  \n",
    "\n",
    "POL_klue_sets = klue_Dataset(pol_tok_sen, POL_labels)\n",
    "\n",
    "POL_klue_sets.__getitem__(758)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31799adf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:03:15.382538Z",
     "start_time": "2022-12-10T09:03:15.295637Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0,   831,  2057,  4039,  2052,  7136,  3940, 21820,  1111,  2089,\n",
       "          2094,  2200,  2197, 32012,     2,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'label': tensor(0)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_tok_sen = tokenizer(dev_POL_Datas, padding='max_length', return_tensors=\"pt\",\n",
    "                    max_length=256, truncation=True, add_special_tokens=True)  \n",
    "\n",
    "dev_POL_klue_sets = klue_Dataset(pol_tok_sen, dev_POL_labels)\n",
    "\n",
    "dev_POL_klue_sets.__getitem__(758)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d206a45c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:03:15.732370Z",
     "start_time": "2022-12-10T09:03:15.549406Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0, 11174,  6396,  9989,  2585,  2051,  9497,  2683,  9151, 13937,\n",
       "         30669,   991,  9534,  2612,  2613,  4212,  2223,  2015, 32003,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'label': tensor(0)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_tok_sen = tokenizer(test_POL_Datas, padding='max_length', return_tensors=\"pt\",\n",
    "                    max_length=256, truncation=True, add_special_tokens=True)  \n",
    "\n",
    "test_POL_klue_sets = klue_Dataset(pol_tok_sen, test_POL_labels)\n",
    "\n",
    "test_POL_klue_sets.__getitem__(758)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29884566",
   "metadata": {},
   "source": [
    "## Trainer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd143496",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:03:15.881744Z",
     "start_time": "2022-12-10T09:03:15.878676Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd48e8aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T05:35:35.917041Z",
     "start_time": "2022-12-10T05:13:39.641009Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /home/egg2018037024/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"klue/roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/roberta-large/resolve/main/pytorch_model.bin from cache at /home/egg2018037024/.cache/huggingface/transformers/fd91c85effc137c99cd14cfe5c3459faa223c005b1577dc2c5aa48f6b2c4fbb1.3d5d467e78cd19d9a87029910ed83289edde0111a75a41e0cc79ad3fc06e4a51\n",
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3001\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 th Trarining.... ========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1880' max='7520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1880/7520 21:49 < 1:05:31, 1.43 it/s, Epoch 5/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.026805</td>\n",
       "      <td>0.996420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.074100</td>\n",
       "      <td>0.027728</td>\n",
       "      <td>0.996420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.037000</td>\n",
       "      <td>0.028548</td>\n",
       "      <td>0.996420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.029300</td>\n",
       "      <td>0.027997</td>\n",
       "      <td>0.996420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.029300</td>\n",
       "      <td>0.031160</td>\n",
       "      <td>0.996420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 838\n",
      "  Batch size = 8\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-376\n",
      "Configuration saved in /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-376/config.json\n",
      "Model weights saved in /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-376/pytorch_model.bin\n",
      "tokenizer config file saved in /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-376/tokenizer_config.json\n",
      "Special tokens file saved in /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-376/special_tokens_map.json\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 838\n",
      "  Batch size = 8\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-752\n",
      "Configuration saved in /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-752/config.json\n",
      "Model weights saved in /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-752/pytorch_model.bin\n",
      "tokenizer config file saved in /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-752/tokenizer_config.json\n",
      "Special tokens file saved in /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-752/special_tokens_map.json\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 838\n",
      "  Batch size = 8\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-1128\n",
      "Configuration saved in /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-1128/config.json\n",
      "Model weights saved in /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-1128/pytorch_model.bin\n",
      "tokenizer config file saved in /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-1128/tokenizer_config.json\n",
      "Special tokens file saved in /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-1128/special_tokens_map.json\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 838\n",
      "  Batch size = 8\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-1504\n",
      "Configuration saved in /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-1504/config.json\n",
      "Model weights saved in /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-1504/pytorch_model.bin\n",
      "tokenizer config file saved in /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-1504/tokenizer_config.json\n",
      "Special tokens file saved in /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-1504/special_tokens_map.json\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 838\n",
      "  Batch size = 8\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-1880\n",
      "Configuration saved in /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-1880/config.json\n",
      "Model weights saved in /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-1880/pytorch_model.bin\n",
      "tokenizer config file saved in /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-1880/tokenizer_config.json\n",
      "Special tokens file saved in /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-1880/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /mnt/HDD4T/egg2018037024/ABSA14/checkpoint-376 (score: 0.0).\n",
      "Configuration saved in /mnt/HDD4T/egg2018037024/ABSA14_best/config.json\n",
      "Model weights saved in /mnt/HDD4T/egg2018037024/ABSA14_best/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(rep_entity_property_pair)):\n",
    "    MODEL_NAME = 'klue/roberta-large'\n",
    "    tmp_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "    odir = '/mnt/HDD4T/egg2018037024/ABSA' + str(i)\n",
    "\n",
    "    training_ars = TrainingArguments(\n",
    "    output_dir=odir,\n",
    "    num_train_epochs=20,\n",
    "    #    max_steps=5000,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    save_total_limit=5,\n",
    "    save_strategy = \"epoch\",\n",
    "    #    save_steps=1000,\n",
    "    learning_rate=1e-6,\n",
    "    weight_decay=0.01,\n",
    "    #    eval_steps=1000,\n",
    "    evaluation_strategy='epoch',\n",
    "    metric_for_best_model = 'f1',\n",
    "    load_best_model_at_end = True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "    model=tmp_model,\n",
    "    args=training_ars,\n",
    "    train_dataset=klue_sets[i],\n",
    "    eval_dataset=dev_klue_sets[i],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=4)]\n",
    "    )\n",
    "\n",
    "    print(i, \"th Trarining.... ========================================\")\n",
    "    trainer.train()\n",
    "    tmp_model.save_pretrained(odir + \"_best\")\n",
    "\n",
    "    \n",
    "    ################################## GPU CLEANING #################################\n",
    "    with torch.no_grad(): tmp_model\n",
    "    del tmp_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dabdaf",
   "metadata": {},
   "source": [
    "## Test set 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5780821f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T05:56:08.717659Z",
     "start_time": "2022-12-10T05:50:47.482110Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /mnt/HDD4T/egg2018037024/ABSA0_best/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/mnt/HDD4T/egg2018037024/ABSA0_best\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /mnt/HDD4T/egg2018037024/ABSA0_best/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th Test.... ========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /mnt/HDD4T/egg2018037024/ABSA0_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "100%|█████████████████████████████████████████| 489/489 [00:15<00:00, 30.74it/s]\n",
      "loading configuration file /mnt/HDD4T/egg2018037024/ABSA1_best/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/mnt/HDD4T/egg2018037024/ABSA1_best\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /mnt/HDD4T/egg2018037024/ABSA1_best/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 th Test.... ========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /mnt/HDD4T/egg2018037024/ABSA1_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "100%|█████████████████████████████████████████| 489/489 [00:15<00:00, 30.70it/s]\n",
      "loading configuration file /mnt/HDD4T/egg2018037024/ABSA2_best/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/mnt/HDD4T/egg2018037024/ABSA2_best\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /mnt/HDD4T/egg2018037024/ABSA2_best/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 th Test.... ========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /mnt/HDD4T/egg2018037024/ABSA2_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "100%|█████████████████████████████████████████| 489/489 [00:15<00:00, 30.62it/s]\n",
      "loading configuration file /mnt/HDD4T/egg2018037024/ABSA3_best/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/mnt/HDD4T/egg2018037024/ABSA3_best\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /mnt/HDD4T/egg2018037024/ABSA3_best/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 th Test.... ========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /mnt/HDD4T/egg2018037024/ABSA3_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "100%|█████████████████████████████████████████| 489/489 [00:15<00:00, 30.71it/s]\n",
      "loading configuration file /mnt/HDD4T/egg2018037024/ABSA4_best/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/mnt/HDD4T/egg2018037024/ABSA4_best\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /mnt/HDD4T/egg2018037024/ABSA4_best/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 th Test.... ========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /mnt/HDD4T/egg2018037024/ABSA4_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "100%|█████████████████████████████████████████| 489/489 [00:15<00:00, 30.70it/s]\n",
      "loading configuration file /mnt/HDD4T/egg2018037024/ABSA5_best/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/mnt/HDD4T/egg2018037024/ABSA5_best\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /mnt/HDD4T/egg2018037024/ABSA5_best/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 th Test.... ========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /mnt/HDD4T/egg2018037024/ABSA5_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "100%|█████████████████████████████████████████| 489/489 [00:15<00:00, 30.67it/s]\n",
      "loading configuration file /mnt/HDD4T/egg2018037024/ABSA6_best/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/mnt/HDD4T/egg2018037024/ABSA6_best\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /mnt/HDD4T/egg2018037024/ABSA6_best/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 th Test.... ========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /mnt/HDD4T/egg2018037024/ABSA6_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "100%|█████████████████████████████████████████| 489/489 [00:15<00:00, 30.61it/s]\n",
      "loading configuration file /mnt/HDD4T/egg2018037024/ABSA7_best/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/mnt/HDD4T/egg2018037024/ABSA7_best\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /mnt/HDD4T/egg2018037024/ABSA7_best/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 th Test.... ========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /mnt/HDD4T/egg2018037024/ABSA7_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "100%|█████████████████████████████████████████| 489/489 [00:15<00:00, 30.66it/s]\n",
      "loading configuration file /mnt/HDD4T/egg2018037024/ABSA8_best/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/mnt/HDD4T/egg2018037024/ABSA8_best\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /mnt/HDD4T/egg2018037024/ABSA8_best/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 th Test.... ========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /mnt/HDD4T/egg2018037024/ABSA8_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "100%|█████████████████████████████████████████| 489/489 [00:15<00:00, 30.71it/s]\n",
      "loading configuration file /mnt/HDD4T/egg2018037024/ABSA9_best/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/mnt/HDD4T/egg2018037024/ABSA9_best\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /mnt/HDD4T/egg2018037024/ABSA9_best/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 th Test.... ========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /mnt/HDD4T/egg2018037024/ABSA9_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "100%|█████████████████████████████████████████| 489/489 [00:15<00:00, 30.68it/s]\n",
      "loading configuration file /mnt/HDD4T/egg2018037024/ABSA10_best/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/mnt/HDD4T/egg2018037024/ABSA10_best\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /mnt/HDD4T/egg2018037024/ABSA10_best/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 th Test.... ========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /mnt/HDD4T/egg2018037024/ABSA10_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "100%|█████████████████████████████████████████| 489/489 [00:15<00:00, 30.70it/s]\n",
      "loading configuration file /mnt/HDD4T/egg2018037024/ABSA11_best/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/mnt/HDD4T/egg2018037024/ABSA11_best\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /mnt/HDD4T/egg2018037024/ABSA11_best/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 th Test.... ========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /mnt/HDD4T/egg2018037024/ABSA11_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "100%|█████████████████████████████████████████| 489/489 [00:15<00:00, 30.63it/s]\n",
      "loading configuration file /mnt/HDD4T/egg2018037024/ABSA12_best/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/mnt/HDD4T/egg2018037024/ABSA12_best\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /mnt/HDD4T/egg2018037024/ABSA12_best/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 th Test.... ========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /mnt/HDD4T/egg2018037024/ABSA12_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "100%|█████████████████████████████████████████| 489/489 [00:15<00:00, 30.61it/s]\n",
      "loading configuration file /mnt/HDD4T/egg2018037024/ABSA13_best/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/mnt/HDD4T/egg2018037024/ABSA13_best\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /mnt/HDD4T/egg2018037024/ABSA13_best/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 th Test.... ========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /mnt/HDD4T/egg2018037024/ABSA13_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "100%|█████████████████████████████████████████| 489/489 [00:15<00:00, 30.60it/s]\n",
      "loading configuration file /mnt/HDD4T/egg2018037024/ABSA14_best/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/mnt/HDD4T/egg2018037024/ABSA14_best\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /mnt/HDD4T/egg2018037024/ABSA14_best/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 th Test.... ========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /mnt/HDD4T/egg2018037024/ABSA14_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "100%|█████████████████████████████████████████| 489/489 [00:15<00:00, 30.59it/s]\n",
      "loading configuration file /mnt/HDD4T/egg2018037024/ABSA15_best/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/mnt/HDD4T/egg2018037024/ABSA15_best\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /mnt/HDD4T/egg2018037024/ABSA15_best/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 th Test.... ========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /mnt/HDD4T/egg2018037024/ABSA15_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "100%|█████████████████████████████████████████| 489/489 [00:15<00:00, 30.59it/s]\n",
      "loading configuration file /mnt/HDD4T/egg2018037024/ABSA16_best/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/mnt/HDD4T/egg2018037024/ABSA16_best\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /mnt/HDD4T/egg2018037024/ABSA16_best/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 th Test.... ========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /mnt/HDD4T/egg2018037024/ABSA16_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "100%|█████████████████████████████████████████| 489/489 [00:15<00:00, 30.58it/s]\n",
      "loading configuration file /mnt/HDD4T/egg2018037024/ABSA17_best/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/mnt/HDD4T/egg2018037024/ABSA17_best\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /mnt/HDD4T/egg2018037024/ABSA17_best/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 th Test.... ========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /mnt/HDD4T/egg2018037024/ABSA17_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "100%|█████████████████████████████████████████| 489/489 [00:16<00:00, 30.41it/s]\n"
     ]
    }
   ],
   "source": [
    "infers = [[] for i in range(len(rep_entity_property_pair))]\n",
    "infer_labels = [[] for i in range(len(rep_entity_property_pair))]\n",
    "\n",
    "for i in range(len(rep_entity_property_pair)):\n",
    "    print(i, \"th Test.... ========================================\")\n",
    "    MODEL_NAME = '/mnt/HDD4T/egg2018037024/ABSA' + str(i) + \"_best\"\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "    model.to(device)\n",
    "    dataloader = DataLoader(test_klue_sets[i], batch_size=4, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    output_pred = []\n",
    "    output_prob = []\n",
    "    labels = []\n",
    "\n",
    "    for z, data in enumerate(tqdm(dataloader)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=data['input_ids'].to(device),\n",
    "                attention_mask=data['attention_mask'].to(device),\n",
    "                token_type_ids=data['token_type_ids'].to(device)\n",
    "            )\n",
    "        logits = outputs[0]\n",
    "        prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        result = np.argmax(logits, axis=-1)\n",
    "        labels.append(data['label'].tolist())\n",
    "\n",
    "        output_pred.append(result)\n",
    "        output_prob.append(prob)\n",
    "\n",
    "    pred_answer, output_prob = np.concatenate(output_pred).tolist(), np.concatenate(output_prob, axis=0).tolist()\n",
    "    \n",
    "    infers[i].extend(pred_answer)\n",
    "    infer_labels[i].extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3ca97f5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T06:04:36.191029Z",
     "start_time": "2022-12-10T06:04:36.073830Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th Test.... ========================================\n",
      "1956 1956\n",
      "Accuracy:  0.9008179959100204\n",
      "Precision: 0.43089430894308944\n",
      "Recall: 0.2994350282485876\n",
      "F1-score: 0.3533333333333334\n",
      "1 th Test.... ========================================\n",
      "1956 1956\n",
      "Accuracy:  0.9570552147239264\n",
      "Precision: 0.3763440860215054\n",
      "Recall: 0.5737704918032787\n",
      "F1-score: 0.4545454545454546\n",
      "2 th Test.... ========================================\n",
      "1956 1956\n",
      "Accuracy:  0.9662576687116564\n",
      "Precision: 0.632183908045977\n",
      "Recall: 0.6179775280898876\n",
      "F1-score: 0.625\n",
      "3 th Test.... ========================================\n",
      "1956 1956\n",
      "Accuracy:  0.8629856850715747\n",
      "Precision: 0.7562380038387716\n",
      "Recall: 0.7364485981308411\n",
      "F1-score: 0.7462121212121211\n",
      "4 th Test.... ========================================\n",
      "1956 1956\n",
      "Accuracy:  0.9969325153374233\n",
      "Precision: 0.8918918918918919\n",
      "Recall: 0.9428571428571428\n",
      "F1-score: 0.9166666666666667\n",
      "5 th Test.... ========================================\n",
      "1956 1956\n",
      "Accuracy:  0.9815950920245399\n",
      "Precision: 0.6610169491525424\n",
      "Recall: 0.7090909090909091\n",
      "F1-score: 0.6842105263157895\n",
      "6 th Test.... ========================================\n",
      "1956 1956\n",
      "Accuracy:  0.9826175869120655\n",
      "Precision: 0.64\n",
      "Recall: 0.3902439024390244\n",
      "F1-score: 0.48484848484848486\n",
      "7 th Test.... ========================================\n",
      "1956 1956\n",
      "Accuracy:  0.9928425357873211\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1-score: 0.0\n",
      "8 th Test.... ========================================\n",
      "1956 1956\n",
      "Accuracy:  0.9887525562372188\n",
      "Precision: 0.5\n",
      "Recall: 0.2727272727272727\n",
      "F1-score: 0.3529411764705882\n",
      "9 th Test.... ========================================\n",
      "1956 1956\n",
      "Accuracy:  0.9974437627811861\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1-score: 0.0\n",
      "10 th Test.... ========================================\n",
      "1956 1956\n",
      "Accuracy:  0.9575664621676891\n",
      "Precision: 0.7906976744186046\n",
      "Recall: 0.6455696202531646\n",
      "F1-score: 0.710801393728223\n",
      "11 th Test.... ========================================\n",
      "1956 1956\n",
      "Accuracy:  0.9984662576687117\n",
      "Precision: 1.0\n",
      "Recall: 0.625\n",
      "F1-score: 0.7692307692307693\n",
      "12 th Test.... ========================================\n",
      "1956 1956\n",
      "Accuracy:  0.8542944785276073\n",
      "Precision: 0.7747747747747747\n",
      "Recall: 0.890038809831824\n",
      "F1-score: 0.8284166164960866\n",
      "13 th Test.... ========================================\n",
      "1956 1956\n",
      "Accuracy:  0.9591002044989775\n",
      "Precision: 0.40625\n",
      "Recall: 0.6290322580645161\n",
      "F1-score: 0.49367088607594944\n",
      "14 th Test.... ========================================\n",
      "1956 1956\n",
      "Accuracy:  0.9984662576687117\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1-score: 0.0\n",
      "15 th Test.... ========================================\n",
      "1956 1956\n",
      "Accuracy:  0.9979550102249489\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1-score: 0.0\n",
      "16 th Test.... ========================================\n",
      "1956 1956\n",
      "Accuracy:  0.9851738241308794\n",
      "Precision: 0.6875\n",
      "Recall: 0.3142857142857143\n",
      "F1-score: 0.43137254901960786\n",
      "17 th Test.... ========================================\n",
      "1956 1956\n",
      "Accuracy:  0.9969325153374233\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1-score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/egg2018037024/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for x in range(len(rep_entity_property_pair)):\n",
    "    print(x, \"th Test.... ========================================\")\n",
    "    labelss = []\n",
    "\n",
    "    for i in infer_labels[x]:\n",
    "        for j in i:\n",
    "            labelss.append(j)\n",
    "        \n",
    "    print(len(labelss), len(infers[x]))\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labelss, infers[x], average='binary')\n",
    "    acc = accuracy_score(labelss, infers[x])\n",
    "    \n",
    "    print(\"Accuracy: \", acc)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca3e09b",
   "metadata": {},
   "source": [
    "## Polarity model 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca4a644d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T06:20:55.397260Z",
     "start_time": "2022-12-10T06:20:51.259688Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32018, 1024)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = 'klue/roberta-large'\n",
    "tmp_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)\n",
    "tmp_model.resize_token_embeddings(tokenizer.vocab_size + num_added_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1149013c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T07:22:38.586790Z",
     "start_time": "2022-12-10T07:22:38.580353Z"
    }
   },
   "outputs": [],
   "source": [
    "def pol_compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, \n",
    "                                                               preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7e65c19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T07:56:10.409691Z",
     "start_time": "2022-12-10T07:22:38.800501Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3190\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 th Trarining.... ========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3192' max='7980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3192/7980 33:29 < 50:16, 1.59 it/s, Epoch 8/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.146707</td>\n",
       "      <td>0.962760</td>\n",
       "      <td>0.955389</td>\n",
       "      <td>0.951512</td>\n",
       "      <td>0.962760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.132600</td>\n",
       "      <td>0.155193</td>\n",
       "      <td>0.960570</td>\n",
       "      <td>0.955129</td>\n",
       "      <td>0.952920</td>\n",
       "      <td>0.960570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.189856</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.954783</td>\n",
       "      <td>0.953127</td>\n",
       "      <td>0.963855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.171981</td>\n",
       "      <td>0.967141</td>\n",
       "      <td>0.962345</td>\n",
       "      <td>0.960207</td>\n",
       "      <td>0.967141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.180138</td>\n",
       "      <td>0.959474</td>\n",
       "      <td>0.957866</td>\n",
       "      <td>0.956390</td>\n",
       "      <td>0.959474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.089300</td>\n",
       "      <td>0.216531</td>\n",
       "      <td>0.961665</td>\n",
       "      <td>0.958150</td>\n",
       "      <td>0.955461</td>\n",
       "      <td>0.961665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>0.241342</td>\n",
       "      <td>0.957284</td>\n",
       "      <td>0.955520</td>\n",
       "      <td>0.955818</td>\n",
       "      <td>0.957284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.049700</td>\n",
       "      <td>0.264814</td>\n",
       "      <td>0.958379</td>\n",
       "      <td>0.957013</td>\n",
       "      <td>0.955863</td>\n",
       "      <td>0.958379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 913\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-399\n",
      "Configuration saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-399/config.json\n",
      "Model weights saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-399/pytorch_model.bin\n",
      "tokenizer config file saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-399/tokenizer_config.json\n",
      "Special tokens file saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-399/special_tokens_map.json\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 913\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-798\n",
      "Configuration saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-798/config.json\n",
      "Model weights saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-798/pytorch_model.bin\n",
      "tokenizer config file saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-798/tokenizer_config.json\n",
      "Special tokens file saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-798/special_tokens_map.json\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 913\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-1197\n",
      "Configuration saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-1197/config.json\n",
      "Model weights saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-1197/pytorch_model.bin\n",
      "tokenizer config file saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-1197/tokenizer_config.json\n",
      "Special tokens file saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-1197/special_tokens_map.json\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 913\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-1596\n",
      "Configuration saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-1596/config.json\n",
      "Model weights saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-1596/pytorch_model.bin\n",
      "tokenizer config file saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-1596/tokenizer_config.json\n",
      "Special tokens file saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-1596/special_tokens_map.json\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 913\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-1995\n",
      "Configuration saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-1995/config.json\n",
      "Model weights saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-1995/pytorch_model.bin\n",
      "tokenizer config file saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-1995/tokenizer_config.json\n",
      "Special tokens file saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-1995/special_tokens_map.json\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 913\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-2394\n",
      "Configuration saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-2394/config.json\n",
      "Model weights saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-2394/pytorch_model.bin\n",
      "tokenizer config file saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-2394/tokenizer_config.json\n",
      "Special tokens file saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-2394/special_tokens_map.json\n",
      "Deleting older checkpoint [/mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-399] due to args.save_total_limit\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 913\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-2793\n",
      "Configuration saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-2793/config.json\n",
      "Model weights saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-2793/pytorch_model.bin\n",
      "tokenizer config file saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-2793/tokenizer_config.json\n",
      "Special tokens file saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-2793/special_tokens_map.json\n",
      "Deleting older checkpoint [/mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-798] due to args.save_total_limit\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 913\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-3192\n",
      "Configuration saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-3192/config.json\n",
      "Model weights saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-3192/pytorch_model.bin\n",
      "tokenizer config file saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-3192/tokenizer_config.json\n",
      "Special tokens file saved in /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-3192/special_tokens_map.json\n",
      "Deleting older checkpoint [/mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-1197] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /mnt/HDD4T/egg2018037024/ABSA_pol/checkpoint-1596 (score: 0.9623447766394697).\n",
      "Configuration saved in /mnt/HDD4T/egg2018037024/ABSA_pol_best/config.json\n",
      "Model weights saved in /mnt/HDD4T/egg2018037024/ABSA_pol_best/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "odir = '/mnt/HDD4T/egg2018037024/ABSA_pol'\n",
    "\n",
    "training_ars = TrainingArguments(\n",
    "output_dir=odir,\n",
    "num_train_epochs=20,\n",
    "#    max_steps=5000,\n",
    "per_device_train_batch_size=4,\n",
    "per_device_eval_batch_size=4,\n",
    "save_total_limit=5,\n",
    "save_strategy = \"epoch\",\n",
    "#    save_steps=1000,\n",
    "learning_rate=1e-6,\n",
    "weight_decay=0.01,\n",
    "#    eval_steps=1000,\n",
    "evaluation_strategy='epoch',\n",
    "metric_for_best_model = 'f1',\n",
    "load_best_model_at_end = True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "model=tmp_model,\n",
    "args=training_ars,\n",
    "train_dataset=POL_klue_sets,\n",
    "eval_dataset=dev_POL_klue_sets,\n",
    "tokenizer=tokenizer,\n",
    "compute_metrics=pol_compute_metrics,\n",
    "callbacks = [EarlyStoppingCallback(early_stopping_patience=4)]\n",
    ")\n",
    "\n",
    "print(i, \"th Trarining.... ========================================\")\n",
    "trainer.train()\n",
    "tmp_model.save_pretrained(odir + \"_best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa9d7a",
   "metadata": {},
   "source": [
    "### polarity 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27547788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:03:40.391971Z",
     "start_time": "2022-12-10T09:03:18.163519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 th Test.... ========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 521/521 [00:16<00:00, 30.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[0.9996291399002075, 5.441081157187e-05, 0.00031632857280783355], [0.9995307922363281, 5.168029747437686e-05, 0.0004174570494797081], [0.9995740056037903, 6.222449155757204e-05, 0.00036373420152813196], [0.9995310306549072, 5.295513983583078e-05, 0.00041611489723436534], [0.9989759922027588, 0.00014646428462583572, 0.000877557962667197], [0.9996259212493896, 5.717257590731606e-05, 0.00031680637039244175], [0.9995067119598389, 5.6221546401502565e-05, 0.00043710230966098607], [0.9994507431983948, 6.985832442296669e-05, 0.0004794518172275275], [0.9996323585510254, 5.64371730433777e-05, 0.000311128213070333], [0.99915611743927, 7.948405982460827e-05, 0.0007644173456355929], [0.9996616840362549, 6.44526444375515e-05, 0.0002738951297942549], [0.999583899974823, 5.63515059184283e-05, 0.00035971892066299915], [0.9994439482688904, 6.338992534438148e-05, 0.0004926378605887294], [0.9996309280395508, 5.251342736301012e-05, 0.00031648180447518826], [0.9996414184570312, 5.997564585413784e-05, 0.00029868920682929456], [0.999251663684845, 7.755935803288594e-05, 0.0006707665743306279], [0.9995599389076233, 6.192886212375015e-05, 0.00037807883927598596], [0.9995493292808533, 5.0018254114547744e-05, 0.00040058206650428474], [0.9990022778511047, 0.00010026118980022147, 0.0008974639931693673], [0.9996424913406372, 5.871104804100469e-05, 0.00029876615735702217], [0.9994534850120544, 5.732326826546341e-05, 0.0004891576245427132], [0.9995830655097961, 5.5502154282294214e-05, 0.0003614183224271983], [0.999512791633606, 7.091202132869512e-05, 0.00041633655200712383], [0.9981353282928467, 0.00017224533075932413, 0.0016924025258049369], [0.9974194765090942, 0.00014273289707489312, 0.0024378143716603518], [0.9995924830436707, 5.9070796851301566e-05, 0.00034847186179831624], [0.9993768334388733, 6.642190419370309e-05, 0.0005567753105424345], [0.999526858329773, 5.164077811059542e-05, 0.0004216210509184748], [0.9991082549095154, 7.438973261741921e-05, 0.0008173227543011308], [0.9991962313652039, 5.366627738112584e-05, 0.0007500327774323523], [0.9996008276939392, 5.179726213100366e-05, 0.0003473448450677097], [0.9995288848876953, 6.307852163445204e-05, 0.00040805063326843083], [0.9996302127838135, 6.348214810714126e-05, 0.0003061967727262527], [0.9996015429496765, 5.587435589404777e-05, 0.00034263019915670156], [0.9995939135551453, 5.358268026611768e-05, 0.0003524457279127091], [0.9992188215255737, 5.471508120535873e-05, 0.0007264778832904994], [0.9995935559272766, 6.689788278890774e-05, 0.00033962406450882554], [0.9995811581611633, 5.474828867590986e-05, 0.0003641058865468949], [0.9994761347770691, 7.558938523288816e-05, 0.0004483426455408335], [0.9995330572128296, 6.250099249882624e-05, 0.0004044261295348406], [0.9996188879013062, 6.067633876227774e-05, 0.0003204877721145749], [0.9996490478515625, 6.04436791036278e-05, 0.00029051367891952395], [0.9996398687362671, 6.12736475886777e-05, 0.000298928382107988], [0.999640703201294, 5.40593609912321e-05, 0.00030524306930601597], [0.9991546869277954, 7.06531063769944e-05, 0.0007747337222099304], [0.9995023012161255, 4.862820787820965e-05, 0.0004491206491366029], [0.9993321299552917, 6.418993143597618e-05, 0.0006036292179487646], [0.9978901743888855, 0.0001367598306387663, 0.001973034581169486], [0.9996333122253418, 6.455317634390667e-05, 0.0003020620788447559], [0.9992715716362, 6.889750511618331e-05, 0.0006595575832761824], [0.9996365308761597, 5.8852303482126445e-05, 0.00030463343136943877], [0.9993871450424194, 6.443505844799802e-05, 0.0005484047578647733], [0.9996041655540466, 4.97870278195478e-05, 0.00034601392690092325], [0.9995632767677307, 6.336256046779454e-05, 0.0003733476041816175], [0.9985889792442322, 0.0002122118603438139, 0.0011987158795818686], [0.999213695526123, 7.226447633001953e-05, 0.0007139896624721587], [0.99935382604599, 5.123610026203096e-05, 0.0005949996411800385], [0.9996035695075989, 5.871122266398743e-05, 0.0003376497479621321], [0.9993603825569153, 7.141492096707225e-05, 0.0005682313931174576], [0.9995520710945129, 6.010145807522349e-05, 0.0003878016723319888], [0.9993311166763306, 5.370439976104535e-05, 0.0006152171408757567], [0.9995336532592773, 6.647333066212013e-05, 0.0003998715546913445], [0.9981157779693604, 0.00012675531615968794, 0.0017575416713953018], [0.9996442794799805, 5.959910049568862e-05, 0.0002961404388770461], [0.9996356964111328, 5.899284951738082e-05, 0.00030524563044309616], [0.9994902610778809, 5.458785381051712e-05, 0.00045512442011386156], [0.999370276927948, 6.35469114058651e-05, 0.0005662232870236039], [0.9994937181472778, 5.234722630120814e-05, 0.00045393372420221567], [0.9994263648986816, 5.9941186918877065e-05, 0.0005136817926540971], [0.9995972514152527, 5.6035751185845584e-05, 0.00034677828080020845], [0.9995843768119812, 7.423662464134395e-05, 0.00034144968958571553], [0.9988263249397278, 0.00012159487960161641, 0.0010520766954869032], [0.9994031190872192, 5.5994209105847403e-05, 0.0005409462610259652], [0.9995288848876953, 5.413383769337088e-05, 0.00041709173819981515], [0.9993027448654175, 5.3699804993811995e-05, 0.0006435743998736143], [0.9996387958526611, 5.837144999532029e-05, 0.00030278536723926663], [0.9995294809341431, 5.094377411296591e-05, 0.00041949047590605915], [0.9995429515838623, 5.602875171462074e-05, 0.0004010284028481692], [0.9996063113212585, 6.162095814943314e-05, 0.0003320774994790554], [0.9995195865631104, 5.166424671187997e-05, 0.00042866822332143784], [0.9996122717857361, 5.5069427617127076e-05, 0.000332759169396013], [0.9995412826538086, 5.1732124120462686e-05, 0.0004070086870342493], [0.9994638562202454, 5.282316851662472e-05, 0.0004833955317735672], [0.9996013045310974, 5.5256525229196995e-05, 0.0003434243844822049], [0.9996322393417358, 6.027067138347775e-05, 0.00030750283622182906], [0.9995841383934021, 5.3599069360643625e-05, 0.00036222650669515133], [0.9995540976524353, 5.189743751543574e-05, 0.0003938970621675253], [0.9991033673286438, 0.00010214745270786807, 0.0007944012177176774], [0.9994049072265625, 5.8413799706613645e-05, 0.000536764448042959], [0.9993588328361511, 6.594242586288601e-05, 0.0005752053111791611], [0.999614953994751, 5.130219869897701e-05, 0.0003336580703034997], [0.9994999170303345, 5.8331788750365376e-05, 0.0004416308947838843], [0.9996346235275269, 6.509381637442857e-05, 0.0003003418678417802], [0.9994779229164124, 6.65275365463458e-05, 0.0004555936611723155], [0.9991099238395691, 6.715046765748411e-05, 0.0008229288505390286], [0.9994283318519592, 6.783280696254224e-05, 0.0005039043608121574], [0.9993141889572144, 5.3981693781679496e-05, 0.0006318290834315121], [0.9996318817138672, 5.5754848290234804e-05, 0.00031234370544552803], [0.9996384382247925, 5.922457421547733e-05, 0.00030237575992941856], [0.9996011853218079, 5.108156256028451e-05, 0.00034777220571413636], [0.9956375956535339, 0.00029199098935350776, 0.004070480354130268], [0.9994614720344543, 6.177229079185054e-05, 0.00047675950918346643], [0.9995624423027039, 6.562852649949491e-05, 0.0003718977968674153], [0.9989591836929321, 7.18546798452735e-05, 0.000968942535109818], [0.9995560050010681, 5.2911338570993394e-05, 0.0003911207022611052], [0.9996694326400757, 6.311085599008948e-05, 0.0002674472925718874], [0.9994173049926758, 5.7947563618654385e-05, 0.0005247650551609695], [0.9995785355567932, 5.1313247240614146e-05, 0.0003700527304317802], [0.9995081424713135, 4.8143014282686636e-05, 0.0004437145544216037], [0.9995960593223572, 5.6111617595888674e-05, 0.00034777409746311605], [0.999643087387085, 5.820643491460942e-05, 0.00029878399800509214], [0.9994803071022034, 5.4502157581737265e-05, 0.0004653021751437336], [0.9995813965797424, 5.404589683166705e-05, 0.00036453906795941293], [0.9996021389961243, 5.8270306908525527e-05, 0.0003396031679585576], [0.9994927644729614, 6.059442966943607e-05, 0.0004465336096473038], [0.9958051443099976, 0.0002714841393753886, 0.0039232936687767506], [0.9994901418685913, 5.682661503669806e-05, 0.00045297553879208863], [0.9986525177955627, 9.76979426923208e-05, 0.0012497968273237348], [0.9995976090431213, 5.662243711412884e-05, 0.0003458646824583411], [0.9996519088745117, 5.956818858976476e-05, 0.0002885281865019351], [0.9995754361152649, 4.894293670076877e-05, 0.00037553723086602986], [0.999497652053833, 5.9470909036463127e-05, 0.00044289985089562833], [0.9996414184570312, 6.525225035147741e-05, 0.0002932300849352032], [0.9993521571159363, 6.03434928052593e-05, 0.0005874820635654032], [0.9995855689048767, 5.492455966304988e-05, 0.00035946781281381845], [0.9995704293251038, 5.346422767615877e-05, 0.0003760937543120235], [0.9995379447937012, 6.645593384746462e-05, 0.00039556133560836315], [0.9996063113212585, 6.249913712963462e-05, 0.000331270246533677], [0.9996379613876343, 6.0469930758699775e-05, 0.0003016581467818469], [0.9990037083625793, 7.225741137517616e-05, 0.000923967338167131], [0.9994581341743469, 5.1213890401413664e-05, 0.0004906460526399314], [0.995309054851532, 0.000263182504568249, 0.00442779716104269], [0.9992536902427673, 5.478288949234411e-05, 0.0006914594559930265], [0.9995754361152649, 5.739416155847721e-05, 0.00036714240559376776], [0.9995366334915161, 5.9689882618840784e-05, 0.0004037202743347734], [0.9995403289794922, 5.1164202886866406e-05, 0.00040863361209630966], [0.9995212554931641, 5.699631219613366e-05, 0.00042181499884463847], [0.9993560910224915, 6.0195496189408004e-05, 0.0005837945500388741], [0.9995662569999695, 5.2227656851755455e-05, 0.0003814866649918258], [0.9996572732925415, 5.97475009271875e-05, 0.00028297066455706954], [0.9995218515396118, 5.090171907795593e-05, 0.00042722278158180416], [0.9993975162506104, 6.330324686132371e-05, 0.0005392287857830524], [0.9995158910751343, 5.2201765356585383e-05, 0.00043189930147491395], [0.999528169631958, 4.741642624139786e-05, 0.0004243869916535914], [0.9987592697143555, 0.0001034443048411049, 0.0011372787412256002], [0.9996101260185242, 6.0545684391399845e-05, 0.00032940463279373944], [0.9996110796928406, 6.063385808374733e-05, 0.0003282675170339644], [0.9995201826095581, 5.694941137335263e-05, 0.0004229264741297811], [0.9995951056480408, 4.947754860040732e-05, 0.0003555079747457057], [0.009502165950834751, 0.9657567143440247, 0.02474110573530197], [0.9996308088302612, 5.412645623437129e-05, 0.0003150501870550215], [0.9995280504226685, 5.608180435956456e-05, 0.00041588296880945563], [0.9995890259742737, 5.8775749494088814e-05, 0.0003522054466884583], [0.999605119228363, 5.352531297830865e-05, 0.00034141624928452075], [0.998654842376709, 0.00010430140537209809, 0.0012408095644786954], [0.9996299743652344, 5.9893853176617995e-05, 0.00031008239602670074], [0.9995237588882446, 8.405290282098576e-05, 0.0003922252799384296], [0.9995226860046387, 6.193671288201585e-05, 0.00041538066579960287], [0.9962730407714844, 0.00022469992109108716, 0.0035023100208491087], [0.9994407296180725, 5.126347969053313e-05, 0.0005080382106825709], [0.9993889331817627, 6.0679041780531406e-05, 0.0005503788124769926], [0.9995385408401489, 5.3880336054135114e-05, 0.0004076185869053006], [0.9986934065818787, 7.570505840703845e-05, 0.0012309143785387278], [0.9993941783905029, 6.25587344984524e-05, 0.0005432357429526746], [0.9989650249481201, 8.115549280773848e-05, 0.0009538673330098391], [0.9996016621589661, 6.227656558621675e-05, 0.00033605602220632136], [0.9994569420814514, 4.830919351661578e-05, 0.0004948325804434717], [0.9992544054985046, 7.70496335462667e-05, 0.0006685042753815651], [0.9985823631286621, 0.00010756312258308753, 0.001310088555328548], [0.9994670748710632, 7.435189036186785e-05, 0.0004585681308526546], [0.9993941783905029, 6.776997179258615e-05, 0.0005380073562264442], [0.9996598958969116, 6.782044511055574e-05, 0.00027230539126321673], [0.99954754114151, 4.760810770676471e-05, 0.0004047421971336007], [0.9992445707321167, 7.64816504670307e-05, 0.0006789368926547468], [0.9996504783630371, 5.978636545478366e-05, 0.0002896584046538919], [0.9995273351669312, 5.5722710385452956e-05, 0.00041689843055792153], [0.9995880722999573, 5.739297193940729e-05, 0.00035448945709504187], [0.9995054006576538, 5.4672244004905224e-05, 0.0004399081808514893], [0.9996485710144043, 6.08747432124801e-05, 0.0002904922002926469], [0.9996261596679688, 6.719676457578316e-05, 0.0003065993369091302], [0.9992082715034485, 6.228090933291242e-05, 0.0007294425740838051], [0.9993384480476379, 5.5777727538952604e-05, 0.0006057488499209285], [0.9995661377906799, 6.286976713454351e-05, 0.0003709059674292803], [0.999158501625061, 8.193168469006196e-05, 0.0007595891947858036], [0.9996309280395508, 6.533525447594002e-05, 0.0003037617134395987], [0.9992035031318665, 6.855044193798676e-05, 0.0007279651472344995], [0.999455988407135, 5.125658572069369e-05, 0.0004928242997266352], [0.9996554851531982, 6.352584750857204e-05, 0.0002810249279718846], [0.9996387958526611, 6.117876182543114e-05, 0.00030008741305209696], [0.9994012117385864, 5.318997864378616e-05, 0.0005457101506181061], [0.9993687272071838, 5.765474634245038e-05, 0.0005736426101066172], [0.9996398687362671, 5.933835564064793e-05, 0.0003007828490808606], [0.9996316432952881, 6.94309055688791e-05, 0.00029882104718126357], [0.9996362924575806, 5.7135290262522176e-05, 0.0003065992204938084], [0.9995952248573303, 6.567644595634192e-05, 0.0003391831705812365], [0.99959796667099, 5.8416175306774676e-05, 0.00034361029975116253], [0.9996283054351807, 6.138254684628919e-05, 0.00031031057005748153], [0.9995278120040894, 5.65146365261171e-05, 0.0004156750801485032], [0.9995189905166626, 4.673142757383175e-05, 0.0004343472246546298], [0.9996415376663208, 5.874840644537471e-05, 0.0002996978582814336], [0.9996376037597656, 5.9852223785128444e-05, 0.0003026178455911577], [0.9995874762535095, 5.9299887652741745e-05, 0.0003531658439897001], [0.9994076490402222, 5.428137592389248e-05, 0.0005380158545449376], [0.99960857629776, 5.122321090311743e-05, 0.0003401744179427624], [0.9995686411857605, 5.326631435309537e-05, 0.000378125550923869], [0.9996316432952881, 5.54700854991097e-05, 0.0003128966491203755], [0.9996055960655212, 5.842525570187718e-05, 0.0003360949922353029], [0.9995192289352417, 5.434623017208651e-05, 0.00042648889939300716], [0.9995941519737244, 6.0850416048197076e-05, 0.00034498245804570615], [0.9994969367980957, 6.609156844206154e-05, 0.00043691214523278177], [0.9995765089988708, 5.1568538765423e-05, 0.00037193280877545476], [0.9996252059936523, 6.002050940878689e-05, 0.0003146571689285338], [0.9995427131652832, 6.37709308648482e-05, 0.00039355442277155817], [0.999591052532196, 6.548766396008432e-05, 0.0003433776437304914], [0.9989977478981018, 5.850075467606075e-05, 0.0009438121924176812], [0.9996390342712402, 6.273796316236258e-05, 0.00029828623519279063], [0.9993910789489746, 8.419858204433694e-05, 0.0005246418877504766], [0.9996142387390137, 6.190264684846625e-05, 0.00032394140725955367], [0.9994750618934631, 5.821862941957079e-05, 0.0004666949971579015], [0.9996047616004944, 5.560200588661246e-05, 0.00033956021070480347], [0.9996047616004944, 5.5039607104845345e-05, 0.0003401544818188995], [0.996027946472168, 0.00022128991258796304, 0.003750751493498683], [0.9994433522224426, 5.9652167692547664e-05, 0.0004970028530806303], [0.9995481371879578, 5.507219975697808e-05, 0.0003967692900914699], [0.9996297359466553, 5.36023399035912e-05, 0.0003165819507557899], [0.9995989203453064, 5.5453569075325504e-05, 0.0003457106649875641], [0.9994779229164124, 5.207215144764632e-05, 0.0004700642602983862], [0.9995155334472656, 5.976056490908377e-05, 0.00042470794869586825], [0.9995427131652832, 5.9264839364914224e-05, 0.000398077885620296], [0.9955952763557434, 0.00016054848674684763, 0.004244137089699507], [0.9992934465408325, 5.771982978330925e-05, 0.0006487706559710205], [0.9995612502098083, 5.043269629823044e-05, 0.0003883829922415316], [0.9996232986450195, 5.4036823712522164e-05, 0.00032265184563584626], [0.9995923638343811, 5.775295721832663e-05, 0.0003498555743135512], [0.9996001124382019, 5.4844229453010485e-05, 0.00034506365773268044], [0.9996262788772583, 5.721841807826422e-05, 0.00031647339346818626], [0.9920642971992493, 0.0005302854697220027, 0.007405440788716078], [0.9991052746772766, 6.456665141740814e-05, 0.000830183329526335], [0.9996201992034912, 5.612946188193746e-05, 0.000323703046888113], [0.9996311664581299, 7.073177403071895e-05, 0.000297978229355067], [0.9995971322059631, 5.476357910083607e-05, 0.0003481009625829756], [0.9993626475334167, 5.69824842386879e-05, 0.0005803683307021856], [0.9978973865509033, 0.00010608236334519461, 0.0019965008832514286], [0.9995104074478149, 5.27505690115504e-05, 0.00043682262185029685], [0.9995905756950378, 5.832828537677415e-05, 0.0003511121904011816], [0.9994255304336548, 5.2767220040550455e-05, 0.0005217482103034854], [0.9994644522666931, 6.647183909080923e-05, 0.00046908907825127244], [0.9996351003646851, 6.0782957007177174e-05, 0.00030406235600821674], [0.9995285272598267, 6.357050006045029e-05, 0.0004078343918081373], [0.9992331266403198, 6.661495717708021e-05, 0.0007002585916779935], [0.9996126294136047, 5.716467057936825e-05, 0.0003302298719063401], [0.9994195699691772, 5.049965693615377e-05, 0.0005299784243106842], [0.9995620846748352, 5.9446105296956375e-05, 0.0003784127766266465], [0.9995500445365906, 5.410939047578722e-05, 0.00039585671038366854], [0.999554455280304, 5.4270560212899e-05, 0.00039127905620262027], [0.9996178150177002, 5.252866685623303e-05, 0.00032961773104034364], [0.9996341466903687, 5.416516069089994e-05, 0.00031165083055384457], [0.9992361068725586, 5.971788414171897e-05, 0.0007042501820251346], [0.999458372592926, 6.986738299019635e-05, 0.000471802573883906], [0.9993014335632324, 5.806414264952764e-05, 0.0006405987660400569], [0.9995241165161133, 6.344001303659752e-05, 0.0004124655097257346], [0.9996147155761719, 6.71500019961968e-05, 0.0003179902851115912], [0.9995204210281372, 6.250234582694247e-05, 0.00041702520684339106], [0.9994927644729614, 5.5927928769961e-05, 0.00045128667261451483], [0.9993707537651062, 7.528372952947393e-05, 0.0005539762205444276], [0.999553382396698, 8.162435551639646e-05, 0.0003649501013569534], [0.9995135068893433, 5.154965401743539e-05, 0.00043496061698533595], [0.9978618025779724, 0.00012239192437846214, 0.0020158407278358936], [0.9994344115257263, 5.034124478697777e-05, 0.0005151196382939816], [0.9994717240333557, 6.148148531792685e-05, 0.00046676996862515807], [0.9992666840553284, 7.992173050297424e-05, 0.000653398223221302], [0.9996463060379028, 5.588842395809479e-05, 0.0002977898984681815], [0.9992498755455017, 6.927001959411427e-05, 0.0006807594909332693], [0.9996330738067627, 5.3749015933135524e-05, 0.00031314906664192677], [0.9993769526481628, 7.147695578169078e-05, 0.000551587319932878], [0.9995610117912292, 4.8780024371808395e-05, 0.00039024074794724584], [0.9996116757392883, 5.417923239292577e-05, 0.00033414672361686826], [0.9996047616004944, 5.438297375803813e-05, 0.0003408897027838975], [0.9995301961898804, 4.8647929361322895e-05, 0.0004211623454466462], [0.9995543360710144, 6.321279215626419e-05, 0.00038237430271692574], [0.9995854496955872, 4.8763260565465316e-05, 0.00036578343133442104], [0.9993119239807129, 5.901571057620458e-05, 0.0006290532182902098], [0.9995976090431213, 6.711913738399744e-05, 0.0003352822095621377], [0.9996218681335449, 5.3352730901679024e-05, 0.0003247338754590601], [0.99951171875, 6.790992483729497e-05, 0.0004203109419904649], [0.9994611144065857, 6.584133370779455e-05, 0.00047295872354879975], [0.9995902180671692, 5.0754922995110974e-05, 0.0003590573032852262], [0.9995321035385132, 7.512782030971721e-05, 0.0003927559591829777], [0.9996044039726257, 5.192113894736394e-05, 0.0003437220002524555], [0.9996065497398376, 5.774099554400891e-05, 0.0003356289817020297], [0.9989182949066162, 0.00010620112152537331, 0.0009755209903232753], [0.9996252059936523, 6.458892312366515e-05, 0.0003101847250945866], [0.9978765249252319, 0.00012941713794134557, 0.0019940948113799095], [0.9996448755264282, 6.297619256656617e-05, 0.0002922545245382935], [0.9995130300521851, 5.2363015129230917e-05, 0.0004346694622654468], [0.9994211196899414, 6.182802462717518e-05, 0.0005170144140720367], [0.9996097683906555, 6.041052984073758e-05, 0.00032982637640088797], [0.9995729327201843, 6.721286626998335e-05, 0.000359834375558421], [0.9995513558387756, 6.853148806840181e-05, 0.0003801276907324791], [0.999521017074585, 5.743456131312996e-05, 0.00042155207484029233], [0.9996139407157898, 5.5850592616479844e-05, 0.00033015658846125007], [0.999538779258728, 5.594885442405939e-05, 0.00040527398232370615], [0.9995094537734985, 5.220347156864591e-05, 0.00043840977014042437], [0.9791602492332458, 0.001542428508400917, 0.019297271966934204], [0.9989457726478577, 7.973940955707803e-05, 0.0009744376875460148], [0.9995703101158142, 5.163416062714532e-05, 0.0003780582337640226], [0.9995915293693542, 5.3790419769939035e-05, 0.00035473419120535254], [0.9995237588882446, 5.624964978778735e-05, 0.00042000188841484487], [0.9996477365493774, 5.983400114928372e-05, 0.0002924616856034845], [0.9995113611221313, 5.2799394325120375e-05, 0.0004358168807812035], [0.9994452595710754, 5.9119633078807965e-05, 0.000495647662319243], [0.9996278285980225, 5.108755067340098e-05, 0.0003210299473721534], [0.9995487332344055, 5.141732981428504e-05, 0.0003997599706053734], [0.9994140863418579, 7.526645640609786e-05, 0.0005106486496515572], [0.9988478422164917, 8.963797881733626e-05, 0.001062501105479896], [0.9994304776191711, 6.436293188016862e-05, 0.0005051726475358009], [0.9993306398391724, 6.21964136371389e-05, 0.0006072044488973916], [0.9995684027671814, 5.507142122951336e-05, 0.0003764898865483701], [0.9994250535964966, 5.898789822822437e-05, 0.0005159479333087802], [0.9994215965270996, 6.611785647692159e-05, 0.0005122667644172907], [0.999387264251709, 4.940394501318224e-05, 0.0005633094115182757], [0.9992812275886536, 6.670029688393697e-05, 0.0006520314491353929], [0.9995288848876953, 5.560817589866929e-05, 0.0004154781636316329], [0.988274335861206, 0.000704706646502018, 0.011020859703421593], [0.9995830655097961, 7.340117736021057e-05, 0.0003435871622059494], [0.9995300769805908, 6.055174890207127e-05, 0.0004094296891707927], [0.9995695948600769, 5.8259349316358566e-05, 0.00037215821794234216], [0.9988202452659607, 7.272581569850445e-05, 0.0011070173932239413], [0.9996280670166016, 5.652478284901008e-05, 0.00031541369389742613], [0.9995176792144775, 6.263153773033991e-05, 0.0004196029622107744], [0.9991624355316162, 6.420069985324517e-05, 0.0007733835955150425], [0.9994671940803528, 5.385388794820756e-05, 0.00047892730799503624], [0.9995805621147156, 5.21303663845174e-05, 0.0003673537285067141], [0.9996088147163391, 5.5415992392227054e-05, 0.00033579685259610415], [0.9993686079978943, 7.059447671053931e-05, 0.0005607475177384913], [0.9995898604393005, 5.61260458198376e-05, 0.00035396753810346127], [0.9996365308761597, 6.18611229583621e-05, 0.00030160191818140447], [0.9993560910224915, 7.363344047917053e-05, 0.0005702856578864157], [0.9994356036186218, 5.069512189948e-05, 0.0005135948886163533], [0.999625563621521, 5.2857070841128007e-05, 0.00032153481151908636], [0.9994496703147888, 5.0646354793570936e-05, 0.0004997343057766557], [0.9995436072349548, 5.4296735470416024e-05, 0.0004020154883619398], [0.9996349811553955, 7.315319817280397e-05, 0.00029181604622863233], [0.9995747208595276, 5.41427762072999e-05, 0.00037122960202395916], [0.9995905756950378, 5.23696253367234e-05, 0.00035710647352971137], [0.999636173248291, 6.541453331010416e-05, 0.00029835596797056496], [0.9996399879455566, 6.143409700598568e-05, 0.000298492843285203], [0.999546229839325, 7.066744728945196e-05, 0.00038306653732433915], [0.9995442032814026, 5.188583963899873e-05, 0.00040388951310887933], [0.9991828799247742, 6.745146674802527e-05, 0.0007497392361983657], [0.9996397495269775, 6.324948481051251e-05, 0.0002969958004541695], [0.9995207786560059, 5.772354415967129e-05, 0.00042150335502810776], [0.9955680966377258, 0.00032021990045905113, 0.004111643880605698], [0.9996185302734375, 5.7783447118708864e-05, 0.000323651620419696], [0.999556839466095, 5.467432856676169e-05, 0.0003884561010636389], [0.9995438456535339, 5.473580677062273e-05, 0.0004014300066046417], [0.9995611310005188, 5.129532655701041e-05, 0.0003876761475112289], [0.9995606541633606, 5.768254050053656e-05, 0.00038163576391525567], [0.9995056390762329, 5.608578794635832e-05, 0.00043826052569784224], [0.9994295239448547, 6.488717190222815e-05, 0.0005055434885434806], [0.9996076226234436, 6.149716500658542e-05, 0.00033096186234615743], [0.9994526505470276, 5.069376857136376e-05, 0.0004967512795701623], [0.9995463490486145, 6.019897773512639e-05, 0.00039348253631033003], [0.9993851184844971, 6.16807519691065e-05, 0.0005530891357921064], [0.9995799660682678, 5.318843250279315e-05, 0.0003668554709292948], [0.999566376209259, 6.259888323256746e-05, 0.0003710812015924603], [0.9996217489242554, 5.6036275054793805e-05, 0.0003222196246497333], [0.9984561204910278, 8.34031161502935e-05, 0.001460414845496416], [0.9995402097702026, 4.912638905807398e-05, 0.0004107614222448319], [0.9995656609535217, 6.354497600113973e-05, 0.00037076626904308796], [0.9995331764221191, 5.686082658939995e-05, 0.00040996132884174585], [0.9995940327644348, 7.292361260624602e-05, 0.00033306103432551026], [0.9995501637458801, 6.17562182014808e-05, 0.00038807265809737146], [0.9995525479316711, 5.9537909692153335e-05, 0.000387833482818678], [0.9995712637901306, 5.389130092225969e-05, 0.00037492180126719177], [0.9996293783187866, 6.369422771967947e-05, 0.0003068741352763027], [0.9995274543762207, 5.154693280928768e-05, 0.0004209580074530095], [0.9992905855178833, 5.629233055515215e-05, 0.0006530052633024752], [0.9991869330406189, 0.00010211655171588063, 0.0007109995349310338], [0.9994468092918396, 5.268461973173544e-05, 0.0005004719132557511], [0.9995377063751221, 5.120707646710798e-05, 0.0004110607551410794], [0.9994592070579529, 5.7768284023040906e-05, 0.00048300184425897896], [0.9996168613433838, 5.9381993196439e-05, 0.00032371649285778403], [0.9994016885757446, 6.198103073984385e-05, 0.0005364100798033178], [0.9995670914649963, 7.009464025031775e-05, 0.0003628454578574747], [0.9994412064552307, 5.908833918510936e-05, 0.0004997029318474233], [0.999354898929596, 5.778181730420329e-05, 0.0005873237387277186], [0.5871673226356506, 0.044829752296209335, 0.3680029809474945], [0.999394416809082, 6.5038533648476e-05, 0.0005405694828368723], [0.9995531439781189, 5.644615521305241e-05, 0.0003903871402144432], [0.9995007514953613, 5.140241046319716e-05, 0.0004478374612517655], [0.8490867614746094, 0.009756520390510559, 0.14115668833255768], [0.9993476271629333, 5.9590624005068094e-05, 0.0005928546306677163], [0.9995121955871582, 7.399928290396929e-05, 0.0004138100484851748], [0.9996398687362671, 5.9741105360444635e-05, 0.0003004155878443271], [0.999268114566803, 6.359165126923472e-05, 0.0006683202227577567], [0.9991980195045471, 5.9951558796456084e-05, 0.0007421175250783563], [0.9994451403617859, 7.899010961409658e-05, 0.00047585778520442545], [0.9996334314346313, 6.786679296055809e-05, 0.00029865148826502264], [0.9996465444564819, 7.046523387543857e-05, 0.0002830353332683444], [0.9996446371078491, 6.17621117271483e-05, 0.0002936465898528695], [0.9995712637901306, 5.4149877541931346e-05, 0.00037465820787474513], [0.9994046688079834, 5.5056731071090326e-05, 0.000540327571798116], [0.9996280670166016, 6.4567917434033e-05, 0.00030729107675142586], [0.9996329545974731, 5.382739254855551e-05, 0.0003131167613901198], [0.9994439482688904, 5.985598545521498e-05, 0.0004962074453942478], [0.9995550513267517, 6.297936488408595e-05, 0.0003818454861175269], [0.9992726445198059, 6.230108556337655e-05, 0.00066511140903458], [0.9995947480201721, 5.638006769004278e-05, 0.0003489303926471621], [0.9994854927062988, 5.031847467762418e-05, 0.0004641655832529068], [0.9995967745780945, 5.1971172069897875e-05, 0.00035132758785039186], [0.9995954632759094, 6.513777043437585e-05, 0.0003393415827304125], [0.9996179342269897, 5.6384287745459005e-05, 0.00032567724701948464], [0.9995915293693542, 4.906476897303946e-05, 0.00035942194517701864], [0.9994862079620361, 6.402972212526947e-05, 0.000449803308583796], [0.9996017813682556, 5.449895252240822e-05, 0.0003437502309679985], [0.9996246099472046, 5.767539551015943e-05, 0.00031769153429195285], [0.9994737505912781, 7.789864321239293e-05, 0.00044834669097326696], [0.9995081424713135, 5.169258656678721e-05, 0.00044012488797307014], [0.9994320273399353, 4.897990220342763e-05, 0.0005189355579204857], [0.9995841383934021, 6.689911242574453e-05, 0.0003490624949336052], [0.9995728135108948, 5.395566768129356e-05, 0.0003731945180334151], [0.9996053576469421, 5.829644942423329e-05, 0.0003363930154591799], [0.9995735287666321, 5.730408520321362e-05, 0.00036916593671776354], [0.9995397329330444, 5.565504761761986e-05, 0.00040466184145770967], [0.9995583891868591, 5.2352010243339464e-05, 0.0003893384709954262], [0.9996578693389893, 5.849432272952981e-05, 0.0002835688937921077], [0.4955978989601135, 0.2421712577342987, 0.26223087310791016], [0.9996083378791809, 6.331696204142645e-05, 0.0003284031408838928], [0.9995331764221191, 5.487450835062191e-05, 0.0004119719669688493], [0.9995514750480652, 5.1701728807529435e-05, 0.0003967427765019238], [0.9996217489242554, 6.351437332341447e-05, 0.00031471162219531834], [0.9995511174201965, 5.4426338465418667e-05, 0.0003944317577406764], [0.9996330738067627, 5.302645149640739e-05, 0.0003139330947306007], [0.9995328187942505, 6.708464934490621e-05, 0.00040006727795116603], [0.999593198299408, 5.7874636695487425e-05, 0.0003488859219942242], [0.9995014667510986, 5.31410078110639e-05, 0.0004453806614037603], [0.9994714856147766, 5.501632040250115e-05, 0.00047343046753667295], [0.9995324611663818, 6.832899089204147e-05, 0.0003992554557044059], [0.9992795586585999, 5.236536162556149e-05, 0.0006680803489871323], [0.999629020690918, 5.548353510675952e-05, 0.0003154115693178028], [0.9996102452278137, 5.986137693980709e-05, 0.00032994040520861745], [0.9996010661125183, 6.023454261594452e-05, 0.0003387207689229399], [0.9782779216766357, 0.0010126479901373386, 0.020709462463855743], [0.9995985627174377, 5.297650204738602e-05, 0.0003484623448457569], [0.9992210865020752, 7.015348091954365e-05, 0.0007087913691066206], [0.9995145797729492, 5.210346353123896e-05, 0.00043332463246770203], [0.9995984435081482, 5.817974670208059e-05, 0.0003433762467466295], [0.9995397329330444, 4.943617750541307e-05, 0.0004108113644178957], [0.999640941619873, 5.746277020080015e-05, 0.0003016636474058032], [0.9995015859603882, 5.3158000810071826e-05, 0.00044522786629386246], [0.999464213848114, 6.127889355411753e-05, 0.00047457069740630686], [0.9994271993637085, 4.9172158469446003e-05, 0.000523646071087569], [0.9995918869972229, 5.6672033679205924e-05, 0.00035135785583406687], [0.9995773434638977, 5.943035284872167e-05, 0.00036322427331469953], [0.9994015693664551, 6.431879592128098e-05, 0.0005340816569514573], [0.9995793700218201, 5.944010990788229e-05, 0.00036119064316153526], [0.9995854496955872, 4.909554627374746e-05, 0.0003654302272479981], [0.9995993971824646, 5.997781045152806e-05, 0.00034067206433974206], [0.9996421337127686, 6.290171586442739e-05, 0.00029489188455045223], [0.9996256828308105, 6.157398456707597e-05, 0.00031268957536667585], [0.999434769153595, 5.274482100503519e-05, 0.0005124833551235497], [0.9991902709007263, 7.921578799141571e-05, 0.0007305134204216301], [0.9994719624519348, 6.094506898079999e-05, 0.0004669949412345886], [0.9992357492446899, 5.3084648243384436e-05, 0.0007111220620572567], [0.9992578625679016, 8.748175605433062e-05, 0.0006546465447172523], [0.9995625615119934, 5.573620364884846e-05, 0.00038165156729519367], [0.9995632767677307, 6.5499400079716e-05, 0.00037121883360669017], [0.9995682835578918, 5.973090082989074e-05, 0.000372069887816906], [0.9996063113212585, 6.076416320865974e-05, 0.0003329746250528842], [0.9988771080970764, 0.00012682164378929883, 0.0009960418101400137], [0.9971861243247986, 0.0001842360943555832, 0.002629620488733053], [0.9995311498641968, 4.8467649321537465e-05, 0.0004203862335998565], [0.999123752117157, 7.587641448481008e-05, 0.0008003412513062358], [0.9996010661125183, 6.983531784499064e-05, 0.00032913286122493446], [0.9993908405303955, 8.876803622115403e-05, 0.0005203934269957244], [0.9996273517608643, 5.513047653948888e-05, 0.00031747674802318215], [0.9995260238647461, 5.484691064339131e-05, 0.0004191579355392605], [0.9996218681335449, 5.5399865232175216e-05, 0.0003226661356166005], [0.9994774460792542, 5.861341196577996e-05, 0.00046399165876209736], [0.9995230436325073, 5.848096770932898e-05, 0.00041844332008622587], [0.9994503855705261, 5.1725495723076165e-05, 0.0004978763172402978], [0.9996163845062256, 5.6633492931723595e-05, 0.0003270372108090669], [0.9953155517578125, 0.00020531514019239694, 0.004479146096855402], [0.9996126294136047, 5.3435389418154955e-05, 0.00033403298584744334], [0.9995180368423462, 6.887265044497326e-05, 0.0004130444140173495], [0.999419093132019, 5.304832666297443e-05, 0.0005279509932734072], [0.999301552772522, 6.332271004794165e-05, 0.0006350211333483458], [0.9995194673538208, 6.23824744252488e-05, 0.00041805714135989547], [0.9995985627174377, 5.43883943464607e-05, 0.0003470656229183078], [0.9994396567344666, 5.545731619349681e-05, 0.0005049106548540294], [0.9995900988578796, 5.12119077029638e-05, 0.00035871673026122153], [0.9995317459106445, 5.682963092112914e-05, 0.00041141908150166273], [0.9993981122970581, 6.635962927248329e-05, 0.0005355238681659102], [0.9995291233062744, 5.1817412895616144e-05, 0.00041912024607881904], [0.9996330738067627, 6.39499121461995e-05, 0.00030287951813079417], [0.9995737671852112, 5.6380071328021586e-05, 0.00036982272285968065], [0.99965500831604, 6.621496140724048e-05, 0.00027875014347955585], [0.9993529915809631, 6.866441253805533e-05, 0.0005784076056443155], [0.9996404647827148, 6.88050567987375e-05, 0.00029076231294311583], [0.9996063113212585, 5.840334415552206e-05, 0.0003353283100295812], [0.9994558691978455, 5.719851469621062e-05, 0.00048696884186938405], [0.9996355772018433, 6.609808770008385e-05, 0.00029821525095030665], [0.9996039271354675, 6.960811879253015e-05, 0.0003264883707743138], [0.9996330738067627, 5.5507156503153965e-05, 0.00031137539190240204], [0.9996583461761475, 6.00763269176241e-05, 0.00028162889066152275], [0.9995487332344055, 5.540331403608434e-05, 0.00039579658186994493], [0.9952238202095032, 0.00022543853265233338, 0.0045508467592298985], [0.9993115663528442, 7.059434574330226e-05, 0.0006177811301313341], [0.99964439868927, 7.104674295987934e-05, 0.0002845788549166173], [0.9994310736656189, 5.946195960859768e-05, 0.0005094741354696453], [0.9995498061180115, 5.1140887080691755e-05, 0.00039902108255773783], [0.9996533393859863, 5.9807465731864795e-05, 0.00028687220765277743], [0.9993653893470764, 7.88820325396955e-05, 0.0005558065604418516], [0.9995593428611755, 4.953229290549643e-05, 0.0003910515224561095], [0.9992456436157227, 5.8440095017431304e-05, 0.0006959508755244315], [0.9994070529937744, 7.180350803537294e-05, 0.0005210929084569216], [0.9996494054794312, 5.8477209677221254e-05, 0.0002920928527601063], [0.999642014503479, 5.7431767345406115e-05, 0.00030048677581362426], [0.9995517134666443, 5.040644100517966e-05, 0.0003979175235144794], [0.9995261430740356, 5.5986576626310125e-05, 0.0004179188108537346], [0.9995691180229187, 5.7012526667676866e-05, 0.0003738867526408285], [0.9996325969696045, 5.845503619639203e-05, 0.00030883101862855256], [0.999601423740387, 5.969535413896665e-05, 0.0003388493205420673], [0.9995318651199341, 4.793875996256247e-05, 0.00042020660475827754], [0.9984571933746338, 0.00011319256736896932, 0.0014296448789536953], [0.9995359182357788, 8.631604578113183e-05, 0.00037782048457302153], [0.9994064569473267, 8.99856531759724e-05, 0.000503584451507777], [0.9996275901794434, 5.829118890687823e-05, 0.0003140888875350356], [0.9994662404060364, 7.452376303263009e-05, 0.0004592750046867877], [0.9995504021644592, 6.88870350131765e-05, 0.00038073628093115985], [0.9996371269226074, 5.996869731461629e-05, 0.00030294922180473804], [0.9995872378349304, 5.898430390516296e-05, 0.00035380054032430053], [0.999556839466095, 5.7316898164572194e-05, 0.00038578908424824476], [0.999514102935791, 4.955849362886511e-05, 0.00043628094135783613], [0.9996108412742615, 5.855580457136966e-05, 0.00033069917117245495], [0.9996365308761597, 5.915097062825225e-05, 0.0003043807519134134], [0.9993364214897156, 7.98613327788189e-05, 0.0005837346543557942], [0.9991889595985413, 6.486238999059424e-05, 0.000746165809687227], [0.999570906162262, 5.6321707234019414e-05, 0.00037275897921063006], [0.9995668530464172, 5.817558121634647e-05, 0.0003750025643967092], [0.9994425177574158, 7.089642895152792e-05, 0.00048661133041605353], [0.9996562004089355, 6.23550295131281e-05, 0.0002814226027112454], [0.9994756579399109, 7.425998046528548e-05, 0.0004501019138842821], [0.9994920492172241, 5.030781903769821e-05, 0.00045764114474877715], [0.9994297623634338, 5.4194799304241315e-05, 0.0005160014843568206], [0.9994557499885559, 5.941773270023987e-05, 0.0004848352982662618], [0.9996387958526611, 5.4485280998051167e-05, 0.00030677899485453963], [0.9994767308235168, 5.483150016516447e-05, 0.0004685364256147295], [0.9994664788246155, 5.781665458926e-05, 0.0004757322312798351], [0.9995296001434326, 5.153180245542899e-05, 0.00041890828288160264], [0.9995378255844116, 5.54910147911869e-05, 0.0004066159890498966], [0.9996222257614136, 5.1559440180426463e-05, 0.00032618470140732825], [0.9994034767150879, 5.1193510444136336e-05, 0.000545303279068321], [0.9995775818824768, 6.0873258917126805e-05, 0.00036153249675408006], [0.9988034963607788, 0.00011663705663522705, 0.001079818233847618], [0.9995947480201721, 5.877591684111394e-05, 0.0003465455083642155], [0.9996223449707031, 6.074677730794065e-05, 0.0003168043331243098], [0.9995999932289124, 6.033480167388916e-05, 0.0003397770633455366], [0.9988921284675598, 6.870052311569452e-05, 0.001039200578816235], [0.9995693564414978, 5.424968549050391e-05, 0.00037645630072802305], [0.9995964169502258, 5.60789012524765e-05, 0.00034750631311908364], [0.9995089769363403, 5.760249405284412e-05, 0.000433449080446735], [0.9981354475021362, 8.720615733182058e-05, 0.0017773243598639965], [0.9993929862976074, 6.438219133997336e-05, 0.0005426231073215604], [0.9992831349372864, 8.99411752470769e-05, 0.0006269978475756943], [0.9995905756950378, 6.370000483002514e-05, 0.000345727545209229], [0.9967449903488159, 0.00021317615755833685, 0.0030418301466852427], [0.9996351003646851, 5.595766197075136e-05, 0.0003088821831624955], [0.006741812452673912, 0.8322023749351501, 0.16105584800243378], [0.9993923902511597, 5.408955621533096e-05, 0.0005535469390451908], [0.9996528625488281, 5.787979534943588e-05, 0.0002892679476644844], [0.9995477795600891, 4.9898681027116254e-05, 0.000402252800995484], [0.9994682669639587, 7.561394886579365e-05, 0.0004560815868899226], [0.999631404876709, 6.756481161573902e-05, 0.00030096995760686696], [0.9995653033256531, 5.881595643586479e-05, 0.000375857314793393], [0.999495267868042, 6.453508831327781e-05, 0.00044019013876095414], [0.9996141195297241, 5.796322875539772e-05, 0.0003279584052506834], [0.9987209439277649, 0.00017505561118014157, 0.0011040603276342154], [0.9988477230072021, 0.00010194042988587171, 0.001050339313223958], [0.9996084570884705, 5.625333869829774e-05, 0.0003352912899572402], [0.999656081199646, 6.645960820605978e-05, 0.00027746037812903523], [0.9996134638786316, 6.072549876989797e-05, 0.00032583950087428093], [0.999636173248291, 6.359293183777481e-05, 0.00030013045761734247], [0.9996422529220581, 6.578565808013082e-05, 0.00029200996505096555], [0.9995180368423462, 5.272718408377841e-05, 0.00042915347148664296], [0.9994373917579651, 6.712541653541848e-05, 0.0004955157055519521], [0.9995637536048889, 5.5471555242547765e-05, 0.00038067440618760884], [0.9994807839393616, 5.787993359263055e-05, 0.0004613908822648227], [0.9994014501571655, 6.571471021743491e-05, 0.0005328369443304837], [0.99951171875, 5.442216934170574e-05, 0.00043383133015595376], [0.9996498823165894, 5.807009438285604e-05, 0.00029210385400801897], [0.9994888305664062, 5.1509116019587964e-05, 0.0004597076040226966], [0.9995656609535217, 5.523716754396446e-05, 0.00037900914321653545], [0.9991976618766785, 7.264140003826469e-05, 0.0007296525873243809], [0.9996094107627869, 5.057084854342975e-05, 0.00034010011586360633], [0.9996355772018433, 6.706167914671823e-05, 0.00029737773002125323], [0.999160647392273, 7.34959976398386e-05, 0.0007658631075173616], [0.999481737613678, 5.499055259861052e-05, 0.00046322873095050454], [0.9995779395103455, 5.2680381486425176e-05, 0.0003694111364893615], [0.9982126951217651, 0.00011733630526578054, 0.0016699363477528095], [0.9994753003120422, 4.83273142890539e-05, 0.0004763745528180152], [0.9993674159049988, 6.076296995161101e-05, 0.0005718745524063706], [0.9996134638786316, 5.714841972803697e-05, 0.00032934261253103614], [0.9995342493057251, 6.507485522888601e-05, 0.0004007729876320809], [0.9993294477462769, 7.728289347141981e-05, 0.000593233504332602], [0.9995933175086975, 4.8190613597398624e-05, 0.0003585523518268019], [0.999612033367157, 5.4084368457552046e-05, 0.000333886913722381], [0.9996139407157898, 5.946931923972443e-05, 0.00032667789491824806], [0.9994382262229919, 5.0933198508573696e-05, 0.0005109159392304718], [0.9994587302207947, 4.733614332508296e-05, 0.0004939779173582792], [0.9995920062065125, 5.4977110266918316e-05, 0.0003530952089931816], [0.9995731711387634, 5.880554090254009e-05, 0.0003679930523503572], [0.9986498951911926, 7.355390698648989e-05, 0.0012765300925821066], [0.9996457099914551, 6.34637035545893e-05, 0.00029073416953906417], [0.999455988407135, 5.125658572069369e-05, 0.0004928242997266352], [0.9996044039726257, 5.994786260998808e-05, 0.0003357308451086283], [0.9995538592338562, 6.118453165981919e-05, 0.0003849400964099914], [0.9996218681335449, 5.654648703057319e-05, 0.00032146001467481256], [0.9996134638786316, 5.8563167840475217e-05, 0.00032802732312120497], [0.9996034502983093, 5.3051637223688886e-05, 0.0003435787803027779], [0.9996138215065002, 7.009704131633043e-05, 0.0003161279601044953], [0.999504804611206, 6.678855424979702e-05, 0.0004283747693989426], [0.9995512366294861, 6.931391544640064e-05, 0.0003794090589508414], [0.999178946018219, 7.785911293467507e-05, 0.0007431153790093958], [0.9996250867843628, 5.8086050557903945e-05, 0.00031686807051301], [0.9996035695075989, 5.434107151813805e-05, 0.0003421567671466619], [0.9995008707046509, 5.5720698583172634e-05, 0.0004433276772033423], [0.9995942711830139, 5.307959145284258e-05, 0.00035273938556201756], [0.999582827091217, 5.722422429244034e-05, 0.00035999997635371983], [0.9995864033699036, 5.244102067081258e-05, 0.00036120935692451894], [0.999484658241272, 6.12605144851841e-05, 0.00045408462756313384], [0.9989258646965027, 8.683239138917997e-05, 0.0009872554801404476], [0.9989222288131714, 0.00015737237117718905, 0.0009203647496178746], [0.9995816349983215, 5.071094710729085e-05, 0.0003676029446069151], [0.9907389879226685, 0.0005972698563709855, 0.008663691580295563], [0.9995645880699158, 6.163567013572901e-05, 0.00037384050665423274], [0.9994220733642578, 5.548723493120633e-05, 0.0005224435008130968], [0.9995908141136169, 5.250776303000748e-05, 0.0003566615341696888], [0.998884379863739, 8.067562157521024e-05, 0.0010348509531468153], [0.9996063113212585, 6.255154585232958e-05, 0.0003311072650831193], [0.9995027780532837, 4.7566449211444706e-05, 0.00044962464016862214], [0.9994041919708252, 6.0037233197363093e-05, 0.0005357537302188575], [0.9994114637374878, 7.029149128356948e-05, 0.000518280896358192], [0.9996083378791809, 5.606775084743276e-05, 0.000335542019456625], [0.99947589635849, 5.872914334759116e-05, 0.0004654128570109606], [0.9996265172958374, 6.600502092624083e-05, 0.00030748991412110627], [0.9994189739227295, 5.378733840188943e-05, 0.0005273460410535336], [0.9994083642959595, 5.787601548945531e-05, 0.0005336913745850325], [0.9989977478981018, 7.394469139399007e-05, 0.0009283122490160167], [0.9995676875114441, 7.221096893772483e-05, 0.0003600576601456851], [0.9994847774505615, 5.248162051429972e-05, 0.00046271758037619293], [0.9995880722999573, 6.039533400326036e-05, 0.0003515269490890205], [0.9995090961456299, 5.0912334700115025e-05, 0.00043997736065648496], [0.9996378421783447, 6.332197517622262e-05, 0.00029888618155382574], [0.9995763897895813, 6.253655737964436e-05, 0.00036105679464526474], [0.9993422627449036, 6.915088306413963e-05, 0.0005885972641408443], [0.9996083378791809, 5.600613440037705e-05, 0.00033573058317415416], [0.999407172203064, 5.183362736715935e-05, 0.0005409971927292645], [0.9989631175994873, 8.04929222795181e-05, 0.0009563731728121638], [0.9871852993965149, 0.0008515666122548282, 0.011963136494159698], [0.9993888139724731, 0.00010178070806432515, 0.0005094447988085449], [0.9995877146720886, 5.472635893966071e-05, 0.0003576245217118412], [0.9996165037155151, 5.611683809547685e-05, 0.00032739952439442277], [0.9994298815727234, 5.3022904467070475e-05, 0.0005170515505596995], [0.9995313882827759, 5.960123598924838e-05, 0.00040907759102992713], [0.9995658993721008, 5.077305468148552e-05, 0.00038338033482432365], [0.9984221458435059, 0.00011498607636895031, 0.0014628880890086293], [0.9996218681335449, 6.372832285705954e-05, 0.0003143178182654083], [0.9995760321617126, 6.453211972257122e-05, 0.00035938722430728376], [0.9992693066596985, 6.149793625809252e-05, 0.0006692867027595639], [0.9994993209838867, 6.26410183031112e-05, 0.0004380149766802788], [0.9991534948348999, 0.00011057309893658385, 0.0007359391893260181], [0.9993626475334167, 6.216597103048116e-05, 0.0005751625285483897], [0.9994966983795166, 4.9594509619055316e-05, 0.00045367219718173146], [0.999430239200592, 5.582915400736965e-05, 0.0005138905253261328], [0.9947572946548462, 0.00026689920923672616, 0.004975849762558937], [0.9996322393417358, 5.9266476455377415e-05, 0.0003085253410972655], [0.9983869791030884, 0.00011075338261434808, 0.0015022141160443425], [0.9996433258056641, 5.934286309638992e-05, 0.0002973363734781742], [0.9995343685150146, 5.0136037316406146e-05, 0.000415457267081365], [0.999583899974823, 5.934289947617799e-05, 0.00035672186641022563], [0.9507750868797302, 0.0029072288889437914, 0.04631771519780159], [0.9980579018592834, 0.00020651482918765396, 0.0017354913288727403], [0.9996460676193237, 6.103468331275508e-05, 0.0002928913163486868], [0.9992473125457764, 6.640905485255644e-05, 0.0006863878224976361], [0.9995574355125427, 6.768982711946592e-05, 0.0003749092575162649], [0.9996433258056641, 6.527366349473596e-05, 0.0002913654607255012], [0.9995754361152649, 4.768522921949625e-05, 0.00037684058770537376], [0.9980599284172058, 0.0001475247845519334, 0.001792564638890326], [0.9995040893554688, 5.868338485015556e-05, 0.00043716683285310864], [0.9992480874061584, 6.114989082561806e-05, 0.0006907915230840445], [0.9996405839920044, 5.925848745391704e-05, 0.00030021590646356344], [0.9994658827781677, 5.015540955355391e-05, 0.00048396250349469483], [0.9996241331100464, 5.8252695453120396e-05, 0.00031758713885210454], [0.9996229410171509, 5.640059316647239e-05, 0.000320660590659827], [0.9996023774147034, 6.55252079013735e-05, 0.00033211353002116084], [0.9993705153465271, 6.35193646303378e-05, 0.0005660175229422748], [0.9972499012947083, 0.0001789742091204971, 0.0025711171329021454], [0.9995681643486023, 5.305001468514092e-05, 0.0003788429603446275], [0.9996606111526489, 5.988711563986726e-05, 0.000279461033642292], [0.9995120763778687, 5.016634167986922e-05, 0.00043773578363470733], [0.9994864463806152, 5.7960471167461947e-05, 0.0004556533822324127], [0.9995512366294861, 5.056424197391607e-05, 0.0003982573398388922], [0.9995697140693665, 5.108740879222751e-05, 0.0003791151975747198], [0.9995900988578796, 5.982689617667347e-05, 0.000350104906829074], [0.9996144771575928, 5.279492324916646e-05, 0.00033265011734329164], [0.999649167060852, 5.4731495765736327e-05, 0.0002961466962005943], [0.9995593428611755, 4.904737579636276e-05, 0.00039153886609710753], [0.9995524287223816, 6.110088725108653e-05, 0.00038647340261377394], [0.9995347261428833, 6.585523078683764e-05, 0.0003994203289039433], [0.9995142221450806, 5.816080738441087e-05, 0.0004275920509826392], [0.9994760155677795, 7.56701483624056e-05, 0.00044836782035417855], [0.031047046184539795, 0.0262654647231102, 0.942687451839447], [0.999610960483551, 5.37070409336593e-05, 0.00033532222732901573], [0.9993347525596619, 6.34293828625232e-05, 0.0006018821732141078], [0.999567449092865, 5.6581167882541195e-05, 0.0003759506216738373], [0.9995736479759216, 5.499345934367739e-05, 0.00037134482408873737], [0.9995561242103577, 6.17795594735071e-05, 0.00038219019188545644], [0.999586284160614, 5.6886088714236394e-05, 0.00035692891106009483], [0.9996193647384644, 5.228076406638138e-05, 0.0003282924590166658], [0.9995492100715637, 5.855035487911664e-05, 0.00039225438376888633], [0.9996457099914551, 5.7618064602138475e-05, 0.0002965961757581681], [0.9995371103286743, 5.718069223803468e-05, 0.0004057292826473713], [0.9993982315063477, 5.389168654801324e-05, 0.0005478558596223593], [0.999454915523529, 6.420602585421875e-05, 0.0004808339290320873], [0.9995881915092468, 6.046352064004168e-05, 0.0003513603878673166], [0.9995564818382263, 5.86149217269849e-05, 0.00038485703407786787], [0.9995014667510986, 8.352316945092753e-05, 0.00041504224645905197], [0.9995679259300232, 5.584791870205663e-05, 0.000376162031898275], [0.9995551705360413, 5.147673437022604e-05, 0.0003933113475795835], [0.9985882639884949, 0.00010218245006399229, 0.0013096161419525743], [0.9995843768119812, 6.124834180809557e-05, 0.00035439286148175597], [0.9991685152053833, 5.7817833294393495e-05, 0.0007736922125332057], [0.9995784163475037, 5.62511122552678e-05, 0.00036539367283694446], [0.9995287656784058, 5.109724952490069e-05, 0.00042011935147456825], [0.9996188879013062, 5.773521479568444e-05, 0.0003232704184483737], [0.9994727969169617, 5.9841728216269985e-05, 0.000467464531539008], [0.9992721676826477, 6.89527514623478e-05, 0.0006588676478713751], [0.999599277973175, 5.773831799160689e-05, 0.0003429098869673908], [0.9993150234222412, 6.0365880926838145e-05, 0.000624572450760752], [0.0033481542486697435, 0.987006425857544, 0.00964544340968132], [0.9993354678153992, 4.951623850502074e-05, 0.0006149453110992908], [0.9995817542076111, 6.746603321516886e-05, 0.00035083346301689744], [0.9968003034591675, 0.00019288861949462444, 0.0030068724881857634], [0.9996143579483032, 5.728620453737676e-05, 0.00032828960684128106], [0.998383641242981, 0.00011673042899928987, 0.0014995747478678823], [0.9975564479827881, 0.00013903348008170724, 0.0023044911213219166], [0.9995611310005188, 5.9174384659854695e-05, 0.00037961985799483955], [0.9995404481887817, 5.112889630254358e-05, 0.00040847391937859356], [0.9996167421340942, 6.300031236605719e-05, 0.00032019594800658524], [0.9995918869972229, 5.321626667864621e-05, 0.00035485581611283123], [0.9995217323303223, 5.927676102146506e-05, 0.0004190499894320965], [0.9995030164718628, 5.221632818575017e-05, 0.0004448337422218174], [0.9994334578514099, 5.760627027484588e-05, 0.0005089131882414222], [0.9992508292198181, 5.4592896049143746e-05, 0.0006945906206965446], [0.9995788931846619, 5.771545329480432e-05, 0.0003634412423707545], [0.9993278980255127, 6.649622810073197e-05, 0.0006056788261048496], [0.998323380947113, 7.455058948835358e-05, 0.0016020648181438446], [0.9995880722999573, 5.600473377853632e-05, 0.0003558823373168707], [0.9995231628417969, 5.207942012930289e-05, 0.0004248047771397978], [0.9995861649513245, 5.85653651796747e-05, 0.00035527703585103154], [0.9995647072792053, 5.133598460815847e-05, 0.0003839594719465822], [0.9995214939117432, 5.222605977905914e-05, 0.0004263353766873479], [0.9996285438537598, 5.6953362218337134e-05, 0.0003144371439702809], [0.9996100068092346, 5.150233846507035e-05, 0.0003385340969543904], [0.9996547698974609, 6.60263976897113e-05, 0.0002792545419652015], [0.9995018243789673, 6.201882206369191e-05, 0.00043608754640445113], [0.9995662569999695, 5.286648229230195e-05, 0.0003808005712926388], [0.9994649291038513, 5.26806506968569e-05, 0.00048245699144899845], [0.9995713829994202, 6.455668335547671e-05, 0.0003639734932221472], [0.9996447563171387, 5.262103513814509e-05, 0.0003026110935024917], [0.9995167255401611, 5.5787459132261574e-05, 0.0004275164974387735], [0.9996284246444702, 6.598739855689928e-05, 0.00030554545810446143], [0.9996514320373535, 6.122266495367512e-05, 0.0002873439807444811], [0.9996060729026794, 5.5661817896179855e-05, 0.0003383220173418522], [0.999509334564209, 6.546445365529507e-05, 0.0004252613871358335], [0.9995613694190979, 5.593118112301454e-05, 0.00038277104613371193], [0.9972758889198303, 0.00019731245993170887, 0.0025267775636166334], [0.9995661377906799, 6.132642010925338e-05, 0.00037251078174449503], [0.9970115423202515, 0.00018970434030052274, 0.002798702334985137], [0.9994884729385376, 5.0268121412955225e-05, 0.0004612567718140781], [0.9996156692504883, 5.559572309721261e-05, 0.0003286349237896502], [0.9993828535079956, 6.316712097031996e-05, 0.0005540249403566122], [0.9992644190788269, 5.9086672990815714e-05, 0.0006764575373381376], [0.9996324777603149, 5.445002898341045e-05, 0.0003131043922621757], [0.9994971752166748, 5.21971051057335e-05, 0.0004505694378167391], [0.9991865754127502, 0.00010509380081202835, 0.0007083239615894854], [0.9993818998336792, 7.335568807320669e-05, 0.0005447454750537872], [0.9995511174201965, 6.316690269159153e-05, 0.0003857181000057608], [0.9994839429855347, 6.602771463803947e-05, 0.00045006099389865994], [0.9996469020843506, 5.538123150472529e-05, 0.0002976855612359941], [0.999619722366333, 5.8849127526627854e-05, 0.0003213921736460179], [0.999496579170227, 6.131057307356969e-05, 0.0004420046170707792], [0.9994878768920898, 5.47427662240807e-05, 0.00045739751658402383], [0.9969907999038696, 0.00013647119340021163, 0.0028727080207318068], [0.9996488094329834, 6.387283792719245e-05, 0.00028728132019750774], [0.9996490478515625, 6.28960260655731e-05, 0.0002880734100472182], [0.9995415210723877, 5.581172808888368e-05, 0.000402795965783298], [0.9995419979095459, 5.1153019740013406e-05, 0.00040686537977308035], [0.9994811415672302, 7.454164733644575e-05, 0.00044435044401325285], [0.9990359544754028, 7.209274917840958e-05, 0.0008920076070353389], [0.9995831847190857, 5.2346265874803066e-05, 0.00036448027822189033], [0.9995336532592773, 5.3791460231877863e-05, 0.00041258276905864477], [0.999575674533844, 5.202849570196122e-05, 0.0003721961402334273], [0.9996138215065002, 5.4133692174218595e-05, 0.00033215602161362767], [0.9860109090805054, 0.0010062913643196225, 0.012982857413589954], [0.9989551305770874, 7.278455450432375e-05, 0.0009721300448291004], [0.9995378255844116, 5.7969416957348585e-05, 0.00040419460856355727], [0.9995535016059875, 5.331988359102979e-05, 0.0003932201361749321], [0.9992054104804993, 6.916488928254694e-05, 0.0007254508091136813], [0.9990552067756653, 8.008287113625556e-05, 0.0008647758513689041], [0.9995267391204834, 6.223502714419737e-05, 0.0004110978334210813], [0.043182093650102615, 0.8868361711502075, 0.06998176127672195], [0.9995250701904297, 6.670773291261867e-05, 0.0004082326195202768], [0.9994638562202454, 6.754981586709619e-05, 0.0004685489402618259], [0.9994799494743347, 6.213479355210438e-05, 0.00045795581536367536], [0.9983336329460144, 0.00010547530837357044, 0.0015608873218297958], [0.9996206760406494, 6.039656000211835e-05, 0.00031892152037471533], [0.9771689176559448, 0.0012698611244559288, 0.02156132273375988], [0.9995760321617126, 5.2429077186388895e-05, 0.00037159124622121453], [0.9995800852775574, 4.710787834483199e-05, 0.0003728671290446073], [0.9996071457862854, 6.718805525451899e-05, 0.0003256814961787313], [0.999530553817749, 5.106611206429079e-05, 0.00041838185279630125], [0.999652624130249, 6.144331564428285e-05, 0.000285853078821674], [0.9995049238204956, 5.549463821807876e-05, 0.0004395716532599181], [0.9996157884597778, 6.56344709568657e-05, 0.0003185381938237697], [0.9996185302734375, 6.674076576018706e-05, 0.0003147202078253031], [0.9993703961372375, 5.145583054400049e-05, 0.0005782033549621701], [0.9995471835136414, 6.036597551428713e-05, 0.000392350455513224], [0.997927188873291, 0.00012495031114667654, 0.0019478659378364682], [0.9996161460876465, 6.160459452075884e-05, 0.00032222919980995357], [0.9995658993721008, 5.3949897846905515e-05, 0.0003801268758252263], [0.9996053576469421, 7.094530883478001e-05, 0.0003237358760088682], [0.9995059967041016, 5.21958245371934e-05, 0.00044177257223054767], [0.9994723200798035, 5.542259168578312e-05, 0.0004723489691969007], [0.9992867112159729, 8.013596379896626e-05, 0.000633192656096071], [0.9919142723083496, 0.0017434280598536134, 0.006342326290905476], [0.9995848536491394, 5.526636596187018e-05, 0.0003598702314775437], [0.9976810216903687, 0.0002057344390777871, 0.0021132330875843763], [0.9994275569915771, 5.452261757454835e-05, 0.0005179349682293832], [0.9995629191398621, 5.223953485256061e-05, 0.00038490467704832554], [0.9995831847190857, 6.767285958630964e-05, 0.0003491746902000159], [0.9994761347770691, 7.950526924105361e-05, 0.00044439395423978567], [0.9996285438537598, 5.6614204368088394e-05, 0.0003148815594613552], [0.9996019005775452, 6.113963900133967e-05, 0.0003370429330971092], [0.99956876039505, 5.6376884458586574e-05, 0.00037488885573111475], [0.9995061159133911, 5.3261202992871404e-05, 0.0004406172374729067], [0.999546468257904, 4.6957386075519025e-05, 0.00040658831130713224], [0.9995450377464294, 4.600781903718598e-05, 0.00040899269515648484], [0.9996337890625, 5.8715915656648576e-05, 0.000307391892420128], [0.9994691014289856, 5.4687727242708206e-05, 0.00047626823652535677], [0.9995831847190857, 5.57665953238029e-05, 0.0003610559506341815], [0.9995089769363403, 6.027990821166895e-05, 0.0004306714399717748], [0.9995417594909668, 5.288585816742852e-05, 0.00040541222551837564], [0.9996250867843628, 5.723630602005869e-05, 0.0003176547179464251], [0.9995362758636475, 5.4885982535779476e-05, 0.00040882278699427843], [0.9993807077407837, 5.264527862891555e-05, 0.0005666100769303739], [0.9994186162948608, 6.067279537091963e-05, 0.00052080606110394], [0.9996411800384521, 5.578052150667645e-05, 0.0003030957595910877], [0.9994718432426453, 5.647860962199047e-05, 0.00047170952893793583], [0.9993392825126648, 0.00010514396126382053, 0.0005555369425565004], [0.9995695948600769, 5.718108150176704e-05, 0.0003731922770384699], [0.9995513558387756, 5.345005411072634e-05, 0.00039516660035587847], [0.999029278755188, 9.54912684392184e-05, 0.0008751959539949894], [0.999434769153595, 5.72121498407796e-05, 0.0005080327391624451], [0.9996106028556824, 5.883596168132499e-05, 0.00033061017165891826], [0.9995786547660828, 5.799440259579569e-05, 0.00036334843025542796], [0.9995915293693542, 5.380390939535573e-05, 0.00035462528467178345], [0.9994807839393616, 5.270299516269006e-05, 0.00046658102655783296], [0.9994370341300964, 5.660341048496775e-05, 0.0005063574062660336], [0.9996362924575806, 5.771601718151942e-05, 0.00030605701613239944], [0.9966408014297485, 0.00033016057568602264, 0.0030290347058326006], [0.9992623925209045, 5.727040479541756e-05, 0.0006802925490774214], [0.9996341466903687, 5.695840445696376e-05, 0.0003088744997512549], [0.9313622117042542, 0.004401220008730888, 0.06423655152320862], [0.9995632767677307, 5.2490144298644736e-05, 0.00038411954301409423], [0.9994169473648071, 5.8334619097877294e-05, 0.000524642993696034], [0.9993330836296082, 6.384462176356465e-05, 0.0006030531949363649], [0.999427855014801, 5.2732884796569124e-05, 0.0005193908582441509], [0.9994760155677795, 5.264818901196122e-05, 0.0004713903763331473], [0.999637246131897, 5.996058462187648e-05, 0.0003027970378752798], [0.9995362758636475, 5.889636668143794e-05, 0.00040488585364073515], [0.9993619322776794, 6.257300265133381e-05, 0.0005754973390139639], [0.9995542168617249, 5.888461964786984e-05, 0.0003868612984661013], [0.9994194507598877, 5.8247929700883105e-05, 0.0005222782492637634], [0.9995412826538086, 5.02547190990299e-05, 0.0004084582906216383], [0.9993882179260254, 6.419060082407668e-05, 0.0005475973011925817], [0.9994075298309326, 6.185207166709006e-05, 0.0005306302336975932], [0.999639630317688, 5.50212716916576e-05, 0.0003052593383472413], [0.999521017074585, 4.964294203091413e-05, 0.00042928900802507997], [0.9911555051803589, 0.0008655162528157234, 0.007978998124599457], [0.9994385838508606, 5.494005745276809e-05, 0.0005065137520432472], [0.999460756778717, 4.840640758629888e-05, 0.0004908743430860341], [0.9995642304420471, 5.636394052999094e-05, 0.0003794239601120353], [0.9993682503700256, 7.487212860723957e-05, 0.0005569030181504786], [0.9993312358856201, 4.955409895046614e-05, 0.0006192133878357708], [0.9994820952415466, 4.93647348775994e-05, 0.0004685664316639304], [0.9992960691452026, 8.750737470109016e-05, 0.0006164000951685011], [0.9996052384376526, 6.60360383335501e-05, 0.00032881891820579767], [0.9994946718215942, 6.333174678729847e-05, 0.00044192455243319273], [0.9995028972625732, 6.67280110064894e-05, 0.0004303551686462015], [0.9996299743652344, 5.6149605370592326e-05, 0.00031392014352604747], [0.9996349811553955, 5.723905633203685e-05, 0.00030776066705584526], [0.9996113181114197, 5.736294770031236e-05, 0.00033142740721814334], [0.9996165037155151, 5.597060589934699e-05, 0.00032749102683737874], [0.9995993971824646, 5.133761806064285e-05, 0.00034928260720334947], [0.9991624355316162, 0.00016583551769144833, 0.0006717642536386847], [0.9996351003646851, 5.9147503634449095e-05, 0.0003057774738408625], [0.9995548129081726, 5.399655128712766e-05, 0.0003912388638127595], [0.9995653033256531, 5.445613351184875e-05, 0.0003802801948040724], [0.9990930557250977, 7.25218778825365e-05, 0.0008343945955857635], [0.9994981288909912, 5.322162542142905e-05, 0.0004485757672227919], [0.9995637536048889, 5.7843462855089456e-05, 0.0003784229629673064], [0.999657154083252, 6.133686110842973e-05, 0.0002815294428728521], [0.9995864033699036, 4.986683416063897e-05, 0.00036380565143190324], [0.9941512942314148, 0.0002964858431369066, 0.005552147980779409], [0.9996291399002075, 6.484568439191207e-05, 0.0003060484305024147], [0.9996330738067627, 6.286246207309887e-05, 0.00030395304202102125], [0.999563992023468, 5.932128624408506e-05, 0.0003767044108826667], [0.9995741248130798, 5.737202809541486e-05, 0.0003685071715153754], [0.999637246131897, 5.7420871598878875e-05, 0.00030526937916874886], [0.9995012283325195, 5.760764906881377e-05, 0.0004412126145325601], [0.9996261596679688, 5.883502308279276e-05, 0.0003149291733279824], [0.9996228218078613, 5.441025859909132e-05, 0.000322738807881251], [0.9996342658996582, 6.004784154356457e-05, 0.00030570285161957145], [0.99953293800354, 5.592451634583995e-05, 0.00041125231655314565], [0.9995624423027039, 7.001721678534523e-05, 0.00036744429962709546], [0.999384880065918, 5.5476597481174394e-05, 0.0005595464608632028], [0.9995728135108948, 5.046724254498258e-05, 0.00037668654113076627], [0.9995236396789551, 5.3435065638041124e-05, 0.000423009623773396], [0.9980018734931946, 8.97937425179407e-05, 0.0019083289662376046], [0.9995654225349426, 6.071823736419901e-05, 0.00037383599556051195], [0.9994563460350037, 5.8253972383681685e-05, 0.00048542130389250815], [0.9993040561676025, 5.8437715779291466e-05, 0.0006374372751452029], [0.999569833278656, 7.042630750220269e-05, 0.00035969584132544696], [0.9994322657585144, 5.365608012652956e-05, 0.0005140871508046985], [0.9996424913406372, 5.926261655986309e-05, 0.00029823294607922435], [0.9994388222694397, 5.942772259004414e-05, 0.0005017163348384202], [0.9995288848876953, 4.7213045036187395e-05, 0.0004239309928379953], [0.9995835423469543, 5.698486347682774e-05, 0.0003595568996388465], [0.9994910955429077, 5.1827795687131584e-05, 0.0004569997836370021], [0.999579131603241, 5.616956332232803e-05, 0.00036475626984611154], [0.9995904564857483, 5.731401688535698e-05, 0.00035224558087065816], [0.9996337890625, 6.249293073778972e-05, 0.0003037298156414181], [0.999642014503479, 5.930385668762028e-05, 0.00029865032411180437], [0.9995554089546204, 5.76326456211973e-05, 0.00038693114765919745], [0.9993394017219543, 9.577384480508044e-05, 0.0005648281658068299], [0.9995777010917664, 6.126813968876377e-05, 0.00036104745231568813], [0.9996169805526733, 6.950103852432221e-05, 0.0003135429578833282], [0.9996180534362793, 6.928603397682309e-05, 0.0003126645169686526], [0.9996236562728882, 5.480379695654847e-05, 0.0003215329779777676], [0.9995033740997314, 5.4036885558161885e-05, 0.0004425806400831789], [0.9996405839920044, 6.982975173741579e-05, 0.0002895737998187542], [0.9993308782577515, 8.310667180921882e-05, 0.0005859624361619353], [0.9996272325515747, 5.596645860350691e-05, 0.0003166998503729701], [0.9996302127838135, 5.305376907926984e-05, 0.0003167107643093914], [0.999583899974823, 5.4275005823001266e-05, 0.00036186250508762896], [0.9995570778846741, 5.8466786867938936e-05, 0.000384371291147545], [0.9989567995071411, 0.0001061278599081561, 0.0009370892657898366], [0.9993280172348022, 6.711370951961726e-05, 0.0006049536168575287], [0.9996474981307983, 5.9792742831632495e-05, 0.00029265356715768576], [0.9995941519737244, 6.604461668757722e-05, 0.0003398736007511616], [0.9996117949485779, 6.274924817262217e-05, 0.0003254563780501485], [0.9994539618492126, 8.058271487243474e-05, 0.00046546279918402433], [0.9993553757667542, 8.251221152022481e-05, 0.0005620550946332514], [0.9990111589431763, 6.817674147896469e-05, 0.0009206850663758814], [0.9995680451393127, 5.713242353522219e-05, 0.0003748301533050835], [0.9995922446250916, 5.438545849756338e-05, 0.00035348409437574446], [0.9996042847633362, 5.244956264505163e-05, 0.00034332784707657993], [0.9992708563804626, 5.686678196070716e-05, 0.0006723146070726216], [0.9995273351669312, 5.083272117190063e-05, 0.0004217847599647939], [0.9992764592170715, 6.576955638593063e-05, 0.000657771248370409], [0.9996069073677063, 5.369022983359173e-05, 0.0003393421065993607], [0.9995228052139282, 5.332922955858521e-05, 0.00042378573562018573], [0.9993181228637695, 5.9799611335620284e-05, 0.0006221429794095457], [0.9996424913406372, 6.402898725355044e-05, 0.0002934757503680885], [0.9995300769805908, 5.7155317335855216e-05, 0.0004127481079194695], [0.9994459748268127, 5.835222691530362e-05, 0.0004956730990670621], [0.9996176958084106, 5.6870085245464e-05, 0.0003254806506447494], [0.9995531439781189, 6.031822704244405e-05, 0.0003864895843435079], [0.9995514750480652, 5.169147334527224e-05, 0.00039676911546848714], [0.9994179010391235, 5.832321767229587e-05, 0.0005237606237642467], [0.9995156526565552, 4.895532038062811e-05, 0.0004353767435532063], [0.9996123909950256, 5.925625373492949e-05, 0.00032837537582963705], [0.9995537400245667, 5.333932494977489e-05, 0.0003929165832232684], [0.9996320009231567, 7.2455957706552e-05, 0.0002955823438242078], [0.9996353387832642, 6.986435619182885e-05, 0.00029478667420335114], [0.9992054104804993, 7.591591565869749e-05, 0.0007187564624473453], [0.9995724558830261, 5.5507280194433406e-05, 0.00037201502709649503], [0.9987419247627258, 8.61244261614047e-05, 0.0011720280162990093], [0.9994961023330688, 5.244598651188426e-05, 0.0004514959000516683], [0.9996176958084106, 5.684411371476017e-05, 0.0003253841132391244], [0.9995953440666199, 5.847515421919525e-05, 0.00034618814243003726], [0.9994115829467773, 5.224467895459384e-05, 0.0005361269577406347], [0.9994352459907532, 5.461703040054999e-05, 0.000510127458255738], [0.9995023012161255, 6.257773202378303e-05, 0.00043514990829862654], [0.9982163310050964, 0.00015130829706322402, 0.0016322598094120622], [0.9995349645614624, 5.671778853866272e-05, 0.0004083151288796216], [0.9996527433395386, 5.661978866555728e-05, 0.00029061007080599666], [0.9984315037727356, 0.00012843951117247343, 0.001440013642422855], [0.9992662072181702, 8.049392636166885e-05, 0.0006533020059578121], [0.9989017248153687, 6.684380787191913e-05, 0.0010314366081729531], [0.9996271133422852, 5.724018774344586e-05, 0.0003156890452373773], [0.9992376565933228, 6.859529094072059e-05, 0.0006937922444194555], [0.999525785446167, 7.808604277670383e-05, 0.00039614090928807855], [0.9995057582855225, 4.8247089580399916e-05, 0.000446012505562976], [0.9996380805969238, 6.26298860879615e-05, 0.00029925585840828717], [0.9994945526123047, 6.163240323076025e-05, 0.0004438528558239341], [0.9996469020843506, 6.145638326415792e-05, 0.0002915627555921674], [0.9995331764221191, 4.765093035530299e-05, 0.000419228250393644], [0.999613344669342, 6.182293145684525e-05, 0.00032478931825608015], [0.9851374626159668, 0.0009430833742953837, 0.013919387944042683], [0.9995536208152771, 6.960820610402152e-05, 0.00037680345121771097], [0.9995085000991821, 5.682998744305223e-05, 0.0004346737405285239], [0.9995377063751221, 5.399794099503197e-05, 0.0004083417297806591], [0.9988399147987366, 8.401693776249886e-05, 0.0010759938741102815], [0.9993252754211426, 5.2988692914368585e-05, 0.0006217921036295593], [0.9993732571601868, 6.418822886189446e-05, 0.0005625890917144716], [0.9995726943016052, 4.720318247564137e-05, 0.00038010894786566496], [0.9995183944702148, 5.348143167793751e-05, 0.000428079190896824], [0.9994555115699768, 5.070319457445294e-05, 0.0004937041085213423], [0.9838520884513855, 0.0012964931083843112, 0.014851479791104794], [0.9994106292724609, 6.181700882734731e-05, 0.0005275285220704973], [0.9995494484901428, 6.21912840870209e-05, 0.0003882695164065808], [0.9996154308319092, 5.9881444030907005e-05, 0.0003246159467380494], [0.9995429515838623, 5.401578528108075e-05, 0.0004031170392408967], [0.9993138313293457, 7.901702338131145e-05, 0.0006072281976230443], [0.9995957016944885, 5.5333519412670285e-05, 0.00034898199373856187], [0.9978199005126953, 0.0001759338629199192, 0.002004208043217659], [0.9995859265327454, 4.9868522182805464e-05, 0.0003641280927695334], [0.9994363188743591, 6.331427721306682e-05, 0.0005003833794035017], [0.9996247291564941, 7.293718954315409e-05, 0.00030226208036765456], [0.999605119228363, 6.947350630071014e-05, 0.0003254287294112146], [0.999626636505127, 5.496035737451166e-05, 0.0003184395900461823], [0.9993640780448914, 5.120506466482766e-05, 0.0005846834974363446], [0.9995026588439941, 5.7634602853795514e-05, 0.0004396838485263288], [0.9994206428527832, 7.650264160474762e-05, 0.0005029350286349654], [0.9994605183601379, 5.409391087596305e-05, 0.0004854168801102787], [0.9995050430297852, 5.52873570995871e-05, 0.0004396597505547106], [0.9996078610420227, 5.595613765763119e-05, 0.00033621006878092885], [0.9995993971824646, 5.94283155805897e-05, 0.00034121700446121395], [0.9994805455207825, 4.9684877012623474e-05, 0.0004697612312156707], [0.9994522929191589, 5.0120637752115726e-05, 0.0004975798656232655], [0.9996455907821655, 5.9955134929623455e-05, 0.0002944129519164562], [0.999447762966156, 5.440156382974237e-05, 0.0004977250355295837], [0.9996609687805176, 6.28386260359548e-05, 0.00027627882082015276], [0.9994439482688904, 5.7483943237457424e-05, 0.0004986239364370704], [0.9991617202758789, 0.00010422872583149001, 0.0007340460433624685], [0.9996542930603027, 6.308312731562182e-05, 0.0002826302661560476], [0.9995927214622498, 5.353097731131129e-05, 0.0003537367156241089], [0.9993292093276978, 7.532755989814177e-05, 0.0005955051747150719], [0.9995318651199341, 6.171151471789926e-05, 0.0004064276581630111], [0.9995831847190857, 5.158034400665201e-05, 0.0003652322047855705], [0.9993570446968079, 5.803703243145719e-05, 0.0005849925219081342], [0.9993801116943359, 5.648107617162168e-05, 0.0005633424734696746], [0.9995182752609253, 5.915621659369208e-05, 0.0004226027231197804], [0.999529242515564, 6.60564037389122e-05, 0.0004047297697979957], [0.9996054768562317, 5.208540096646175e-05, 0.0003424453316256404], [0.9995119571685791, 5.629062798107043e-05, 0.000431837048381567], [0.9996064305305481, 7.04006088199094e-05, 0.0003231440205127001], [0.9996142387390137, 5.351406434783712e-05, 0.00033222997444681823], [0.9995255470275879, 5.989261990180239e-05, 0.00041452376171946526], [0.9995790123939514, 5.134523962624371e-05, 0.00036967580672353506], [0.9993107318878174, 6.415502139134333e-05, 0.0006250750739127398], [0.9995713829994202, 5.4959513363428414e-05, 0.000373631133697927], [0.9993383288383484, 6.029548603692092e-05, 0.0006013111560605466], [0.9991961121559143, 7.496483158320189e-05, 0.000728967075701803], [0.9989621639251709, 9.77554518613033e-05, 0.0009400136186741292], [0.999470055103302, 5.21223628311418e-05, 0.0004778606235049665], [0.9996294975280762, 5.6620734540047124e-05, 0.000313831988023594], [0.33003172278404236, 0.009919431060552597, 0.6600489020347595], [0.9995487332344055, 5.7291730627184734e-05, 0.00039389549056068063], [0.9994876384735107, 5.965151285636239e-05, 0.00045262675848789513], [0.999461829662323, 6.707694410579279e-05, 0.00047105250996537507], [0.999431312084198, 5.8463498135097325e-05, 0.0005101758288219571], [0.9992917776107788, 5.9315825637895614e-05, 0.0006488840444944799], [0.9991705417633057, 8.285720105050132e-05, 0.0007465595263056457], [0.9994383454322815, 5.258810051600449e-05, 0.0005091397906653583], [0.9987713694572449, 8.634977712063119e-05, 0.0011422670213505626], [0.9995564818382263, 5.2110572141828015e-05, 0.00039139066939242184], [0.9992183446884155, 7.117253699107096e-05, 0.0007104467367753386], [0.9995092153549194, 5.464117930387147e-05, 0.0004360704042483121], [0.9995874762535095, 5.2015268011018634e-05, 0.0003604079829528928], [0.9996263980865479, 6.0758899053325877e-05, 0.00031284880242310464], [0.9994732737541199, 6.077381476643495e-05, 0.00046600945643149316], [0.9996190071105957, 5.194052209844813e-05, 0.0003290574823040515], [0.9988535642623901, 7.111222657840699e-05, 0.0010753320530056953], [0.999530553817749, 5.3036488679936156e-05, 0.00041644182056188583], [0.9993079900741577, 6.54430259601213e-05, 0.0006265947595238686], [0.9987461566925049, 8.257327135652304e-05, 0.0011712184641510248], [0.9995759129524231, 6.181254866532981e-05, 0.00036228192038834095], [0.9995310306549072, 5.457133738673292e-05, 0.00041444774251431227], [0.9995085000991821, 5.0293052481720224e-05, 0.0004412090638652444], [0.9993434548377991, 5.434164995676838e-05, 0.0006021973676979542], [0.9995898604393005, 5.390345904743299e-05, 0.0003562429337762296], [0.9995902180671692, 5.448502270155586e-05, 0.0003553523274604231], [0.9994472861289978, 5.369645805330947e-05, 0.0004989554872736335], [0.999394416809082, 5.9389501984696835e-05, 0.000546220107935369], [0.9968737363815308, 0.00014231374370865524, 0.002983882324770093], [0.9995157718658447, 5.6816585129126906e-05, 0.0004273542435839772], [0.9983609318733215, 8.849905134411529e-05, 0.0015504570910707116], [0.9996278285980225, 5.0926006224472076e-05, 0.00032125445432029665], [0.9995796084403992, 5.300035991240293e-05, 0.0003674003528431058], [0.9993599057197571, 4.819144669454545e-05, 0.0005918738315813243], [0.999131977558136, 6.404489249689505e-05, 0.0008039596723392606], [0.9991588592529297, 9.157278691418469e-05, 0.0007495649624615908], [0.9995284080505371, 4.946712215314619e-05, 0.00042214198037981987], [0.997952938079834, 9.407661127625033e-05, 0.0019529626006260514], [0.999567449092865, 4.7430727136088535e-05, 0.0003850793291348964], [0.9992744326591492, 7.209971954580396e-05, 0.0006534830899909139], [0.9984956979751587, 7.791598909534514e-05, 0.001426282455213368], [0.9996199607849121, 6.0613070672843605e-05, 0.0003193887823726982], [0.9996297359466553, 5.561745274462737e-05, 0.000314563192659989], [0.9995489716529846, 5.3054063755553216e-05, 0.0003979251778218895], [0.9994131326675415, 6.413798109861091e-05, 0.0005228029331192374], [0.9994696974754333, 5.5723161494825035e-05, 0.0004745006444863975], [0.9993995428085327, 7.215566438389942e-05, 0.0005282007623463869], [0.9908632636070251, 0.0010416486766189337, 0.008095006458461285], [0.9996342658996582, 6.881887384224683e-05, 0.00029679114231839776], [0.9994958639144897, 5.045022408012301e-05, 0.0004537494678515941], [0.9986554384231567, 8.452544716419652e-05, 0.0012600617483258247], [0.9996022582054138, 6.487870268756524e-05, 0.00033295960747636855], [0.9996084570884705, 5.7603279856266454e-05, 0.00033402713597752154], [0.9995912909507751, 6.848535122117028e-05, 0.0003401513386052102], [0.9995201826095581, 6.317748193396255e-05, 0.0004167012812104076], [0.9994263648986816, 5.649101149174385e-05, 0.0005172425298951566], [0.9994995594024658, 7.200655818451196e-05, 0.00042846708674915135], [0.9994068145751953, 6.410223431885242e-05, 0.0005290384287945926], [0.9995692372322083, 5.8187310060020536e-05, 0.00037266293657012284], [0.9996531009674072, 5.915544898016378e-05, 0.00028774948441423476], [0.9996376037597656, 5.63067733310163e-05, 0.00030603029881604016], [0.9996135830879211, 5.256361691863276e-05, 0.0003338632232043892], [0.9995494484901428, 6.327961455099285e-05, 0.00038727177889086306], [0.9996473789215088, 6.454564572777599e-05, 0.0002880336542148143], [0.9981550574302673, 9.194531594403088e-05, 0.0017529537435621023], [0.9994964599609375, 6.0410249716369435e-05, 0.0004430983099155128], [0.9996121525764465, 5.278366734273732e-05, 0.0003350691113155335], [0.011063147336244583, 0.5783411264419556, 0.41059574484825134], [0.999393105506897, 5.4339529015123844e-05, 0.0005525373853743076], [0.9993448853492737, 6.768292223569006e-05, 0.0005874290363863111], [0.9996203184127808, 6.280108937062323e-05, 0.0003169160627294332], [0.9987539052963257, 0.00012261542724445462, 0.0011234153062105179], [0.99959796667099, 4.97247492603492e-05, 0.0003523964260239154], [0.9995794892311096, 5.682367554982193e-05, 0.00036370777525007725], [0.999423623085022, 5.2580449846573174e-05, 0.0005238649901002645], [0.9994907379150391, 4.806267315871082e-05, 0.00046119026956148446], [0.9996200799942017, 5.967671677353792e-05, 0.0003202132065780461], [0.9994484782218933, 5.180344669497572e-05, 0.0004997184732928872], [0.9995960593223572, 5.529898407985456e-05, 0.0003486488130874932], [0.9994032382965088, 6.021056105964817e-05, 0.0005365439574234188], [0.9995658993721008, 4.841390182264149e-05, 0.00038575727376155555], [0.9996161460876465, 5.618160867015831e-05, 0.00032764210482127964], [0.9994292855262756, 6.047512943041511e-05, 0.0005101743154227734], [0.9994733929634094, 5.019306263420731e-05, 0.0004764329351019114], [0.9996399879455566, 6.078917067497969e-05, 0.0002991959627252072], [0.99952232837677, 5.7014203775906935e-05, 0.0004207262536510825], [0.9996426105499268, 6.36937256786041e-05, 0.0002937622193712741], [0.9995288848876953, 5.97878715780098e-05, 0.0004113659670110792], [0.9996004700660706, 5.940832124906592e-05, 0.0003400970599614084], [0.9996496438980103, 6.042048698873259e-05, 0.0002899235114455223], [0.8924790024757385, 0.0247512049973011, 0.08276979625225067], [0.9996367692947388, 6.535289867315441e-05, 0.0002979154232889414], [0.9995706677436829, 5.9587382565950975e-05, 0.0003697531356010586], [0.9995937943458557, 6.192138971528038e-05, 0.0003442530578467995], [0.9995943903923035, 5.4920121328905225e-05, 0.00035066864802502096], [0.9996365308761597, 6.880334694869816e-05, 0.0002947479661088437], [0.9994379878044128, 5.298830001265742e-05, 0.0005089390324428678], [0.9995629191398621, 4.943120075040497e-05, 0.00038761598989367485], [0.9995669722557068, 5.217427315074019e-05, 0.000380840094294399], [0.9987813830375671, 0.00011843876563943923, 0.001100173918530345], [0.9996391534805298, 6.118432793300599e-05, 0.0002996811526827514], [0.9995715022087097, 4.726055703940801e-05, 0.00038120191311463714], [0.9995469450950623, 5.444204361992888e-05, 0.0003986197989434004], [0.9996378421783447, 5.646741556120105e-05, 0.0003056993009522557], [0.9994827508926392, 6.0666508943540975e-05, 0.00045663159107789397], [0.999347984790802, 9.386303281644359e-05, 0.0005581156583502889], [0.9996019005775452, 7.013104186626151e-05, 0.00032798602478578687], [0.9996340274810791, 5.5573153076693416e-05, 0.00031036767177283764], [0.9995982050895691, 5.3837429732084274e-05, 0.00034796856925822794], [0.9989975094795227, 5.7494060456519946e-05, 0.0009449333301745355], [0.9996365308761597, 6.368768663378432e-05, 0.00029986220761202276], [0.9996200799942017, 6.12688745604828e-05, 0.00031867201323620975], [0.9992877840995789, 5.626570782624185e-05, 0.0006559956818819046], [0.9985023736953735, 0.0002688289969228208, 0.0012288483558222651], [0.9995999932289124, 5.4310617997543886e-05, 0.00034572879667393863], [0.9994621872901917, 5.882856930838898e-05, 0.00047894546878524125], [0.9995107650756836, 6.60286023048684e-05, 0.00042317729094065726], [0.9996262788772583, 6.196747563080862e-05, 0.00031173726893030107], [0.9993473887443542, 7.857623131712899e-05, 0.0005739881889894605], [0.9996287822723389, 6.004568422213197e-05, 0.00031115763704292476], [0.9993765950202942, 6.413539085770026e-05, 0.0005593900568783283], [0.9995713829994202, 5.4013718909118325e-05, 0.00037459892337210476], [0.9995285272598267, 5.524438529391773e-05, 0.00041623192373663187], [0.9995883107185364, 5.860644523636438e-05, 0.00035304963239468634], [0.0038440541829913855, 0.9815205931663513, 0.014635348692536354], [0.9995287656784058, 5.0161292165284976e-05, 0.0004211193881928921], [0.999143123626709, 7.58296373533085e-05, 0.0007809934904798865], [0.9995637536048889, 5.167766721569933e-05, 0.0003844959137495607], [0.999561607837677, 5.449031959869899e-05, 0.0003839410492219031], [0.9993882179260254, 5.933529610047117e-05, 0.0005524260923266411], [0.9996285438537598, 5.3321244195103645e-05, 0.0003181952051818371], [0.9971464276313782, 0.00020784890512004495, 0.0026456706691533327], [0.9996181726455688, 5.6163291446864605e-05, 0.00032566089066676795], [0.9996297359466553, 6.686707638436928e-05, 0.00030341855017468333], [0.999488115310669, 6.113425479270518e-05, 0.00045077165123075247], [0.999580442905426, 5.714174039894715e-05, 0.00036233157152310014], [0.9996168613433838, 6.344952998915687e-05, 0.0003197224286850542], [0.9996373653411865, 6.87490901327692e-05, 0.000293915654765442], [0.9995361566543579, 5.2214621973689646e-05, 0.0004116240015719086], [0.9995586276054382, 5.465253707370721e-05, 0.0003866671759169549], [0.9995298385620117, 5.749710544478148e-05, 0.00041260398575104773], [0.9995067119598389, 5.489268369274214e-05, 0.00043836652184836566], [0.9994482398033142, 5.2967116062063724e-05, 0.0004986790590919554], [0.9925859570503235, 0.0004825633077416569, 0.006931537762284279], [0.9994001388549805, 5.351157960831188e-05, 0.0005463370471261442], [0.9995548129081726, 5.447295916383155e-05, 0.0003906744241248816], [0.9996404647827148, 5.724979200749658e-05, 0.00030229275580495596], [0.999256432056427, 7.459600601578131e-05, 0.0006689520087093115], [0.9995254278182983, 5.574854003498331e-05, 0.0004188267921563238], [0.9995026588439941, 6.761708209523931e-05, 0.00042972309165634215], [0.9996308088302612, 6.104405474616215e-05, 0.00030817760853096843], [0.9996449947357178, 6.69638320687227e-05, 0.000288033508695662], [0.999075174331665, 7.883739453973249e-05, 0.0008459144155494869], [0.9995043277740479, 6.434160604840145e-05, 0.00043137362808920443], [0.9993131160736084, 7.598704542033374e-05, 0.0006109345122240484], [0.9996067881584167, 5.494364449987188e-05, 0.0003382524009793997], [0.9992855191230774, 6.337044760584831e-05, 0.000651155540253967], [0.9994970560073853, 5.407474236562848e-05, 0.00044883249211125076], [0.9938920736312866, 0.00026357138995081186, 0.00584430294111371], [0.9995110034942627, 7.01946482877247e-05, 0.0004187980084680021], [0.9995434880256653, 5.672785118804313e-05, 0.0003997567400801927], [0.9993355870246887, 7.840553735150024e-05, 0.0005859931698068976], [0.9995712637901306, 5.8770892792381346e-05, 0.000369959365343675], [0.9989087581634521, 6.674707401543856e-05, 0.0010243876604363322], [0.9996206760406494, 5.320998388924636e-05, 0.0003260579251218587], [0.9995641112327576, 5.497311576618813e-05, 0.00038089766167104244], [0.9992561936378479, 9.786801092559472e-05, 0.0006459266878664494], [0.9996535778045654, 6.054306140867993e-05, 0.0002858566294889897], [0.9995172023773193, 7.240723061840981e-05, 0.0004103976534679532], [0.9996080994606018, 5.752925426349975e-05, 0.00033442353014834225], [0.9996005892753601, 5.856805364601314e-05, 0.0003408432239666581], [0.9996381998062134, 6.570984260179102e-05, 0.0002961050486192107], [0.9996218681335449, 6.11290815868415e-05, 0.0003169649571646005], [0.9995250701904297, 6.99167067068629e-05, 0.00040496475412510335], [0.9994577765464783, 5.35692015546374e-05, 0.0004885689704678953], [0.9996153116226196, 6.597791070817038e-05, 0.00031864893389865756], [0.999517560005188, 5.900387986912392e-05, 0.0004235233354847878], [0.9995786547660828, 5.619461080641486e-05, 0.00036514835665002465], [0.999563992023468, 6.800875416956842e-05, 0.0003679559740703553], [0.9995802044868469, 5.788696216768585e-05, 0.00036187065416015685], [0.9993522763252258, 7.89327677921392e-05, 0.0005687572993338108], [0.9995230436325073, 5.1563467422965914e-05, 0.0004253613296896219], [0.9994909763336182, 4.8127403715625405e-05, 0.0004608758317772299], [0.9995933175086975, 6.533186387969181e-05, 0.0003414201783016324], [0.999619722366333, 5.970458732917905e-05, 0.00032058526994660497], [0.9996145963668823, 6.527371442643926e-05, 0.0003201067156624049], [0.9995606541633606, 5.867501022294164e-05, 0.0003806844470091164], [0.9995988011360168, 5.5294967751251534e-05, 0.0003459450963418931], [0.9996317625045776, 5.6257122196257114e-05, 0.0003119774628430605], [0.9996155500411987, 5.517348472494632e-05, 0.000329242495354265], [0.9987379908561707, 0.00011006724525941536, 0.0011520347325131297], [0.9995555281639099, 6.327300070552155e-05, 0.00038116201176308095], [0.9995473027229309, 6.446899351431057e-05, 0.00038825333467684686], [0.9995450377464294, 5.575989416684024e-05, 0.0003991665143985301], [0.9996358156204224, 6.042114910087548e-05, 0.00030371538014151156], [0.9991379976272583, 8.340024942299351e-05, 0.0007785967318341136], [0.999268114566803, 8.034268830670044e-05, 0.0006516536232084036], [0.9994376301765442, 5.162588058738038e-05, 0.0005107322940602899], [0.9996376037597656, 6.129887333372608e-05, 0.00030113576212897897], [0.9984309077262878, 0.00016814691480249166, 0.001400856301188469], [0.9995549321174622, 5.45143338968046e-05, 0.0003904488985426724], [0.9996176958084106, 6.119940371718258e-05, 0.0003210652794223279], [0.9995689988136292, 5.364546086639166e-05, 0.0003773828793782741], [0.9996024966239929, 5.862236866960302e-05, 0.0003389002522453666], [0.9996045231819153, 6.515177665278316e-05, 0.00033033679937943816], [0.9995923638343811, 7.396464206976816e-05, 0.0003336794616188854], [0.9994919300079346, 5.2189567213645205e-05, 0.0004558971559163183], [0.9995812773704529, 4.932186857331544e-05, 0.00036937359254807234], [0.9993984699249268, 5.793419768451713e-05, 0.0005436778301373124], [0.9994170665740967, 5.469129609991796e-05, 0.0005282798083499074], [0.9996287822723389, 6.0147471231175587e-05, 0.000311017909552902], [0.9995430707931519, 4.9578411562833935e-05, 0.00040737056406214833], [0.9986625909805298, 0.00010666168964235112, 0.0012307275319471955], [0.9995477795600891, 7.62638810556382e-05, 0.00037588246050290763], [0.9994206428527832, 7.001461926847696e-05, 0.0005092693609185517], [0.9995905756950378, 5.3539341024588794e-05, 0.0003559602773748338], [0.9994101524353027, 5.7372901210328564e-05, 0.0005324161029420793], [0.9995349645614624, 6.519044109154493e-05, 0.0003998421598225832], [0.999394416809082, 6.773056520614773e-05, 0.0005377850611694157], [0.9995720982551575, 5.3376435971586034e-05, 0.0003745488356798887], [0.9996576309204102, 6.995953299337998e-05, 0.0002724517835304141], [0.9996330738067627, 7.13277404429391e-05, 0.0002954719529952854], [0.9993461966514587, 7.78476387495175e-05, 0.0005759497871622443], [0.9995736479759216, 5.726639210479334e-05, 0.00036904611624777317], [0.9994679093360901, 5.460121246869676e-05, 0.0004775288689415902], [0.9993090629577637, 5.837579010403715e-05, 0.0006326144211925566], [0.999503493309021, 6.739169475622475e-05, 0.0004290420620236546], [0.9986423850059509, 0.00010098686470882967, 0.0012566503137350082], [0.9995970129966736, 5.2010851504746825e-05, 0.00035106006544083357], [0.9995691180229187, 6.176357419462875e-05, 0.0003691792953759432], [0.9996127486228943, 6.253691390156746e-05, 0.0003247284039389342], [0.9992849230766296, 5.5434928071917966e-05, 0.0006596918683499098], [0.9995774626731873, 5.406960553955287e-05, 0.0003684904659166932], [0.9995206594467163, 6.094995842431672e-05, 0.0004183198616374284], [0.9996252059936523, 5.902886186959222e-05, 0.0003157633764203638], [0.9876977801322937, 0.0014856710331514478, 0.010816539637744427], [0.9996569156646729, 6.600670894840732e-05, 0.0002770595019683242], [0.9974846839904785, 0.00014847418060526252, 0.0023667835630476475], [0.9993433356285095, 6.0539401602e-05, 0.0005961000570096076], [0.9995512366294861, 5.4209398513194174e-05, 0.00039446563459932804], [0.9995360374450684, 5.499395774677396e-05, 0.00040892677498050034], [0.9996405839920044, 6.061206659069285e-05, 0.0002987675543408841], [0.9996362924575806, 5.31603982381057e-05, 0.00031053920974954963], [0.9995114803314209, 5.985295138088986e-05, 0.0004287195042707026], [0.9995811581611633, 6.032193050486967e-05, 0.00035853206645697355], [0.999300479888916, 6.271803431445733e-05, 0.0006368975737132132], [0.999270498752594, 5.516237433766946e-05, 0.0006743207341060042], [0.9996188879013062, 5.410238009062596e-05, 0.0003270380257163197], [0.9995157718658447, 6.210503488546237e-05, 0.00042216843576170504], [0.9996101260185242, 6.20733480900526e-05, 0.0003277776122558862], [0.9994413256645203, 6.82521058479324e-05, 0.0004903571680188179], [0.9994787573814392, 5.8543657360132784e-05, 0.00046275887871161103], [0.9996511936187744, 6.098280209698714e-05, 0.00028780027059838176], [0.9996267557144165, 5.817983765155077e-05, 0.0003150333068333566], [0.999560534954071, 5.132553997100331e-05, 0.0003880613367073238], [0.9996577501296997, 6.228659185580909e-05, 0.00027992756804451346], [0.999504566192627, 6.042675886419602e-05, 0.00043502493645064533], [0.9996377229690552, 6.037909770384431e-05, 0.0003018333518411964], [0.9995903372764587, 5.299622716847807e-05, 0.000356680597178638], [0.9996115565299988, 5.868471271242015e-05, 0.0003298335650470108], [0.9995260238647461, 5.406140917330049e-05, 0.0004198938549961895], [0.9995694756507874, 6.0316506278468296e-05, 0.0003702678659465164], [0.9996551275253296, 6.249551370274276e-05, 0.000282459135632962], [0.9994593262672424, 5.684903226210736e-05, 0.00048389978474006057], [0.9995808005332947, 5.617715942207724e-05, 0.00036304458626545966], [0.9995896220207214, 5.18525812367443e-05, 0.0003585444937925786], [0.9995966553688049, 5.6491804571123794e-05, 0.00034685168066062033], [0.999595582485199, 5.49914657312911e-05, 0.00034946686355397105], [0.9995071887969971, 5.128204065840691e-05, 0.0004416101146489382], [0.9995620846748352, 5.663841147907078e-05, 0.00038129158201627433], [0.9995906949043274, 5.9115041949553415e-05, 0.0003502006293274462], [0.9994878768920898, 5.849865556228906e-05, 0.00045364873949438334], [0.999570906162262, 5.3439831390278414e-05, 0.0003757277154363692], [0.9995796084403992, 5.995335232000798e-05, 0.0003604797529987991], [0.9995973706245422, 5.588067870121449e-05, 0.00034681602846831083], [0.9996203184127808, 5.425767812994309e-05, 0.00032548027229495347], [0.9994553923606873, 5.5021308071445674e-05, 0.0004895197344012558], [0.9996249675750732, 5.571762449108064e-05, 0.0003192149742972106], [0.9991969466209412, 6.886332266731188e-05, 0.0007342536118812859], [0.9994727969169617, 6.521044997498393e-05, 0.00046208492130972445], [0.9995412826538086, 5.1178940339013934e-05, 0.00040745161822997034], [0.9995352029800415, 8.544764568796381e-05, 0.00037935844738967717], [0.9994389414787292, 5.0782877224264666e-05, 0.0005102648283354938], [0.9995892643928528, 5.282989877741784e-05, 0.0003580099728424102], [0.9996216297149658, 5.538247205549851e-05, 0.0003229483845643699], [0.9995120763778687, 4.9189606215804815e-05, 0.00043870077934116125], [0.9995021820068359, 6.506397039629519e-05, 0.00043274639756418765], [0.9996107220649719, 5.1277624152135104e-05, 0.00033805909333750606], [0.9996151924133301, 5.9752052038675174e-05, 0.0003250539302825928], [0.9994841814041138, 5.284036524244584e-05, 0.000462954310933128], [0.9996042847633362, 5.3005103836767375e-05, 0.0003427592455409467], [0.9994315505027771, 6.350531475618482e-05, 0.0005049766623415053], [0.9995338916778564, 5.87816430197563e-05, 0.0004073695163242519], [0.9996094107627869, 5.3945856052450836e-05, 0.00033662686473689973], [0.9995822310447693, 4.7263994929380715e-05, 0.0003704894916154444], [0.9995278120040894, 6.83550097164698e-05, 0.00040386265027336776], [0.9993908405303955, 6.455314724007621e-05, 0.0005445675342343748], [0.9996142387390137, 5.4474225180456415e-05, 0.00033128331415355206], [0.9994627833366394, 7.300296419998631e-05, 0.00046415036194957793], [0.9995773434638977, 5.051909465692006e-05, 0.00037209526635706425], [0.9995383024215698, 4.9556194426259026e-05, 0.00041210802737623453], [0.9995444416999817, 5.857604992343113e-05, 0.00039692845894023776], [0.9995593428611755, 6.477002170868218e-05, 0.00037585830432362854], [0.9996005892753601, 6.278613000176847e-05, 0.0003366349556017667], [0.999523401260376, 5.7872359320754185e-05, 0.00041866942774504423], [0.9996507167816162, 6.158380710985512e-05, 0.0002876382495742291], [0.9996371269226074, 6.0839913203381e-05, 0.00030194601276889443], [0.9995238780975342, 5.06202268297784e-05, 0.0004254569939803332], [0.9992415904998779, 7.124046533135697e-05, 0.0006872082012705505], [0.9995720982551575, 5.328178667696193e-05, 0.0003746066940948367], [0.9996187686920166, 5.691289698006585e-05, 0.00032421021023765206], [0.9996078610420227, 5.542132930713706e-05, 0.00033680329215712845], [0.996369481086731, 0.00011367863771738485, 0.0035168647300451994], [0.9995222091674805, 7.737377018202096e-05, 0.00040052112308330834], [0.9996211528778076, 6.275324267335236e-05, 0.0003160205960739404], [0.9843196272850037, 0.0008300362969748676, 0.014850313775241375], [0.9992067217826843, 0.00011120897397631779, 0.0006820462294854224], [0.9994876384735107, 5.082003917777911e-05, 0.00046145106898620725], [0.999594509601593, 7.409068348351866e-05, 0.00033141361200250685], [0.9995875954627991, 5.568834239966236e-05, 0.00035679255961440504], [0.999212384223938, 6.993331044213846e-05, 0.0007176344515755773], [0.9995974898338318, 5.494093056768179e-05, 0.0003476237179711461], [0.9996192455291748, 5.651162064168602e-05, 0.00032429880229756236], [0.9994764924049377, 5.604496254818514e-05, 0.0004674992524087429], [0.9994810223579407, 7.284057937795296e-05, 0.0004462297074496746], [0.9994381070137024, 8.522858115611598e-05, 0.0004767068021465093], [0.9996201992034912, 5.190131923882291e-05, 0.0003278178337495774], [0.9995924830436707, 4.9613310693530366e-05, 0.0003579377371352166], [0.9991287589073181, 7.07362123648636e-05, 0.0008005215786397457], [0.9991225600242615, 8.191694359993562e-05, 0.0007954585016705096], [0.999393105506897, 5.675615830114111e-05, 0.0005501153063960373], [0.9996225833892822, 7.494284363929182e-05, 0.00030244424124248326], [0.999311089515686, 6.779044633731246e-05, 0.0006211076397448778], [0.9972444772720337, 0.00015041965525597334, 0.00260518048889935], [0.9991913437843323, 8.640668238513172e-05, 0.0007223378052003682], [0.999612033367157, 5.943949145148508e-05, 0.0003285898419562727], [0.9993977546691895, 6.096611832617782e-05, 0.0005412663449533284], [0.9995895028114319, 6.323117122519761e-05, 0.00034727135789580643], [0.9995313882827759, 4.910057032248005e-05, 0.00041948867146857083], [0.9984250068664551, 0.00010448526154505089, 0.0014704924542456865], [0.99943608045578, 6.164208753034472e-05, 0.0005022856639698148], [0.9996463060379028, 6.475168629549444e-05, 0.0002889404131565243], [0.9996160268783569, 6.0107882745796815e-05, 0.0003239052020944655], [0.9995538592338562, 6.118453165981919e-05, 0.0003849400964099914], [0.9996059536933899, 5.331698775989935e-05, 0.0003408406628295779], [0.9995054006576538, 6.257990025915205e-05, 0.0004320121952332556], [0.9996545314788818, 6.517195288324729e-05, 0.00028027634834870696], [0.9995726943016052, 6.263827526709065e-05, 0.00036464768345467746], [0.99860018491745, 0.00019281131972093135, 0.0012069691438227892], [0.9993274211883545, 7.347567589022219e-05, 0.000599132792558521], [0.9988189339637756, 9.974263230105862e-05, 0.0010812178952619433], [0.999648928642273, 5.954040898359381e-05, 0.00029150108457542956], [0.9996451139450073, 6.317177030723542e-05, 0.0002917792007792741], [0.9995381832122803, 5.208557195146568e-05, 0.00040975798037834466], [0.9995965361595154, 5.223002881393768e-05, 0.0003512859402690083], [0.9994553923606873, 5.59733271074947e-05, 0.0004886228125542402], [0.999101996421814, 6.007748743286356e-05, 0.0008379918290302157], [0.9995049238204956, 7.035876478767022e-05, 0.00042467814637348056], [0.9994168281555176, 5.879572199773975e-05, 0.0005242935148999095], [0.9994847774505615, 6.221763760549948e-05, 0.0004529947182163596], [0.9994075298309326, 5.4680862376699224e-05, 0.0005378100904636085], [0.9993987083435059, 4.97537330375053e-05, 0.0005514941876754165], [0.9996391534805298, 6.253166066017002e-05, 0.0002982931036967784], [0.9994450211524963, 6.87759238644503e-05, 0.0004861823399551213], [0.9995622038841248, 6.0245933127589524e-05, 0.0003775318618863821], [0.9993267059326172, 5.5489930673502386e-05, 0.0006178173352964222], [0.9993450045585632, 5.2374136430444196e-05, 0.0006025697803124785], [0.9996217489242554, 5.824044637847692e-05, 0.0003199271159246564], [0.9993413090705872, 6.077031139284372e-05, 0.0005979729467071593], [0.999529242515564, 6.272168684517965e-05, 0.0004081052902620286], [0.9994404911994934, 5.37141713721212e-05, 0.0005057264352217317], [0.999624490737915, 5.977785986033268e-05, 0.0003156668390147388], [0.9995682835578918, 5.487952512339689e-05, 0.0003767951566260308], [0.9995902180671692, 5.2008712373208255e-05, 0.0003577990282792598], [0.998242974281311, 0.00010565418051555753, 0.0016514057060703635], [0.9986739158630371, 8.057313243625686e-05, 0.0012454591924324632], [0.9993257522583008, 7.27273800293915e-05, 0.0006016517290845513], [0.9995637536048889, 5.3176165238255635e-05, 0.00038311161915771663], [0.9995998740196228, 5.70308075111825e-05, 0.0003431158547755331], [0.9996017813682556, 5.371582301449962e-05, 0.00034449523082003], [0.999586284160614, 5.430852615972981e-05, 0.00035936900530941784], [0.999190628528595, 0.00012173520372016355, 0.0006876727566123009], [0.9994611144065857, 5.198270810069516e-05, 0.0004868938121944666], [0.9996523857116699, 5.9190508181927726e-05, 0.0002883935230784118], [0.9996104836463928, 5.941639392403886e-05, 0.0003300968965049833], [0.9996161460876465, 7.116285996744409e-05, 0.00031264423159882426], [0.999645471572876, 6.114399729995057e-05, 0.0002934447256848216], [0.9989545345306396, 9.607959509594366e-05, 0.0009493071702308953], [0.9975494742393494, 0.0001669833727646619, 0.0022835340350866318], [0.9976907968521118, 0.0001639193360460922, 0.0021453658118844032], [0.9995056390762329, 6.578445027116686e-05, 0.00042863975977525115], [0.99959796667099, 5.8189263654639944e-05, 0.0003438857092987746], [0.9993941783905029, 5.757984035881236e-05, 0.0005483071436174214], [0.9995297193527222, 5.239040910964832e-05, 0.00041789538227021694], [0.9989765882492065, 7.455363811459392e-05, 0.0009488872601650655], [0.9995573163032532, 6.548532837769017e-05, 0.00037711815093643963], [0.9995744824409485, 5.200928353588097e-05, 0.0003735136706382036], [0.9996121525764465, 5.176471677259542e-05, 0.00033613870618864894], [0.9996424913406372, 5.692532431567088e-05, 0.00030050150235183537], [0.9984851479530334, 9.844712621998042e-05, 0.001416405662894249], [0.9994686245918274, 5.43258101970423e-05, 0.0004770146624650806], [0.9994457364082336, 5.149545177118853e-05, 0.000502822978887707], [0.9996111989021301, 5.502143540070392e-05, 0.00033381214598193765], [0.9995357990264893, 5.035124922869727e-05, 0.00041382116614840925], [0.9995279312133789, 4.8627178330207244e-05, 0.0004234206862747669], [0.9995477795600891, 5.275410512695089e-05, 0.0003994638682343066], [0.9993982315063477, 5.474036152008921e-05, 0.0005470774485729635], [0.9986164569854736, 0.00031302188290283084, 0.001070583239197731], [0.998802900314331, 9.539217717247084e-05, 0.0011017736978828907], [0.9992775321006775, 7.698596164118499e-05, 0.0006455020047724247], [0.9995720982551575, 5.496500671142712e-05, 0.00037296637310646474], [0.9994816184043884, 5.338417395250872e-05, 0.0004650279588531703], [0.9996328353881836, 5.717727981391363e-05, 0.000309912720695138], [0.9994433522224426, 6.20652426732704e-05, 0.000494512845762074], [0.999231219291687, 5.828813664265908e-05, 0.0007105820113793015], [0.9992520213127136, 7.595921488245949e-05, 0.0006719689699821174], [0.9990420937538147, 7.895644375821576e-05, 0.0008788908598944545], [0.9996322393417358, 6.664595275651664e-05, 0.00030115252593532205], [0.9995470643043518, 4.925677058054134e-05, 0.0004036457685288042], [0.9995982050895691, 5.3506464610109106e-05, 0.00034828533534891903], [0.7400158643722534, 0.004685582127422094, 0.2552986443042755], [0.9996147155761719, 5.652704567182809e-05, 0.0003288123698439449], [0.028509803116321564, 0.02376093529164791, 0.9477292895317078], [0.9996179342269897, 5.414186671259813e-05, 0.0003279659431427717], [0.9996504783630371, 6.359863618854433e-05, 0.0002858519146684557], [0.9994156360626221, 7.524275133619085e-05, 0.0005091053317300975], [0.9993460774421692, 5.350633728085086e-05, 0.0006004776805639267], [0.9995948672294617, 5.868048538104631e-05, 0.00034648587461560965], [0.9994670748710632, 5.7190431107301265e-05, 0.0004757334536407143], [0.9995344877243042, 4.8263940698234364e-05, 0.0004172972112428397], [0.9993651509284973, 5.8363537391414866e-05, 0.0005764302331954241], [0.9995661377906799, 5.4586580517934635e-05, 0.00037922480260021985], [0.9995661377906799, 5.4539959819521755e-05, 0.000379232777049765], [0.0039196377620100975, 0.9815701246261597, 0.014510247856378555], [0.9995447993278503, 5.621618765871972e-05, 0.0003989707329310477], [0.9996325969696045, 6.32787196082063e-05, 0.0003040972806047648], [0.9995488524436951, 4.812133192899637e-05, 0.0004029603151138872], [0.999620795249939, 6.11599170952104e-05, 0.00031806016340851784], [0.9994524121284485, 7.759461732348427e-05, 0.0004700272111222148], [0.9991670846939087, 8.162479207385331e-05, 0.0007513145101256669], [0.9995583891868591, 6.419920100597665e-05, 0.0003773345670197159], [0.9995038509368896, 5.340445932233706e-05, 0.00044280168367549777], [0.9994770884513855, 4.739180440083146e-05, 0.00047554110642522573], [0.9994261264801025, 6.22012885287404e-05, 0.00051168876234442], [0.9989041090011597, 7.087371341185644e-05, 0.0010250650811940432], [0.9996019005775452, 5.550013156607747e-05, 0.00034255319042131305], [0.9992369413375854, 7.508315320592374e-05, 0.0006880509899929166], [0.9996349811553955, 6.824905722169206e-05, 0.00029675060068257153], [0.999618649482727, 5.658541340380907e-05, 0.00032469595316797495], [0.9995137453079224, 5.385403710533865e-05, 0.0004324213950894773], [0.999546468257904, 6.306170689640567e-05, 0.00039050535997375846], [0.9995706677436829, 5.2623054216383025e-05, 0.000376600946765393], [0.9995482563972473, 6.429634231608361e-05, 0.0003873283858411014], [0.998058021068573, 0.00017759249021764845, 0.001764309941790998], [0.9996103644371033, 5.966071330476552e-05, 0.000329985428834334], [0.9992690682411194, 5.838069409946911e-05, 0.0006725718267261982], [0.9993522763252258, 5.625018457067199e-05, 0.0005915369256399572], [0.9995589852333069, 5.497718302649446e-05, 0.0003861178702209145], [0.9996175765991211, 6.23839587206021e-05, 0.00031999778002500534], [0.9994121789932251, 5.8704910770757124e-05, 0.0005291537963785231], [0.9995835423469543, 5.4405627452069893e-05, 0.0003619997587520629], [0.9995971322059631, 5.787066402263008e-05, 0.0003450061776675284], [0.9995309114456177, 6.272591417655349e-05, 0.0004063912492711097], [0.9994872808456421, 5.549646448343992e-05, 0.0004571582830976695], [0.9995720982551575, 5.6160439271479845e-05, 0.0003717323997989297], [0.9995020627975464, 5.113024963065982e-05, 0.00044677755795419216], [0.9996410608291626, 5.784699897048995e-05, 0.00030118448194116354], [0.9995991587638855, 5.163156311027706e-05, 0.00034927885280922055], [0.9996172189712524, 6.0961949202464893e-05, 0.00032170326448976994], [0.9996036887168884, 5.537630204344168e-05, 0.0003409067285247147], [0.9994031190872192, 4.954897303832695e-05, 0.00054728495888412], [0.9994939565658569, 5.8559293393045664e-05, 0.00044753518886864185], [0.9995526671409607, 5.9881855122512206e-05, 0.000387323263566941], [0.9996278285980225, 6.297848449321464e-05, 0.00030918820993974805], [0.9994551539421082, 4.746399645227939e-05, 0.0004974275943823159], [0.9995455145835876, 5.196459460421465e-05, 0.0004025372618343681], [0.9996488094329834, 6.0001206293236464e-05, 0.00029117066878825426], [0.9995357990264893, 5.893859270145185e-05, 0.000405205151764676], [0.9993746876716614, 5.7470420870231465e-05, 0.0005679114838130772], [0.9995700716972351, 5.159575448487885e-05, 0.0003783315187320113], [0.9996053576469421, 5.9377118304837495e-05, 0.0003352723433636129], [0.9992852807044983, 5.751714707002975e-05, 0.0006572462152689695], [0.999219536781311, 0.00012212117144372314, 0.0006583745125681162], [0.9995599389076233, 5.298364703776315e-05, 0.0003870356595143676], [0.9995303153991699, 5.4564639867749065e-05, 0.0004151888133492321], [0.9993970394134521, 5.5242177040781826e-05, 0.0005476684309542179], [0.9995446801185608, 5.060365947429091e-05, 0.0004046588728670031], [0.9995773434638977, 5.407011485658586e-05, 0.0003685580741148442], [0.9996345043182373, 5.9375681303208694e-05, 0.0003061647876165807], [0.999370276927948, 6.486067286459729e-05, 0.0005649180384352803], [0.9993139505386353, 6.128300447016954e-05, 0.0006248649442568421], [0.9994962215423584, 5.2890412916895e-05, 0.000450824765721336], [0.9815219640731812, 0.0012342989211902022, 0.01724366843700409], [0.9995236396789551, 5.350451829144731e-05, 0.0004228648031130433], [0.9995113611221313, 5.584806058323011e-05, 0.00043280364479869604], [0.9994409680366516, 5.930850238655694e-05, 0.0004997378564439714], [0.9996201992034912, 6.378834223141894e-05, 0.0003159883781336248], [0.9993853569030762, 6.180129275890067e-05, 0.0005528446054086089], [0.9964794516563416, 0.00021542207105085254, 0.0033052375074476004], [0.9992765784263611, 7.208192982943729e-05, 0.0006513404659926891], [0.9993910789489746, 5.44274334970396e-05, 0.0005545081221498549], [0.9995980858802795, 5.59232612431515e-05, 0.0003460216976236552], [0.9991888403892517, 0.00012490633525885642, 0.0006861697183921933], [0.9989746809005737, 8.985117892734706e-05, 0.000935605785343796], [0.9996057152748108, 5.690721809514798e-05, 0.00033731412258930504], [0.9993689656257629, 5.108951518195681e-05, 0.0005798906786367297], [0.9995726943016052, 5.833419709233567e-05, 0.00036888272734358907], [0.9995980858802795, 5.410084122559056e-05, 0.0003477816062513739], [0.999534010887146, 4.9270034651272e-05, 0.00041685233009047806], [0.9991795420646667, 6.634189048781991e-05, 0.0007540806545875967], [0.9994363188743591, 5.3475767344934866e-05, 0.0005101114511489868], [0.9995367527008057, 6.318618397926912e-05, 0.0004001388733740896], [0.9993647933006287, 5.665332355420105e-05, 0.0005785434623248875], [0.9978492259979248, 0.00011907616135431454, 0.0020317400339990854], [0.9988811612129211, 8.59746869537048e-05, 0.0010328219505026937], [0.9973790645599365, 0.00022401018941309303, 0.0023969635367393494], [0.9994949102401733, 6.045787813491188e-05, 0.000444604957010597], [0.9995895028114319, 5.055580913904123e-05, 0.00035996202495880425], [0.9995840191841125, 7.211833872133866e-05, 0.00034393108217045665], [0.9965140223503113, 0.00018501786689739674, 0.00330102676525712], [0.9996440410614014, 6.277700595092028e-05, 0.00029310755780898035], [0.9991165995597839, 8.119552512653172e-05, 0.0008021583198569715], [0.9995266199111938, 5.246419823379256e-05, 0.0004210112674627453], [0.9995020627975464, 5.4527412430616096e-05, 0.00044333538971841335], [0.9996386766433716, 6.467232014983892e-05, 0.0002965508319903165], [0.999546468257904, 5.689836325473152e-05, 0.0003965225478168577], [0.9995471835136414, 5.611631786450744e-05, 0.0003966472577303648], [0.9934183359146118, 0.00019846216309815645, 0.006383202038705349], [0.9986873269081116, 9.379355469718575e-05, 0.0012188426917418838], [0.9995883107185364, 5.8000328863272443e-05, 0.0003537184093147516], [0.9995712637901306, 5.2352283091749996e-05, 0.0003764771390706301], [0.9990870952606201, 8.613654063083231e-05, 0.0008268491947092116], [0.999617338180542, 5.658695590682328e-05, 0.00032600582926534116], [0.9996228218078613, 5.2138191676931456e-05, 0.0003249048313591629], [0.9996044039726257, 6.195713649503887e-05, 0.00033369657467119396], [0.998668909072876, 6.656922050751746e-05, 0.0012645636452361941], [0.9996193647384644, 5.86818132433109e-05, 0.0003220044309273362], [0.9996050000190735, 5.005147977499291e-05, 0.00034494424471631646], [0.9996383190155029, 5.6729550124146044e-05, 0.0003049010701943189], [0.9995718598365784, 5.447768489830196e-05, 0.0003737113147508353], [0.999454915523529, 5.448819501907565e-05, 0.000490636273752898], [0.9996129870414734, 5.961604983895086e-05, 0.00032742301118560135], [0.9995822310447693, 5.585286999121308e-05, 0.0003619416384026408], [0.9994903802871704, 5.141070141689852e-05, 0.0004582644905894995], [0.9992775321006775, 5.338640767149627e-05, 0.0006691163871437311], [0.9996047616004944, 6.087799192755483e-05, 0.00033432483905926347], [0.9993624091148376, 5.329932173481211e-05, 0.0005843787221238017], [0.9993417859077454, 5.5301268730545416e-05, 0.0006029742071405053], [0.998910665512085, 7.797811122145504e-05, 0.0010113135213032365], [0.9994626641273499, 5.5363314459100366e-05, 0.000481949100503698], [0.9994716048240662, 5.965794844087213e-05, 0.0004687791515607387], [0.9996160268783569, 5.517387398867868e-05, 0.00032875005854293704], [0.9978140592575073, 0.00012857277761213481, 0.002057391917333007], [0.9994434714317322, 5.128900011186488e-05, 0.0005052998312748969], [0.9995145797729492, 7.06125283613801e-05, 0.00041474815225228667], [0.9995310306549072, 5.099644840811379e-05, 0.0004179549578111619], [0.976277768611908, 0.0015701991505920887, 0.022151997312903404], [0.9993485808372498, 6.788414611946791e-05, 0.0005834634648635983], [0.9996115565299988, 6.004619353916496e-05, 0.0003284511622041464], [0.9980306029319763, 0.00011849282600451261, 0.0018508874345570803], [0.999283492565155, 5.1644950872287154e-05, 0.0006648554117418826], [0.999610960483551, 5.5573455028934404e-05, 0.0003335580986458808], [0.9989104270935059, 9.55511350184679e-05, 0.0009940536692738533], [0.9992650151252747, 5.1089074986521155e-05, 0.0006839190027676523], [0.9994480013847351, 5.522887295228429e-05, 0.0004967284039594233], [0.9995524287223816, 7.054621528368443e-05, 0.00037706346483901143], [0.999640703201294, 5.953617437626235e-05, 0.00029973190976306796], [0.999542236328125, 4.92759354528971e-05, 0.00040840686415322125], [0.999626874923706, 6.706192652927712e-05, 0.00030607226653955877], [0.9995701909065247, 6.028152711223811e-05, 0.00036950604408048093], [0.999600350856781, 6.158129690447822e-05, 0.00033806607825681567], [0.9994899034500122, 4.788543810718693e-05, 0.00046216181362979114], [0.9994074106216431, 5.371039515011944e-05, 0.0005388689460232854], [0.9995635151863098, 8.240741590270773e-05, 0.0003540756879374385], [0.9995681643486023, 5.240097380010411e-05, 0.0003793543728534132], [0.9995259046554565, 5.396090273279697e-05, 0.0004201658011879772], [0.9994971752166748, 5.209749360801652e-05, 0.0004507542762439698], [0.9994369149208069, 6.7657143517863e-05, 0.0004954067408107221], [0.9995829463005066, 5.77995742787607e-05, 0.0003592872526496649], [0.999352753162384, 5.3000661864643916e-05, 0.0005942084826529026], [0.9993906021118164, 7.18488372513093e-05, 0.0005375415203161538], [0.9992963075637817, 6.290160672506317e-05, 0.0006407391047105193], [0.9996127486228943, 5.522512583411299e-05, 0.0003321290423627943], [0.999606192111969, 5.739843982155435e-05, 0.00033636760781519115], [0.9994603991508484, 7.35784342396073e-05, 0.00046593346633017063], [0.9996340274810791, 6.832009967183694e-05, 0.00029770954279229045], [0.9995059967041016, 6.119035242591053e-05, 0.0004327757633291185], [0.9994966983795166, 5.022249024477787e-05, 0.0004529662255663425], [0.9994950294494629, 5.240923564997502e-05, 0.0004525170661509037], [0.999518632888794, 5.314683949109167e-05, 0.0004281809669919312], [0.9995725750923157, 6.785806181142107e-05, 0.0003595865855459124], [0.9985119700431824, 0.0001077707638614811, 0.0013803314650431275], [0.9994114637374878, 5.528597830561921e-05, 0.0005333106964826584], [0.9993139505386353, 6.116062286309898e-05, 0.0006249829311855137], [0.9995923638343811, 4.9803536967374384e-05, 0.00035781346377916634], [0.9996272325515747, 5.925148434471339e-05, 0.0003135159786324948], [0.9803348779678345, 0.0010610955068841577, 0.018603991717100143], [0.9994931221008301, 6.31140501354821e-05, 0.0004437630996108055], [0.9993829727172852, 5.093752406537533e-05, 0.0005660491297021508], [0.9996205568313599, 6.32002847851254e-05, 0.0003162085195071995], [0.9996428489685059, 5.643076656269841e-05, 0.00030068220803514123], [0.999525785446167, 5.9401612816145644e-05, 0.00041477871127426624], [0.9996284246444702, 5.334341403795406e-05, 0.00031819212017580867], [0.9996103644371033, 5.266350126476027e-05, 0.00033700413769111037], [0.9993066787719727, 5.7593410019762814e-05, 0.000635763630270958], [0.9984638690948486, 0.00012397696264088154, 0.001412139623425901], [0.9994571805000305, 5.653350308421068e-05, 0.0004863090580329299], [0.9993994235992432, 5.952010178589262e-05, 0.00054101028945297], [0.99955815076828, 6.162245699670166e-05, 0.00038026535185053945], [0.9995966553688049, 4.867102688876912e-05, 0.00035477764322422445], [0.9991482496261597, 7.342440221691504e-05, 0.0007783255423419178], [0.9995266199111938, 6.232655141502619e-05, 0.00041106462595053017], [0.9996179342269897, 5.291920751915313e-05, 0.0003291745379101485], [0.9995816349983215, 5.1679624448297545e-05, 0.00036669886321760714], [0.9992361068725586, 6.198152550496161e-05, 0.0007019660552032292], [0.999349057674408, 6.029958603903651e-05, 0.0005907074664719403], [0.9995309114456177, 5.676490764017217e-05, 0.0004124415572732687], [0.9984057545661926, 0.00017571696662344038, 0.0014185922918841243], [0.9987295269966125, 0.00010733180533861741, 0.0011630177032202482], [0.9995915293693542, 5.997156404191628e-05, 0.00034854080877266824], [0.9995375871658325, 5.6135046179406345e-05, 0.00040625775000080466], [0.8037294149398804, 0.01835797354578972, 0.17791268229484558], [0.9995525479316711, 6.13732700003311e-05, 0.0003861212753690779], [0.9994716048240662, 5.9215650253463537e-05, 0.0004691626818384975], [0.9993979930877686, 4.7582059778505936e-05, 0.000554524885956198], [0.999618411064148, 5.893784327781759e-05, 0.00032262661261484027], [0.9996032118797302, 5.3411298722494394e-05, 0.00034341064747422934], [0.9996354579925537, 6.214695167727768e-05, 0.0003022687742486596], [0.9996422529220581, 7.296991680050269e-05, 0.0002848237054422498], [0.9994901418685913, 5.0951101002283394e-05, 0.00045884601422585547], [0.9996113181114197, 5.2901643357472494e-05, 0.00033575575798749924], [0.9994105100631714, 5.429644443211146e-05, 0.0005351253785192966], [0.999370276927948, 5.377359047997743e-05, 0.0005759557243436575], [0.9995296001434326, 5.339993003872223e-05, 0.0004170202591922134], [0.9995089769363403, 5.921058254898526e-05, 0.0004317670245654881], [0.9995591044425964, 5.217988291406073e-05, 0.0003886915510520339], [0.9993701577186584, 7.855501462472603e-05, 0.000551347155123949], [0.9995381832122803, 5.1792882004519925e-05, 0.0004099625803064555], [0.9133087992668152, 0.0012431396171450615, 0.08544803410768509], [0.9984525442123413, 0.0001479500497225672, 0.001399476546794176], [0.9995090961456299, 4.997532960260287e-05, 0.0004410225956235081], [0.08351858705282211, 0.018900126218795776, 0.8975812792778015], [0.999483585357666, 5.0675902457442135e-05, 0.00046580543857999146], [0.9995155334472656, 4.6987021050881594e-05, 0.0004375824355520308], [0.9994483590126038, 6.985602522036061e-05, 0.00048178641009144485], [0.9995993971824646, 5.238052472122945e-05, 0.0003482068423181772], [0.999531626701355, 5.583454912994057e-05, 0.00041252761729992926], [0.9995194673538208, 6.296403444139287e-05, 0.0004176817019470036], [0.9991866946220398, 9.236815094482154e-05, 0.0007209445466287434], [0.99959796667099, 6.209710409166291e-05, 0.00034001804306171834], [0.9996225833892822, 5.399423753260635e-05, 0.0003233844763599336], [0.9993264675140381, 5.618916839011945e-05, 0.0006173665751703084], [0.9993941783905029, 7.128751167329028e-05, 0.0005345987738110125], [0.9995394945144653, 7.311154331546277e-05, 0.0003873770765494555], [0.9994454979896545, 5.066382436780259e-05, 0.000503860239405185], [0.9974732995033264, 0.00020436861086636782, 0.002322225598618388], [0.9994020462036133, 6.090432361816056e-05, 0.0005370831349864602], [0.9978808760643005, 0.00014489989553112537, 0.001974186161532998], [0.9993984699249268, 7.309662760235369e-05, 0.0005283546634018421], [0.999593198299408, 4.9602465878706425e-05, 0.0003572479181457311], [0.9983891248703003, 0.00017969132750295103, 0.0014312205603346229], [0.9995629191398621, 5.122199218021706e-05, 0.00038585325819440186], [0.9996427297592163, 5.97779217059724e-05, 0.0002974130620714277], [0.9994901418685913, 5.19262976013124e-05, 0.00045793128083460033], [0.9986407160758972, 9.862684964900836e-05, 0.0012606579111889005], [0.9996256828308105, 6.0643193137366325e-05, 0.0003136096929665655], [0.9994966983795166, 4.5866414438933134e-05, 0.00045746591058559716], [0.9996433258056641, 5.605885962722823e-05, 0.0003006585466209799], [0.9994999170303345, 6.468376523116603e-05, 0.00043540933984331787], [0.9996373653411865, 5.696082007489167e-05, 0.0003056297719012946], [0.9996451139450073, 6.403416773537174e-05, 0.0002908920869231224], [0.9995166063308716, 5.184780457057059e-05, 0.0004315694095566869], [0.9995693564414978, 5.309183688950725e-05, 0.00037754408549517393], [0.9993107318878174, 6.618293264182284e-05, 0.0006230372819118202], [0.9995673298835754, 5.1677998271770775e-05, 0.0003808945184573531], [0.9995989203453064, 5.299921031109989e-05, 0.0003480446757748723], [0.999174177646637, 5.92210708418861e-05, 0.0007666181190870702], [0.9996362924575806, 5.449215677799657e-05, 0.00030927633633837104], [0.9996249675750732, 5.718812099075876e-05, 0.00031781132565811276], [0.9995104074478149, 5.1391631131991744e-05, 0.00043828104389831424], [0.9996041655540466, 5.868248626939021e-05, 0.0003372676146682352], [0.9993921518325806, 6.714286428177729e-05, 0.0005406184936873615], [0.9996234178543091, 6.080625462345779e-05, 0.0003157959727104753], [0.9996337890625, 5.851071182405576e-05, 0.0003077063011005521], [0.9995224475860596, 6.30646973149851e-05, 0.00041450702701695263], [0.9962408542633057, 0.0001551888999529183, 0.0036038998514413834], [0.9995852112770081, 5.243451232672669e-05, 0.0003623291850090027], [0.9996453523635864, 6.304244743660092e-05, 0.0002915787335950881], [0.999639630317688, 5.6193326599895954e-05, 0.00030414058710448444], [0.9996108412742615, 6.863242742838338e-05, 0.0003205288958270103], [0.9996139407157898, 5.38725835212972e-05, 0.00033220608020201325], [0.9995631575584412, 5.6078693887684494e-05, 0.00038069882430136204], [0.9995567202568054, 5.369696009438485e-05, 0.000389543769415468], [0.9995015859603882, 6.697817298118025e-05, 0.0004313872195780277], [0.9983590245246887, 0.00011501149128889665, 0.0015260068466886878], [0.999566376209259, 7.295735122170299e-05, 0.0003606546379160136], [0.9995777010917664, 6.186473183333874e-05, 0.0003604220109991729], [0.00820817518979311, 0.7750664949417114, 0.21672537922859192], [0.9994140863418579, 5.7845281844493e-05, 0.0005280054756440222], [0.9996150732040405, 5.80805390200112e-05, 0.00032685030600987375], [0.9996192455291748, 6.367918831529096e-05, 0.000317130412440747], [0.9996101260185242, 5.6423967180307955e-05, 0.0003334185166750103], [0.9994599223136902, 5.100205817143433e-05, 0.000489106634631753], [0.9995414018630981, 5.331689681042917e-05, 0.00040532994898967445], [0.9995399713516235, 5.4517418902833015e-05, 0.0004055064346175641], [0.9996334314346313, 5.363337186281569e-05, 0.0003129070973955095], [0.999380350112915, 5.8095094573218375e-05, 0.0005614803521893919], [0.9995797276496887, 5.0364891649223864e-05, 0.00036991378874517977], [0.9974614381790161, 0.0003715973871294409, 0.0021669059060513973], [0.9989469647407532, 0.00011660196469165385, 0.0009364309953525662], [0.999573290348053, 6.943294283701107e-05, 0.00035724780173040926], [0.9996019005775452, 6.265157571760938e-05, 0.00033542729215696454], [0.9995748400688171, 5.228630834608339e-05, 0.00037282140692695975], [0.9972876310348511, 0.00017880082305055112, 0.002533550374209881], [0.9993659853935242, 4.935137621941976e-05, 0.0005846578860655427], [0.9995239973068237, 5.171633529243991e-05, 0.0004242711002007127], [0.999514102935791, 5.0612055929377675e-05, 0.0004352272371761501], [0.9994320273399353, 6.542593473568559e-05, 0.0005025155260227621], [0.9995895028114319, 4.90842285216786e-05, 0.0003614430606830865], [0.999643087387085, 5.3835741709917784e-05, 0.0003030920051969588], [0.9996119141578674, 5.306104139890522e-05, 0.00033507286570966244], [0.99946528673172, 5.211272218730301e-05, 0.00048261682968586683], [0.9995723366737366, 5.7363718951819465e-05, 0.00037038300069980323], [0.9969689249992371, 0.0002094785450026393, 0.0028215704951435328], [0.998725950717926, 9.515432611806318e-05, 0.0011789293494075537], [0.9996076226234436, 5.4383646784117445e-05, 0.0003380666021257639], [0.9995809197425842, 5.1205533964093775e-05, 0.00036785026895813644], [0.999221682548523, 6.793413194827735e-05, 0.0007103712414391339], [0.9995554089546204, 5.707272430299781e-05, 0.0003874758258461952], [0.9995304346084595, 5.1626586355268955e-05, 0.00041794750723056495], [0.9995625615119934, 5.479334504343569e-05, 0.00038260145811364055], [0.9979479908943176, 0.00013627314183395356, 0.0019158351933583617], [0.9995915293693542, 5.729647818952799e-05, 0.0003511518589220941], [0.999619722366333, 6.306632712949067e-05, 0.0003172424912918359], [0.9996286630630493, 6.045916597940959e-05, 0.00031085123191587627], [0.9990279674530029, 7.028583058854565e-05, 0.000901735620573163], [0.9995908141136169, 5.858518852619454e-05, 0.0003506028442643583], [0.9993114471435547, 6.611044227611274e-05, 0.0006223821546882391], [0.9996262788772583, 6.091426621424034e-05, 0.00031283352291211486], [0.9995526671409607, 6.151309207780287e-05, 0.0003858695272356272], [0.9995266199111938, 5.6631430197739974e-05, 0.00041679953574202955], [0.9996065497398376, 5.9056474128738046e-05, 0.00033433051430620253], [0.9994348883628845, 7.482309592887759e-05, 0.000490293197799474], [0.9995272159576416, 6.68622597004287e-05, 0.00040599695057608187], [0.9994677901268005, 6.265929550863802e-05, 0.0004695550014730543], [0.9994370341300964, 5.4130003263708204e-05, 0.0005088766920380294], [0.9996117949485779, 5.427718861028552e-05, 0.00033407218870706856], [0.9993240833282471, 6.029221185599454e-05, 0.0006156477029435337], [0.9981145858764648, 0.00013018870959058404, 0.0017552077770233154], [0.9995449185371399, 5.5023821914801374e-05, 0.0003999559849034995], [0.999625563621521, 5.721270645153709e-05, 0.00031724738073535264], [0.9994578957557678, 5.387814962887205e-05, 0.00048824489931575954], [0.9436957836151123, 0.003102534217759967, 0.05320165678858757], [0.9996312856674194, 6.20180944679305e-05, 0.00030678664916194975], [0.9995248317718506, 5.400064401328564e-05, 0.0004211826017126441], [0.999596893787384, 6.135281728347763e-05, 0.0003418126725591719], [0.9993875026702881, 7.545397966168821e-05, 0.0005370354629121721], [0.999355137348175, 6.64334511384368e-05, 0.0005783638334833086], [0.9996144771575928, 5.579105709330179e-05, 0.000329682050505653], [0.9994756579399109, 7.856848242226988e-05, 0.00044582513510249555], [0.9996205568313599, 5.2874849643558264e-05, 0.0003264947736170143], [0.9996423721313477, 6.445434701163322e-05, 0.00029311433900147676], [0.9995613694190979, 5.187356873648241e-05, 0.00038682424928992987], [0.9996076226234436, 5.505803710548207e-05, 0.00033740600338205695], [0.9992457628250122, 6.897284038132057e-05, 0.0006853392696939409], [0.9995397329330444, 5.1284641813253984e-05, 0.0004090720321983099], [0.9995216131210327, 6.18931298959069e-05, 0.0004165733407717198], [0.9984423518180847, 0.00013131211744621396, 0.0014263796620070934], [0.9995218515396118, 5.966243406874128e-05, 0.00041846002568490803], [0.9995999932289124, 5.168575080460869e-05, 0.00034826964838430285], [0.9995033740997314, 4.8430272727273405e-05, 0.00044826316297985613], [0.9987609386444092, 0.0001320488372584805, 0.0011070319451391697], [0.9995695948600769, 5.711224730475806e-05, 0.0003732819459401071], [0.9990418553352356, 9.588386456016451e-05, 0.0008623008034192026], [0.9995049238204956, 5.376581248128787e-05, 0.0004413585993461311], [0.999439537525177, 5.676491855410859e-05, 0.0005037260125391185], [0.9996011853218079, 5.242929546511732e-05, 0.0003464035107754171], [0.9995952248573303, 5.597613926511258e-05, 0.00034881028113886714], [0.9996026158332825, 5.2996223530499265e-05, 0.0003443268360570073], [0.9993675351142883, 7.675529195694253e-05, 0.0005557338590733707], [0.9996474981307983, 6.206957914400846e-05, 0.00029045893461443484], [0.9996213912963867, 5.5653650633757934e-05, 0.0003228777786716819], [0.9993785619735718, 7.052620640024543e-05, 0.0005508246831595898], [0.9993247985839844, 6.048057184671052e-05, 0.0006147352396510541], [0.9995600581169128, 5.2821607823716477e-05, 0.00038713630056008697], [0.9992635846138, 0.0001042108196998015, 0.0006321601686067879], [0.9996421337127686, 5.890608736081049e-05, 0.00029902398819103837], [0.9995474219322205, 6.006943294778466e-05, 0.00039244111394509673], [0.999609649181366, 5.447267176350579e-05, 0.00033589883241802454], [0.9996460676193237, 7.04721242072992e-05, 0.00028351772925816476], [0.19991667568683624, 0.020870165899395943, 0.7792131900787354], [0.9995052814483643, 5.452675759443082e-05, 0.00044009991688653827], [0.9995614886283875, 5.0887378165498376e-05, 0.00038760402821935713], [0.9996281862258911, 6.739739910699427e-05, 0.0003043274627998471], [0.9995631575584412, 5.661773684551008e-05, 0.0003801938146352768], [0.9992263317108154, 6.935015699127689e-05, 0.000704377016518265], [0.9995972514152527, 5.951519051450305e-05, 0.000343269610311836], [0.999572217464447, 5.198769213166088e-05, 0.000375817297026515], [0.9967932105064392, 0.00010879552428377792, 0.0030979160219430923], [0.9996097683906555, 5.247841909294948e-05, 0.0003378341207280755], [0.9995137453079224, 5.645997589454055e-05, 0.00042983441380783916], [0.999242901802063, 5.783967935713008e-05, 0.0006992180715315044], [0.9995400905609131, 5.7355850003659725e-05, 0.0004026502138003707], [0.9994500279426575, 4.7558220103383064e-05, 0.0005024100537411869], [0.9994651675224304, 5.5033026001183316e-05, 0.00047974343760870397], [0.9995796084403992, 5.5887518101371825e-05, 0.0003644487005658448], [0.9996083378791809, 5.912640335736796e-05, 0.00033262016950175166], [0.9996274709701538, 5.2284274715930223e-05, 0.0003201838117092848], [0.9995834231376648, 5.737146784667857e-05, 0.00035930800368078053], [0.9986593723297119, 0.00011887318396475166, 0.0012216655304655433], [0.9982234835624695, 0.00010094113531522453, 0.0016755681717768312], [0.9995433688163757, 7.344096957240254e-05, 0.00038316738209687173], [0.9995343685150146, 5.3607087465934455e-05, 0.00041208916809409857], [0.9995174407958984, 5.975988460704684e-05, 0.0004227952449582517], [0.9995574355125427, 5.475106445373967e-05, 0.00038784407661296427], [0.9994391798973083, 5.43802379979752e-05, 0.0005064393626525998], [0.9996090531349182, 5.5413576774299145e-05, 0.0003356149245519191], [0.9996376037597656, 5.857860378455371e-05, 0.0003037475107703358], [0.9996331930160522, 5.609669096884318e-05, 0.0003106603107880801], [0.9995405673980713, 5.7192883105017245e-05, 0.0004022873181384057], [0.9992757439613342, 6.934504926903173e-05, 0.0006549432873725891], [0.9995545744895935, 5.1811959565384313e-05, 0.0003936152206733823], [0.9996082186698914, 5.721312845707871e-05, 0.000334588170517236], [0.9993796348571777, 6.41979422653094e-05, 0.0005560971330851316], [0.9994569420814514, 4.8790905566420406e-05, 0.000494198058731854], [0.9995108842849731, 5.1947481551906094e-05, 0.000437110778875649], [0.9990957975387573, 6.972743722144514e-05, 0.0008345062960870564], [0.9995697140693665, 5.36022380401846e-05, 0.00037667929427698255], [0.9989396929740906, 9.05683555174619e-05, 0.0009696399793028831], [0.9995751976966858, 6.059625229681842e-05, 0.0003641921211965382], [0.9995187520980835, 5.665080971084535e-05, 0.00042464208672754467], [0.999169111251831, 6.718243821524084e-05, 0.0007637204835191369], [0.9993933439254761, 5.533498551812954e-05, 0.0005513594369404018], [0.9995972514152527, 5.999776112730615e-05, 0.0003427424526307732], [0.9993576407432556, 9.11631213966757e-05, 0.0005512661300599575], [0.9996457099914551, 5.5169886763906106e-05, 0.0002991075161844492], [0.9995332956314087, 5.178018182050437e-05, 0.00041484611574560404], [0.999508261680603, 5.893719207961112e-05, 0.0004328039358370006], [0.9996277093887329, 5.468519520945847e-05, 0.00031762797152623534], [0.9996423721313477, 5.4612926760455593e-05, 0.000303085136692971], [0.9993300437927246, 6.24384410912171e-05, 0.0006075745332054794], [0.9996523857116699, 5.8215908211423084e-05, 0.0002893064229283482], [0.9969152212142944, 0.00020604283781722188, 0.002878728089854121], [0.9990283250808716, 7.936997280921787e-05, 0.0008923508576117456], [0.9996066689491272, 6.0238613514229655e-05, 0.0003330846084281802], [0.9991617202758789, 6.652742013102397e-05, 0.0007716546533629298], [0.9994563460350037, 6.520080205518752e-05, 0.00047842771164141595], [0.9988937973976135, 8.323351357830688e-05, 0.0010229786857962608], [0.9994608759880066, 7.306697079911828e-05, 0.00046609723358415067], [0.9996269941329956, 6.440125434892252e-05, 0.0003086287761107087], [0.999553382396698, 5.382423842092976e-05, 0.0003927965590264648], [0.999554455280304, 6.755394133506343e-05, 0.00037796105607412755], [0.9994838237762451, 5.602197052212432e-05, 0.0004601621476467699], [0.9995958209037781, 5.13277409481816e-05, 0.00035278976429253817], [0.9993427395820618, 6.153108552098274e-05, 0.0005957601824775338], [0.9995424747467041, 4.686745160142891e-05, 0.00041072122985497117], [0.9995712637901306, 4.92062208650168e-05, 0.00037954116123728454], [0.9991668462753296, 6.98964431649074e-05, 0.0007633124478161335], [0.9996210336685181, 5.8094119594898075e-05, 0.0003207572444807738], [0.9996572732925415, 6.069938535802066e-05, 0.00028209388256073], [0.9996001124382019, 5.76939492020756e-05, 0.0003422397712711245], [0.9995260238647461, 6.140395271359012e-05, 0.00041264606988988817], [0.9975337982177734, 0.00012179672921774909, 0.0023443887475878], [0.9995936751365662, 6.480294541688636e-05, 0.0003415609826333821], [0.9994018077850342, 6.11601717537269e-05, 0.0005371114239096642], [0.9994196891784668, 5.458909799926914e-05, 0.0005257264710962772], [0.9996367692947388, 6.106671207817271e-05, 0.0003021599550265819], [0.9994638562202454, 5.3383075282908976e-05, 0.000482735107652843], [0.9994962215423584, 5.9556248743319884e-05, 0.0004442308854777366], [0.9995250701904297, 5.2751594921573997e-05, 0.00042217617738060653], [0.9995067119598389, 4.884869122179225e-05, 0.0004444783553481102], [0.9972864389419556, 0.00011438186629675329, 0.00259914412163198], [0.999546229839325, 5.803164094686508e-05, 0.00039568686042912304], [0.9995711445808411, 5.678343586623669e-05, 0.00037195938057266176], [0.9996588230133057, 6.260192458285019e-05, 0.0002786260156426579], [0.9995782971382141, 4.9776728701544926e-05, 0.0003719352826010436], [0.9993084669113159, 6.671254959655926e-05, 0.0006248207064345479], [0.9994420409202576, 8.476379298372194e-05, 0.0004731998487841338], [0.9994938373565674, 5.193025208427571e-05, 0.00045416958164423704], [0.9994543194770813, 5.517321187653579e-05, 0.0004905270179733634], [0.9990187883377075, 8.263236668426543e-05, 0.0008985391468741], [0.99960857629776, 5.000051896786317e-05, 0.0003414491657167673], [0.9995501637458801, 5.2966664952691644e-05, 0.00039673058199696243], [0.9995439648628235, 6.0583901358768344e-05, 0.00039548473432660103], [0.9996458292007446, 6.969778769416735e-05, 0.00028446040232665837], [0.999640941619873, 5.9939822676824406e-05, 0.00029908583383075893], [0.9995371103286743, 6.381388811860234e-05, 0.000398994714487344], [0.9992281198501587, 6.461858720285818e-05, 0.0007072879234328866], [0.9996108412742615, 5.0670223572524264e-05, 0.0003385787713341415], [0.9995984435081482, 5.190003867028281e-05, 0.00034966605016961694], [0.9978970289230347, 0.00012589697143994272, 0.001977044390514493], [0.999464213848114, 5.476653313962743e-05, 0.00048095188685692847], [0.9996127486228943, 5.441837856778875e-05, 0.00033278437331318855], [0.9991838335990906, 6.58554708934389e-05, 0.0007502373773604631], [0.9995447993278503, 6.857634434709325e-05, 0.0003865860344376415], [0.9994032382965088, 8.5140869487077e-05, 0.0005115569802001119], [0.9994856119155884, 5.5843000154709443e-05, 0.00045848480658605695], [0.9996156692504883, 6.0030637541785836e-05, 0.0003242621023673564], [0.9996337890625, 5.254714051261544e-05, 0.00031363076413981616], [0.9996515512466431, 5.89197916269768e-05, 0.0002895841607823968], [0.9995249509811401, 5.0013262807624415e-05, 0.00042502456926740706], [0.9995757937431335, 5.317503018886782e-05, 0.0003710125165525824], [0.9996291399002075, 6.0405822296161205e-05, 0.0003104449133388698], [0.997738242149353, 0.00015287545102182776, 0.002108928980305791], [0.9996362924575806, 5.946719102212228e-05, 0.00030417900416068733], [0.999422550201416, 6.016408224240877e-05, 0.0005172516102902591], [0.9994637370109558, 5.748338298872113e-05, 0.00047885760432109237], [0.9984879493713379, 0.0001306904450757429, 0.0013813084224238992], [0.9994909763336182, 5.690893522114493e-05, 0.0004520136280916631], [0.9996190071105957, 5.335211972123943e-05, 0.0003275396302342415], [0.9994565844535828, 5.446911382023245e-05, 0.000488995632622391], [0.9995443224906921, 5.2112172852503136e-05, 0.00040356614044867456], [0.9995618462562561, 4.9941660108743235e-05, 0.0003881606680806726], [0.9996393918991089, 6.068735456210561e-05, 0.0002999288553837687], [0.9994737505912781, 4.829277531825937e-05, 0.00047801737673580647], [0.9995914101600647, 5.781009895144962e-05, 0.0003508117515593767], [0.9995142221450806, 5.311235145200044e-05, 0.0004325919726397842], [0.9996247291564941, 6.187437247717753e-05, 0.0003133681311737746], [0.9993886947631836, 6.95002090651542e-05, 0.0005418295622803271], [0.9986686706542969, 0.00010496182949282229, 0.0012263525277376175], [0.999578058719635, 5.027996303397231e-05, 0.00037160367355681956], [0.9996365308761597, 5.608307765214704e-05, 0.0003073203261010349], [0.9992756247520447, 5.909172978135757e-05, 0.0006652440642938018], [0.4789160490036011, 0.009443201124668121, 0.511640727519989], [0.9996131062507629, 5.0679278501775116e-05, 0.0003363093128427863], [0.9995463490486145, 4.9449005018686876e-05, 0.0004042202199343592], [0.9995904564857483, 4.860560147790238e-05, 0.0003608957340475172], [0.9994469285011292, 5.9544901887420565e-05, 0.0004934541648253798], [0.9995232820510864, 4.6603407099610195e-05, 0.0004301487351767719], [0.9995761513710022, 5.469421375892125e-05, 0.00036918240948580205], [0.9995957016944885, 6.326721631921828e-05, 0.00034105684608221054], [0.99953293800354, 5.1195551350247115e-05, 0.0004158155934419483], [0.9995594620704651, 5.0804595957743004e-05, 0.0003897856513503939], [0.9995716214179993, 5.006547144148499e-05, 0.00037837610580027103], [0.9996234178543091, 5.435863204183988e-05, 0.0003222724189981818], [0.9996050000190735, 6.831603241153061e-05, 0.0003267902648076415], [0.9994460940361023, 5.055429573985748e-05, 0.0005033452180214226], [0.999579131603241, 5.83673499932047e-05, 0.0003624316886998713], [0.9993880987167358, 6.942842446733266e-05, 0.0005424231057986617], [0.9995573163032532, 6.705821579089388e-05, 0.0003755493962671608], [0.9989234805107117, 8.542411524103954e-05, 0.000991168199107051], [0.9995730519294739, 5.1496805099304765e-05, 0.00037541528581641614], [0.9979548454284668, 0.00017697294242680073, 0.001868109218776226], [0.9995935559272766, 5.892530316486955e-05, 0.00034751364728435874], [0.9995322227478027, 5.789048009319231e-05, 0.0004099228244740516], [0.9995212554931641, 6.220911018317565e-05, 0.0004166063736192882], [0.9995734095573425, 5.410296216723509e-05, 0.00037244491977617145], [0.9996181726455688, 6.928194488864392e-05, 0.00031254292116500437], [0.9996476173400879, 6.134188879514113e-05, 0.000290962983854115], [0.9988231062889099, 0.00015495617117267102, 0.0010219239629805088], [0.9996236562728882, 5.591728040599264e-05, 0.00032046824344433844], [0.9992402791976929, 8.189096843125299e-05, 0.0006779549876227975], [0.9995099306106567, 5.812767631141469e-05, 0.000431978318374604], [0.9991208910942078, 5.7708377426024526e-05, 0.0008212965331040323], [0.9995703101158142, 5.867031723028049e-05, 0.00037105384399183095], [0.9996416568756104, 5.61366650799755e-05, 0.0003022300370503217], [0.999380350112915, 5.933516877121292e-05, 0.0005602864548563957], [0.9994370341300964, 7.209247269202024e-05, 0.0004908870905637741], [0.9994646906852722, 5.87735376029741e-05, 0.00047654990339651704], [0.9995194673538208, 5.188811701373197e-05, 0.00042875809594988823], [0.9995978474617004, 5.7032815675484017e-05, 0.00034507570671848953], [0.9996243715286255, 5.683023846358992e-05, 0.0003188174741808325], [0.9995819926261902, 4.9840276915347204e-05, 0.0003682559181470424], [0.9996330738067627, 5.852116009918973e-05, 0.0003083426272496581], [0.9996436834335327, 5.908193634240888e-05, 0.0002972029324155301], [0.9993945360183716, 6.999730976531282e-05, 0.0005353989545255899], [0.9995790123939514, 5.59895379410591e-05, 0.00036502943839877844], [0.9995585083961487, 6.785587902413681e-05, 0.0003737202496267855], [0.9996271133422852, 5.966694880044088e-05, 0.0003131931880488992], [0.9994089603424072, 6.252613820834085e-05, 0.0005285075167194009], [0.9993579983711243, 6.435826799133793e-05, 0.0005776348407380283], [0.9816766977310181, 0.0008626611088402569, 0.017460569739341736], [0.9996169805526733, 6.897807179484516e-05, 0.00031403941102325916], [0.9996334314346313, 6.223467062227428e-05, 0.0003042968164663762], [0.9994531273841858, 5.513275391422212e-05, 0.00049173878505826], [0.9995996356010437, 6.006403054925613e-05, 0.0003403784357942641], [0.9996317625045776, 6.745383871020749e-05, 0.00030083232559263706], [0.9995934367179871, 5.58112733415328e-05, 0.0003508482768665999], [0.9991198182106018, 0.00010419384489068761, 0.0007759477011859417], [0.9994820952415466, 5.628045983030461e-05, 0.0004616804653778672], [0.9991323351860046, 6.611258868360892e-05, 0.0008015924249775708], [0.999612033367157, 5.164173126104288e-05, 0.00033642989001236856], [0.9994792342185974, 5.307191167958081e-05, 0.00046774494694545865], [0.9995741248130798, 6.549604586325586e-05, 0.0003602934011723846], [0.9995457530021667, 7.60132897994481e-05, 0.0003782577405218035], [0.9996492862701416, 6.449131615227088e-05, 0.00028621056117117405], [0.9992430210113525, 6.785749428672716e-05, 0.0006891127559356391], [0.9996520280838013, 6.312847835943103e-05, 0.00028481316985562444], [0.9995431900024414, 5.250420872471295e-05, 0.00040427406202070415], [0.9995861649513245, 6.0944996221223846e-05, 0.00035296185524202883], [0.9994496703147888, 7.059905328787863e-05, 0.00047961866948753595], [0.9996156692504883, 5.4957021347945556e-05, 0.000329343049088493], [0.9994457364082336, 5.236358629190363e-05, 0.0005018076044507325], [0.9988217949867249, 7.085192919475958e-05, 0.001107380841858685], [0.9991058707237244, 8.594992686994374e-05, 0.0008080526604317129], [0.9995132684707642, 4.858498141402379e-05, 0.0004381546168588102], [0.9993174076080322, 6.88231666572392e-05, 0.0006137249292805791], [0.9996211528778076, 6.311575270956382e-05, 0.000315614917781204], [0.9996681213378906, 6.40727739664726e-05, 0.00026782703935168684], [0.9995086193084717, 5.803581734653562e-05, 0.00043340199044905603], [0.9992696642875671, 6.111354741733521e-05, 0.0006692135357297957], [0.9995249509811401, 5.435083949123509e-05, 0.00042076711542904377], [0.9996404647827148, 6.079731974750757e-05, 0.00029877322958782315], [0.9994572997093201, 5.351270374376327e-05, 0.0004891798016615212], [0.9996083378791809, 5.390671867644414e-05, 0.00033782224636524916], [0.9996256828308105, 6.012016092427075e-05, 0.0003141577763017267], [0.9993818998336792, 7.335568807320669e-05, 0.0005447454750537872], [0.9996334314346313, 6.244411633815616e-05, 0.00030410909675993025], [0.9996111989021301, 5.87809736316558e-05, 0.0003300968382973224], [0.9995627999305725, 5.903097553527914e-05, 0.0003782463609240949], [0.9996156692504883, 5.0027298129862174e-05, 0.00033422617707401514], [0.9987421631813049, 0.00014871139137540013, 0.0011091394117102027]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "infers = []\n",
    "infer_labels = []\n",
    "\n",
    "print(i, \"th Test.... ========================================\")\n",
    "MODEL_NAME = '/mnt/HDD4T/egg2018037024/ABSA_pol_best'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model.to(device)\n",
    "dataloader = DataLoader(test_POL_klue_sets, batch_size=4, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "output_pred = []\n",
    "output_prob = []\n",
    "labels = []\n",
    "\n",
    "for z, data in enumerate(tqdm(dataloader)):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=data['input_ids'].to(device),\n",
    "            attention_mask=data['attention_mask'].to(device),\n",
    "            token_type_ids=data['token_type_ids'].to(device)\n",
    "        )\n",
    "    logits = outputs[0]\n",
    "    prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    result = np.argmax(logits, axis=-1)\n",
    "    labels.append(data['label'].tolist())\n",
    "\n",
    "    output_pred.append(result)\n",
    "    output_prob.append(prob)\n",
    "\n",
    "pred_answer, output_prob = np.concatenate(output_pred).tolist(), np.concatenate(output_prob, axis=0).tolist()\n",
    "\n",
    "\n",
    "print(pred_answer)\n",
    "print(output_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04932941",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T09:05:36.356539Z",
     "start_time": "2022-12-10T09:05:36.338554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2083 2083\n",
      "Accuracy:  0.9774363898223716\n",
      "Precision: 0.9672919150872744\n",
      "Recall: 0.9774363898223716\n",
      "F1-score: 0.9716460637302271\n"
     ]
    }
   ],
   "source": [
    "labelss = []\n",
    "\n",
    "for i in labels:\n",
    "    for j in i:\n",
    "        labelss.append(j)\n",
    "\n",
    "print(len(labelss), len(pred_answer))\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labelss, \n",
    "                                                    pred_answer, average=\"weighted\")\n",
    "acc = accuracy_score(labelss, pred_answer)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 - tf",
   "language": "python",
   "name": "python3-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
