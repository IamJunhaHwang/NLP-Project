{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c2608d1",
   "metadata": {},
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c912dc3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:14.679226Z",
     "start_time": "2022-12-08T07:49:12.439743Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5f5b299",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:14.768471Z",
     "start_time": "2022-12-08T07:49:14.681102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: <class 'torch.cuda.device'>\n",
      "Count of using GPUs: 2\n",
      "Current cuda device: 0\n"
     ]
    }
   ],
   "source": [
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  # 디버깅 위한 세팅\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print('Device:', torch.cuda.device)  # 출력결과: cuda \n",
    "print('Count of using GPUs:', torch.cuda.device_count())   \n",
    "print('Current cuda device:', torch.cuda.current_device()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e99d71c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T10:11:57.190753Z",
     "start_time": "2022-12-02T10:11:57.186018Z"
    }
   },
   "source": [
    "### Load Data\n",
    "Data가 위치한 PATH에서 Data를 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c79d02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:14.789254Z",
     "start_time": "2022-12-08T07:49:14.771648Z"
    }
   },
   "outputs": [],
   "source": [
    "def jsonload(fname, encoding=\"utf-8\"):\n",
    "    with open(fname, encoding=encoding) as f:\n",
    "        j = json.load(f)\n",
    "\n",
    "    return j\n",
    "\n",
    "# json 개체를 파일이름으로 깔끔하게 저장\n",
    "def jsondump(j, fname):\n",
    "    with open(fname, \"w\", encoding=\"UTF8\") as f:\n",
    "        json.dump(j, f, ensure_ascii=False)\n",
    "\n",
    "# jsonl 파일 읽어서 list에 저장\n",
    "def jsonlload(fname, encoding=\"utf-8\"):\n",
    "    json_list = []\n",
    "    with open(fname, encoding=encoding) as f:\n",
    "        for line in f.readlines():\n",
    "            json_list.append(json.loads(line))\n",
    "    return json_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "574f7c53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:14.842146Z",
     "start_time": "2022-12-08T07:49:14.792021Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = jsonlload('nikluge-sa-2022-train.jsonl')\n",
    "dev_data = jsonlload('nikluge-sa-2022-dev.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000ae698",
   "metadata": {},
   "source": [
    "## Preprocessing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9260a723",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:14.846975Z",
     "start_time": "2022-12-08T07:49:14.843674Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(text: str, only_kor: bool = True):\n",
    "    \"\"\"한국어 문장을 옵션에 맞게 전처리\"\"\"\n",
    "    # 한국어 모음과 특수 문자, 숫자 및 영어 제거\n",
    "    if only_kor:\n",
    "        text = re.sub(f\"[^가-힣| |]+\", \"\", text)\n",
    "    else:\n",
    "        text = re.sub(f\"[^가-힣|ㄱ-ㅎ|0-9|]+\", \"\", text)\n",
    "\n",
    "    # 연속 공백 제거\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "\n",
    "    # 좌우 불필요한 공백 제거\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d5db16",
   "metadata": {},
   "source": [
    "## 재현성 위한 seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20c859ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:14.860536Z",
     "start_time": "2022-12-08T07:49:14.848290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed:int = 1004):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "    \n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495015f4",
   "metadata": {},
   "source": [
    "## 태그셋 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "658b9bcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:14.886752Z",
     "start_time": "2022-12-08T07:49:14.861811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#개체#속성 태그셋 정의\n",
    "entity_property_pair = [\n",
    "    '제품 전체#품질', '제품 전체#편의성', '제품 전체#디자인', '제품 전체#일반', '제품 전체#가격', \n",
    "    '제품 전체#인지도',  '제품 전체#다양성',\n",
    "    '패키지/구성품#디자인', '패키지/구성품#가격', '패키지/구성품#다양성', '패키지/구성품#일반',\n",
    "    '패키지/구성품#편의성', '패키지/구성품#품질',\n",
    "    '본품#일반', '본품#다양성', '본품#품질', '본품#편의성', '본품#디자인', '본품#가격',\n",
    "    '브랜드#인지도', '브랜드#일반', '브랜드#디자인', '브랜드#품질', '브랜드#가격']\n",
    "\n",
    "rep_entity_property_pair = [\n",
    "    '제품전체#품질', '제품전체#편의성', '제품전체#디자인', '제품전체#일반', '제품전체#가격', \n",
    "    '제품전체#인지도',\n",
    "    '패키지/구성품#디자인', '패키지/구성품#일반',\n",
    "    '패키지/구성품#편의성', '패키지/구성품#품질',\n",
    "    '본품#일반', '본품#다양성', '본품#품질', '본품#편의성', '본품#디자인',\n",
    "    '브랜드#인지도', '브랜드#일반', '브랜드#품질']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Train데이터에 등장 안하거나 희소하게 등장한 태그들 ==> rep에서 모두 지워주었음.\n",
    "\n",
    "제품 전체#다양성  0\n",
    "\n",
    "패키지/구성품#가격 0 \n",
    "패키지/구성품#다양성 1\n",
    "\n",
    "본품#인지도 1\n",
    "본품#가격  2\n",
    "본품#인지도 1\n",
    "\n",
    "브랜드#디자인  0\n",
    "브랜드#가격  3\n",
    "\"\"\"\n",
    "\n",
    "len(rep_entity_property_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aa8c8e",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b53f3d7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:20.608036Z",
     "start_time": "2022-12-08T07:49:14.888928Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='klue/roberta-large', vocab_size=32000, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = 'klue/roberta-large'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d29a80d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:20.616615Z",
     "start_time": "2022-12-08T07:49:20.610899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "special_tokens_dict = {\n",
    "    'additional_special_tokens': rep_entity_property_pair\n",
    "}\n",
    "\n",
    "print(tokenizer.vocab_size)\n",
    "\n",
    "# 토크나이저는 각 모델 별로 만들지 않아도 된다. [임베딩 layer에만 영향 주므로]\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "print(num_added_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8a12794",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:20.637809Z",
     "start_time": "2022-12-08T07:49:20.619012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['야', '아이패드', '가격', '왜', '이래', '본품#일반']\n",
      "{'input_ids': [0, 1396, 15641, 3852, 1460, 5625, 1163, 2425, 7, 3852, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] 야 아이패드 가격 왜 이래 본품 # 가격 [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"야 아이패드 가격 왜 이래 본품#일반\"))\n",
    "print(tokenizer(\"야 아이패드 가격 왜 이래 본품#가격\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"야 아이패드 가격 왜 이래 본품#가격\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71935b36",
   "metadata": {},
   "source": [
    "## Load  ACD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7cd3fbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:24.818898Z",
     "start_time": "2022-12-08T07:49:20.640226Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32018, 1024)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACD_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, \n",
    "                        num_labels=2)  #True/False \n",
    "ACD_model.resize_token_embeddings(tokenizer.vocab_size + num_added_toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb992cd5",
   "metadata": {},
   "source": [
    "## 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63df4574",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:24.823219Z",
     "start_time": "2022-12-08T07:49:24.820733Z"
    }
   },
   "outputs": [],
   "source": [
    "polarity_id_to_name = ['positive', 'negative', 'neutral']  # 차례대로 0, 1, 2\n",
    "polarity_name_to_id = {polarity_id_to_name[i]: i for i in range(len(polarity_id_to_name))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eb8ee7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:24.900958Z",
     "start_time": "2022-12-08T07:49:24.825784Z"
    }
   },
   "outputs": [],
   "source": [
    "ACD_Datas = []  \n",
    "ACD_labels = []\n",
    "\n",
    "POL_Datas = []\n",
    "POL_labels = []\n",
    "\n",
    "### sentence들 먼저 모으고 한꺼번에 tokenize 하자\n",
    "\n",
    "\n",
    "for datas in train_data:\n",
    "    sen = preprocess(datas['sentence_form'])\n",
    "    annos = datas['annotation']  # 여러 개 일 수도 있음. (이중 리스트)\n",
    "    \n",
    "    for annotation in annos:\n",
    "        for pair in rep_entity_property_pair:\n",
    "            entity_property = annotation[0]  # raw_data의 annotation 추출\n",
    "            entity_property = entity_property.replace(\" \", \"\")  # 띄어쓰기 제거\n",
    "            polarity = annotation[2]\n",
    "\n",
    "            if entity_property == pair:\n",
    "                ACD_Datas.append(sen + \" \" + pair) \n",
    "                ACD_labels.append(1)\n",
    "                POL_Datas.append(sen + \" \" + pair)\n",
    "                POL_labels.append(polarity_name_to_id[polarity])\n",
    "\n",
    "            else:\n",
    "                ACD_Datas.append(sen + \" \" + pair) \n",
    "                ACD_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "201c8708",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:24.906323Z",
     "start_time": "2022-12-08T07:49:24.902393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58590, 58590, 3248, 3248)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ACD_Datas), len(ACD_labels), len(POL_Datas), len(POL_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69b51e42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:24.921865Z",
     "start_time": "2022-12-08T07:49:24.908719Z"
    }
   },
   "outputs": [],
   "source": [
    "devs = dev_data[:838]\n",
    "tests = dev_data[838:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89e67cba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:24.958331Z",
     "start_time": "2022-12-08T07:49:24.924368Z"
    }
   },
   "outputs": [],
   "source": [
    "################### Dev set #######################\n",
    "dev_ACD_Datas = []  \n",
    "dev_ACD_labels = []\n",
    "\n",
    "dev_POL_Datas = []\n",
    "dev_POL_labels = []\n",
    "\n",
    "### sentence들 먼저 모으고 한꺼번에 tokenize 하자\n",
    "\n",
    "\n",
    "for datas in devs:\n",
    "    sen = preprocess(datas['sentence_form'])\n",
    "    annos = datas['annotation']  # 여러 개 일 수도 있음. (이중 리스트)\n",
    "    \n",
    "    for annotation in annos:\n",
    "        for pair in rep_entity_property_pair:\n",
    "            entity_property = annotation[0]  # raw_data의 annotation 추출\n",
    "            entity_property = entity_property.replace(\" \", \"\")  # 띄어쓰기 제거\n",
    "            polarity = annotation[2]\n",
    "\n",
    "            if entity_property == pair:\n",
    "                dev_ACD_Datas.append(sen + \" \" + pair) \n",
    "                dev_ACD_labels.append(1)\n",
    "                dev_POL_Datas.append(sen + \" \" + pair)\n",
    "                dev_POL_labels.append(polarity_name_to_id[polarity])\n",
    "\n",
    "            else:\n",
    "                dev_ACD_Datas.append(sen + \" \" + pair) \n",
    "                dev_ACD_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7e0aae6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:24.971452Z",
     "start_time": "2022-12-08T07:49:24.959620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16884, 16884, 934, 934)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_ACD_Datas), len(dev_ACD_labels), len(dev_POL_Datas), len(dev_POL_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e7202bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:25.019341Z",
     "start_time": "2022-12-08T07:49:24.972767Z"
    }
   },
   "outputs": [],
   "source": [
    "################### Test set #######################\n",
    "test_ACD_Datas = []  \n",
    "test_ACD_labels = []\n",
    "\n",
    "test_POL_Datas = []\n",
    "test_POL_labels = []\n",
    "\n",
    "### sentence들 먼저 모으고 한꺼번에 tokenize 하자\n",
    "\n",
    "\n",
    "for datas in tests:\n",
    "    sen = preprocess(datas['sentence_form'])\n",
    "    annos = datas['annotation']  # 여러 개 일 수도 있음. (이중 리스트)\n",
    "    \n",
    "    for annotation in annos:\n",
    "        for pair in rep_entity_property_pair:\n",
    "            entity_property = annotation[0]  # raw_data의 annotation 추출\n",
    "            entity_property = entity_property.replace(\" \", \"\")  # 띄어쓰기 제거\n",
    "            polarity = annotation[2]\n",
    "\n",
    "            if entity_property == pair:\n",
    "                test_ACD_Datas.append(sen + \" \" + pair) \n",
    "                test_ACD_labels.append(1)\n",
    "                test_POL_Datas.append(sen + \" \" + pair)\n",
    "                test_POL_labels.append(polarity_name_to_id[polarity])\n",
    "\n",
    "            else:\n",
    "                test_ACD_Datas.append(sen + \" \" + pair) \n",
    "                test_ACD_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3800a03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:25.039223Z",
     "start_time": "2022-12-08T07:49:25.020700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38538, 38538, 2138, 2138)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ACD_Datas), len(test_ACD_labels), len(test_POL_Datas), len(test_POL_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba20d121",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:25.052049Z",
     "start_time": "2022-12-08T07:49:25.041661Z"
    }
   },
   "outputs": [],
   "source": [
    "class klue_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, label):  # 전처리된 데이터 셋이 들어옴\n",
    "        self.dataset = dataset\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # getitem이므로 gradient 계산에 영향을 주지 않게 clone().detach() 실행\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.dataset.items()}\n",
    "        item['label'] = torch.tensor(self.label[idx])\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):  # 샘플 수\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45cbb361",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:30.884548Z",
     "start_time": "2022-12-08T07:49:25.054690Z"
    }
   },
   "outputs": [],
   "source": [
    "ACD_tok_sen = tokenizer(ACD_Datas, padding='max_length', return_tensors=\"pt\",\n",
    "                max_length=256, truncation=True, add_special_tokens=True)  \n",
    "\n",
    "ACD_klue_sets = klue_Dataset(ACD_tok_sen, ACD_labels)  # klue_Dataset에서 1차원 텐서로 바뀜."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0857d3ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:30.893513Z",
     "start_time": "2022-12-08T07:49:30.886074Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0,  5560,  7036,  2116, 27316,  4015,  2460,  5153, 13679,  1785,\n",
       "          2235,  2119,   856,  2126,  2496,  2051,  1513,  2051,  2112,  3737,\n",
       "          8286,  2069,  1552,  2210,  1295,  1513,  2062, 32002,     2,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'label': tensor(0)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACD_klue_sets.__getitem__(758)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee5083c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:30.915261Z",
     "start_time": "2022-12-08T07:49:30.896013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58590\n"
     ]
    }
   ],
   "source": [
    "print(ACD_klue_sets.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72064403",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:32.440040Z",
     "start_time": "2022-12-08T07:49:30.917541Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    0, 11398,  2079,  6548,  2116,  7847,  3954,  5763, 27208, 13964,\n",
      "          848,  4442,  3853,  2259,  5009,  6548,  3681,  2052,  2209,  4181,\n",
      "         2190,  2995,  4671,  2085,  2154,  6159,  5370,   887, 25219,  2118,\n",
      "         1899,  2357,  2052,  3614,  3707,  3904, 11187,   831,  6758,  2318,\n",
      "        12110, 32002,     2,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'label': tensor(0)}\n",
      "16884\n"
     ]
    }
   ],
   "source": [
    "dev_ACD_tok_sen = tokenizer(dev_ACD_Datas, padding='max_length', return_tensors=\"pt\",\n",
    "                max_length=256, truncation=True, add_special_tokens=True)  \n",
    "\n",
    "dev_ACD_klue_sets = klue_Dataset(dev_ACD_tok_sen, dev_ACD_labels)  # klue_Dataset에서 1차원 텐서로 바뀜.\n",
    "    \n",
    "print(dev_ACD_klue_sets.__getitem__(758))\n",
    "print(dev_ACD_klue_sets.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24b60ec5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:36.197979Z",
     "start_time": "2022-12-08T07:49:32.441367Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    0,  5204, 31302,  2716,  2073,  9366,  2079,  9471, 32002,     2,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'label': tensor(0)}\n",
      "38538\n"
     ]
    }
   ],
   "source": [
    "test_ACD_tok_sen = tokenizer(test_ACD_Datas, padding='max_length', return_tensors=\"pt\",\n",
    "                max_length=256, truncation=True, add_special_tokens=True)  \n",
    "\n",
    "test_ACD_klue_sets = klue_Dataset(test_ACD_tok_sen, test_ACD_labels)  # klue_Dataset에서 1차원 텐서로 바뀜.\n",
    "    \n",
    "print(test_ACD_klue_sets.__getitem__(758))\n",
    "print(test_ACD_klue_sets.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51c616e",
   "metadata": {},
   "source": [
    "### Polarity datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "baefe0f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:36.429942Z",
     "start_time": "2022-12-08T07:49:36.199252Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0, 22869,  2145, 13408, 14360,  1560,  2073,  2532,  2073,  4458,\n",
       "          4019, 21820,  2047,  2377,  2052, 23548,  1540,  2052,  1141,  2052,\n",
       "          2170,  2182, 32012,     2,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'label': tensor(0)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_tok_sen = tokenizer(POL_Datas, padding='max_length', return_tensors=\"pt\",\n",
    "                    max_length=256, truncation=True, add_special_tokens=True)  \n",
    "\n",
    "POL_klue_sets = klue_Dataset(pol_tok_sen, POL_labels)\n",
    "\n",
    "POL_klue_sets.__getitem__(758)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31799adf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:36.509958Z",
     "start_time": "2022-12-08T07:49:36.431309Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0,  3776,  2170,  1904,  2119,  5723,  2112,  4514, 31302,  6509,\n",
       "           623,  2535,  4168, 32010,     2,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'label': tensor(0)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_tok_sen = tokenizer(dev_POL_Datas, padding='max_length', return_tensors=\"pt\",\n",
    "                    max_length=256, truncation=True, add_special_tokens=True)  \n",
    "\n",
    "dev_POL_klue_sets = klue_Dataset(pol_tok_sen, dev_POL_labels)\n",
    "\n",
    "dev_POL_klue_sets.__getitem__(758)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d206a45c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:36.681447Z",
     "start_time": "2022-12-08T07:49:36.511145Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0, 18391,  2069,  1523,  2259,  1284,  2119,  6978,  2088, 22045,\n",
       "          2205,  2307,  4975,  2118,  2259,  8146, 19630,  2119,  1560,  2088,\n",
       "         32001,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'label': tensor(0)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_tok_sen = tokenizer(test_POL_Datas, padding='max_length', return_tensors=\"pt\",\n",
    "                    max_length=256, truncation=True, add_special_tokens=True)  \n",
    "\n",
    "test_POL_klue_sets = klue_Dataset(pol_tok_sen, test_POL_labels)\n",
    "\n",
    "test_POL_klue_sets.__getitem__(758)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29884566",
   "metadata": {},
   "source": [
    "## Trainer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd143496",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:49:36.685714Z",
     "start_time": "2022-12-08T07:49:36.682783Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd48e8aa",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-08T07:49:12.152Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/egg2018037024/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 58590\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9155\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='9155' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  70/9155 00:53 < 1:59:00, 1.27 it/s, Epoch 0.04/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "odir = '/mnt/HDD4T/egg2018037024/ABSA_ACD'\n",
    "\n",
    "training_ars = TrainingArguments(\n",
    "    output_dir=odir,\n",
    "    num_train_epochs=5,\n",
    " #   max_steps=5000,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    save_total_limit=5,\n",
    "    save_strategy = \"epoch\",\n",
    " #   save_steps=1000,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy='epoch',\n",
    "    load_best_model_at_end = True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=ACD_model,\n",
    "    args=training_ars,\n",
    "    train_dataset=ACD_klue_sets,\n",
    "    eval_dataset=dev_ACD_klue_sets,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "ACD_mpodel.save_pretrained(odir + \"_best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfd0bed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 - tf",
   "language": "python",
   "name": "python3-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
